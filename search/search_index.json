{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Event Driven Reference Architecture Abstract The content of this repository was the source of the event-driven reference architecture in the IBM Garage architecture center . This git repository is maintained on a weekly basis and includes more content not yet formally published to IBM sites. As we are implementing the end to end solution we are updating this main git repository to keep best practices accurate. The modern digital business works in real time; it informs interested parties of things of interest when they happen, it makes sense of, and derives insight from an ever-growing number of sources. It learns, predicts and is intelligent -- it is by nature Event Driven. Events are a way of capturing a statement of fact. Events occur in a continuous stream as things happen in the real and digital worlds. By taking advntage of this continous stream, applications can not only react in real time, but also reason about the future based upon what has happened in the past. For enterprise IT teams, embracing event driven development is foundational to the next generation of digital business applications. IT teams will need to be able to design, develop, deploy and operate event driven solutions, in cloud native styles. While event driven architectures and reactive programming models are not new concepts, the move to Cloud Native architectures with Microservices, Container based workloads and \"server-less\" computing allow us to revisit event driven approaches in this Cloud Native context. Indeed we could think of event driven as extending the Resilience, Agility and Scale characteristics of \"Cloud Native\" to also be Reactive and Responsive. Two aspects of a cloud-native architecture are essential to developing an event driven architecture: Microservices - These provide the loosely coupled application architecture which enables deployment in highly distributed patterns for Resilience, Agility and Scale. Cloud Native platforms with Containers and \"Serverless deployments\" - These provide the application platform and tools which realize the Resilience, Agility and Scale promise of the microservices architectures. An Event Driven Architecture should provide the following essential event capabilities to the Cloud Native Platform. Being able to communicate and persist events. Being able to take direct action on events. Processing event streams to derive real time insight/intelligence. Providing communication for event driven microservices. This repository represents the root of related content about the Cloud Native Event Driven Architecture. It provides guidance for how to approach the design of event driven solutions, introduces the Cloud Native Event Driven reference architecture and provides reusable coding assets for implementation in a cloud native environment. Target audiences While the content of this repository is mostly technical in nature and is intended for a technical audience, it also introduces methods such as Event Storming which would be used with business leaders to identify key business domain events and actions. You may find it useful to share this information with your business leaders before engaging them in such activities. At a high level this is what you should expect to learn by working through this repository and the related examples. As an architect, you will understand how the event driven architecture provides capabilities which support development of event driven solutions. As a developer, you will understand how to develop event driven applications and develop analytics based on event streams. As a project manager, you may understand all the artifacts which may be required for an event driven solution. The related repositories provide sample code and best practices which you may want to reuse during your future implementations. The reference architecture has been designed to be portable and applicable to Public Cloud, Hybrid cloud and across multiple clouds. Examples given are directly deployable in IBM Public Cloud and with IBM Cloud Private. Concepts Before we start looking at the details of the Event Driven Architecture we will quickly examine the core concepts of being event driven: Events Event streams Commands Loose Coupling Cohesion Read more ... Event sources The modern digital business is driven by events. Events come into the business and events likewise need to be pushed outside of the business. For our Cloud Native Event Driven Architecture we consider event sources to be all of those things which may generate events which are of interest to the business. This could include, events coming from IoT devices, mobile apps, web apps, database triggers or microservices. In general terms, an Event Source , or event producer is any component capable of creating an event notification and publishing it to the event backbone, but let look at some specific types of producer to better understand the opportunity with event driven. Read more ... Event Backbone The Event Backbone is the center of the Event Driven Architecture providing the event communication and persistence layer with the following capabilities: Pub/Sub style event communication between event producers and consumers An Event Log to persist events for a given period of time Replay of events Subscriptions from multiple consumers Read more ... Taking an Action After an event has occurred is one of the fundamental operations for any event driven solution. IBM Cloud Functions provides a simplified event driven programming model, enabling developers to simply write the action code in the language of their choice and have Cloud Functions manage the computation workload. With this simplified model: A business event of interest would be published to the event backbone. The action for the event would be written as a Cloud Functions action. Cloud Functions would be configured to subscribe to the event and use it as a trigger to start the action . Cloud functions manages the start-up of all required compute resources. Cloud functions managed execution of the action code. Cloud functions manages the shut-down of the computation resources when the action is complete. Read more ... Real time insights/intelligence Processing continuous streaming events to derive real time insights/intelligence is an essential element of modern event driven solutions. Specialized streaming analytics engines provide the means to run stateful analytical and complex event processing workloads across multiple real time event streams while maintaining low latency processing times. Including these engines as part of the Event Driven Architecture enables: Analysis and understanding of real time event streams Extracting real time event data from the stream so that Data Scientists can understand and derive Machine Learning models Running analytical processes, Machine Learning models in line in real time against the event stream. Matching of complex event patterns across multiple streams and time windows to make decisions and take actions Read more ... Event Managed State While the prime focus for an event driven architecture is for processing events, there are cases where we need to persist events for post processing and queries by other applications. With the event backbone we have a builtin Event Log which provides the means to store and reply events published to the backbone, however when we consider the full scope of Event Driven solutions there are other use cases and types of store that we should support. This includes: Event Stores optimized for analytics Event Sourcing as a pattern for recording state changes and updates across distributed systems Command Query Response Separation (CQRS) as an optimization which separates updates and reads across different stores Read more ... Event Driven Cloud Native Apps (Microservices) The event driven architecture must also reach across into our application platform. Developers will build applications which interact with events and are themselves event driven, that is they will both produce and consume events via the event backbone. In this context we can view the Event Backbone as being part of the microservices mesh, providing the Pub/Sub communication between microservices, and therefore enabling the support of loosely coupled event driven microservices. Read more ... Event Storming When it comes to the design of event driven solutions there are some additional methods which can be utilized to help understand the business events and actions that make up a business. Event Storming , is a workshop format for quickly exploring complex business domains by focusing on domain events generated in the context of a business process or a business application. It focuses on communication between product owner, domain experts and developers. Insights Storming , is an extension to the event storming workshop and encourages a forward-looking approach to consider the insights, (predictive models) which would make a difference to the business when we look at actions for key business events. What if instead of seeing a system has failed event (events are something that has happened) we could see a predictive or derived event, the system will fail in 3 days , we could take preventative actions. Read more about the Event Storming Methodology Applicability of an EDA EDAs are typically not used for distributed transactional processing because this can lead to increased coupling and performance degradation. But as seen in previous section, using a message backbone to support communication between microservices to ensure data consistency is a viable pattern. The use of EDAs for batch processing is also restricted to cases where the potential for parallelizing batch workloads exist. Most often EDAs are used for event driven applications that require near-realtime situation awareness and decision making. Read more about EDA applicability and use cases Sample EDA Applications Container shipment solution : this solution presents real time analytics, pub-sub architecture pattern and micro-service communication on Kafka. Predictive maintenance - analytics and EDA how to mix Apache Kafka, stateful stream, Apache Cassandra and ICP for data to develop machine learning model deployed as a service.","title":"Home"},{"location":"#event-driven-reference-architecture","text":"Abstract The content of this repository was the source of the event-driven reference architecture in the IBM Garage architecture center . This git repository is maintained on a weekly basis and includes more content not yet formally published to IBM sites. As we are implementing the end to end solution we are updating this main git repository to keep best practices accurate. The modern digital business works in real time; it informs interested parties of things of interest when they happen, it makes sense of, and derives insight from an ever-growing number of sources. It learns, predicts and is intelligent -- it is by nature Event Driven. Events are a way of capturing a statement of fact. Events occur in a continuous stream as things happen in the real and digital worlds. By taking advntage of this continous stream, applications can not only react in real time, but also reason about the future based upon what has happened in the past. For enterprise IT teams, embracing event driven development is foundational to the next generation of digital business applications. IT teams will need to be able to design, develop, deploy and operate event driven solutions, in cloud native styles. While event driven architectures and reactive programming models are not new concepts, the move to Cloud Native architectures with Microservices, Container based workloads and \"server-less\" computing allow us to revisit event driven approaches in this Cloud Native context. Indeed we could think of event driven as extending the Resilience, Agility and Scale characteristics of \"Cloud Native\" to also be Reactive and Responsive. Two aspects of a cloud-native architecture are essential to developing an event driven architecture: Microservices - These provide the loosely coupled application architecture which enables deployment in highly distributed patterns for Resilience, Agility and Scale. Cloud Native platforms with Containers and \"Serverless deployments\" - These provide the application platform and tools which realize the Resilience, Agility and Scale promise of the microservices architectures. An Event Driven Architecture should provide the following essential event capabilities to the Cloud Native Platform. Being able to communicate and persist events. Being able to take direct action on events. Processing event streams to derive real time insight/intelligence. Providing communication for event driven microservices. This repository represents the root of related content about the Cloud Native Event Driven Architecture. It provides guidance for how to approach the design of event driven solutions, introduces the Cloud Native Event Driven reference architecture and provides reusable coding assets for implementation in a cloud native environment.","title":"Event Driven Reference Architecture"},{"location":"#target-audiences","text":"While the content of this repository is mostly technical in nature and is intended for a technical audience, it also introduces methods such as Event Storming which would be used with business leaders to identify key business domain events and actions. You may find it useful to share this information with your business leaders before engaging them in such activities. At a high level this is what you should expect to learn by working through this repository and the related examples. As an architect, you will understand how the event driven architecture provides capabilities which support development of event driven solutions. As a developer, you will understand how to develop event driven applications and develop analytics based on event streams. As a project manager, you may understand all the artifacts which may be required for an event driven solution. The related repositories provide sample code and best practices which you may want to reuse during your future implementations. The reference architecture has been designed to be portable and applicable to Public Cloud, Hybrid cloud and across multiple clouds. Examples given are directly deployable in IBM Public Cloud and with IBM Cloud Private.","title":"Target audiences"},{"location":"#concepts","text":"Before we start looking at the details of the Event Driven Architecture we will quickly examine the core concepts of being event driven: Events Event streams Commands Loose Coupling Cohesion Read more ...","title":"Concepts"},{"location":"#event-sources","text":"The modern digital business is driven by events. Events come into the business and events likewise need to be pushed outside of the business. For our Cloud Native Event Driven Architecture we consider event sources to be all of those things which may generate events which are of interest to the business. This could include, events coming from IoT devices, mobile apps, web apps, database triggers or microservices. In general terms, an Event Source , or event producer is any component capable of creating an event notification and publishing it to the event backbone, but let look at some specific types of producer to better understand the opportunity with event driven. Read more ...","title":"Event sources"},{"location":"#event-backbone","text":"The Event Backbone is the center of the Event Driven Architecture providing the event communication and persistence layer with the following capabilities: Pub/Sub style event communication between event producers and consumers An Event Log to persist events for a given period of time Replay of events Subscriptions from multiple consumers Read more ...","title":"Event Backbone"},{"location":"#taking-an-action","text":"After an event has occurred is one of the fundamental operations for any event driven solution. IBM Cloud Functions provides a simplified event driven programming model, enabling developers to simply write the action code in the language of their choice and have Cloud Functions manage the computation workload. With this simplified model: A business event of interest would be published to the event backbone. The action for the event would be written as a Cloud Functions action. Cloud Functions would be configured to subscribe to the event and use it as a trigger to start the action . Cloud functions manages the start-up of all required compute resources. Cloud functions managed execution of the action code. Cloud functions manages the shut-down of the computation resources when the action is complete. Read more ...","title":"Taking an Action"},{"location":"#real-time-insightsintelligence","text":"Processing continuous streaming events to derive real time insights/intelligence is an essential element of modern event driven solutions. Specialized streaming analytics engines provide the means to run stateful analytical and complex event processing workloads across multiple real time event streams while maintaining low latency processing times. Including these engines as part of the Event Driven Architecture enables: Analysis and understanding of real time event streams Extracting real time event data from the stream so that Data Scientists can understand and derive Machine Learning models Running analytical processes, Machine Learning models in line in real time against the event stream. Matching of complex event patterns across multiple streams and time windows to make decisions and take actions Read more ...","title":"Real time insights/intelligence"},{"location":"#event-managed-state","text":"While the prime focus for an event driven architecture is for processing events, there are cases where we need to persist events for post processing and queries by other applications. With the event backbone we have a builtin Event Log which provides the means to store and reply events published to the backbone, however when we consider the full scope of Event Driven solutions there are other use cases and types of store that we should support. This includes: Event Stores optimized for analytics Event Sourcing as a pattern for recording state changes and updates across distributed systems Command Query Response Separation (CQRS) as an optimization which separates updates and reads across different stores Read more ...","title":"Event Managed State"},{"location":"#event-driven-cloud-native-apps-microservices","text":"The event driven architecture must also reach across into our application platform. Developers will build applications which interact with events and are themselves event driven, that is they will both produce and consume events via the event backbone. In this context we can view the Event Backbone as being part of the microservices mesh, providing the Pub/Sub communication between microservices, and therefore enabling the support of loosely coupled event driven microservices. Read more ...","title":"Event Driven Cloud Native Apps (Microservices)"},{"location":"#event-storming","text":"When it comes to the design of event driven solutions there are some additional methods which can be utilized to help understand the business events and actions that make up a business. Event Storming , is a workshop format for quickly exploring complex business domains by focusing on domain events generated in the context of a business process or a business application. It focuses on communication between product owner, domain experts and developers. Insights Storming , is an extension to the event storming workshop and encourages a forward-looking approach to consider the insights, (predictive models) which would make a difference to the business when we look at actions for key business events. What if instead of seeing a system has failed event (events are something that has happened) we could see a predictive or derived event, the system will fail in 3 days , we could take preventative actions. Read more about the Event Storming Methodology","title":"Event Storming"},{"location":"#applicability-of-an-eda","text":"EDAs are typically not used for distributed transactional processing because this can lead to increased coupling and performance degradation. But as seen in previous section, using a message backbone to support communication between microservices to ensure data consistency is a viable pattern. The use of EDAs for batch processing is also restricted to cases where the potential for parallelizing batch workloads exist. Most often EDAs are used for event driven applications that require near-realtime situation awareness and decision making. Read more about EDA applicability and use cases","title":"Applicability of an EDA"},{"location":"#sample-eda-applications","text":"Container shipment solution : this solution presents real time analytics, pub-sub architecture pattern and micro-service communication on Kafka. Predictive maintenance - analytics and EDA how to mix Apache Kafka, stateful stream, Apache Cassandra and ICP for data to develop machine learning model deployed as a service.","title":"Sample EDA Applications"},{"location":"architecture/","text":"Reference Architecture We defined the starting point for a Cloud Native Event Driven Architecture to be that it supports at least the following important capabilities: Being able to communicate and persist events. Being able to take direct action on events. Processing streams of events to derive real time insight/intelligence. Providing communication between event driven microservices and functions. With an event backbone providing the connectivity between the capabilities, we can visualize a reference Event Driven Architecture as below: Where: Event sources : generates events and event streams from sources such as IoT devices, web app, mobile app, microservices\u2026 IBM Event Streams : Provides an Event Backbone supporting Pub/Sub communication, an event log, and simple event stream processing based on Apache Kafka . IBM Cloud Functions : Provides a simplified programming model to take action on an event through a \"serverless\" function-based compute model. Streaming Analytics : Provides continuous ingest and analytical processing across multiple event streams. Decision Server Insights: Provides the means to take action on events and event streams through business rules. Event Stores: Provide optimized persistence (data stores), for event sourcing, Command Query Response Separation (CQRS) and analytical use cases. Event Driven Microservices : Applications that run as serverless functions or containerized workloads which are connected via pub/sub event communication through the event backbone. Extended Architecture The event-driven reference architecture provides the framework to support event-driven applications and solutions. The extended architecture provides the connections for: Integration with legacy apps and data resources Integration with analytics or machine learning to derive real-time insights The diagram below shows how these capabilities fit together to form an extended event-driven architecture. In 7. the AI workbench includes tools to do data analysis and visualization, build training and test sets from any datasource and in particular Event Store, and develop models. Models are pushed to streaming analytics component. Integration with analytics and machine learning The extended architecture extends the basic EDA reference architecture with concepts showing how data science, artificial intelligence and machine learning can be incorporated into an event-driven solution. The starting point for data scientists to be able to derive machine learning models or analyze data for trends and behaviors is the existence of the data in a form that they can be consumed. For real-time intelligent solutions, data scientists typically inspect event histories and decision or action records from a system. Then, they reduce this data to some simplified model that scores new event data as it arrives. Getting the data for the data scientist: With real-time event streams, the challenge is in handling unbounded data or a continuous flow of events. To make this consumable for the data scientist you need to capture the relevant data and store it so that it can be pulled into the analysis and model-building process as required. Following our event-driven reference architecture the event stream would be a Kafka topic on the event backbone. From here there are two possibilities for making that event data available and consumable to the data scientist: The event stream or event log can be accessed directly through Kafka and pulled into the analysis process The event stream can be pre-processed by the streaming analytics system and stored for future use in the analysis process. You have a choice of store type to use. Within public IBM cloud object storage Cloud Object Store can be used as a cost-effective historical store. Both approaches are valid, pre-processing through streaming analytics provides opportunity for greater manipulation of the data, or storing data over time windows for complex event processing. However, the more interesting distinction is where you use a predictive (ML model) to score arriving events or stream data in real time. In this case you may use streaming analytics to extract and save the event data for analysis, model building, and model training and also for scoring (executing) a derived model in line in the real time against arriving event data. The event and decision or action data is made available in cloud object storage for model building through streaming analytics. Models may be developed by tuning and parameter fitting, standard form fitting, classification techniques, and text analytics methods. Increasingly artificial intelligence (AI) and machine learning (ML) frameworks are used to discover and train useful predictive models as an alternative to parameterizing existing model types manually. These techniques lead to process and data flows where the predictive model is trained offline using event histories from the event and the decision or action store possibly augmented with some supervisory outcome labelling, as illustrated by the paths from the Event Backbone and Stream Processing store into Learn/Analyze . A model trained in this way includes some \u201cscoring\u201d API that can be invoked with fresh event data to generate a model-based prediction for future behavior and event properties of that specific context. The scoring function is then easily reincorporated into the streaming analytics processing to generate predictions and insights. These combined techniques can lead to the creation of real-time intelligent applications: 1. Event-driven architecture 2. Identification of predictive insights using event storming methodology 3. Developing models for these insights using machine learning 4. Real-time scoring of the insight models using a streaming analytics processing framework These are scalable easily extensible, and adaptable applications responding in near real time to new situations. There are easily extended to build out and evolve from an initial minimal viable product (MVP) because of the loose coupling in the event-driven architecture, , and streams process domains. Data scientist workbench To complete the extended architecture for integration with analytics and machine learning, consider the toolset and frameworks that the data scientist can use to derive the models. Watson Studio provides tools for data scientists, application developers, and subject matter experts to collaboratively and easily work with data to build and train models at scale. For more information see Getting started with Watson Studio. Legacy integration While you create new digital business applications as self-contained systems, you likely need to integrate legacy apps and databases into the event-driven system. Two ways of coming directly into the event-driven architecture are as follows: Where legacy applications are connected with MQ. You can connect directly from MQ to the Kafka in the event backbone. See IBM Event Streams getting started with MQ article . Where databases support the capture of changes to data, you can publish changes as events to Kafka and hence into the event infrastructure. See the confluent blog for more details","title":"Reference diagrams"},{"location":"architecture/#reference-architecture","text":"We defined the starting point for a Cloud Native Event Driven Architecture to be that it supports at least the following important capabilities: Being able to communicate and persist events. Being able to take direct action on events. Processing streams of events to derive real time insight/intelligence. Providing communication between event driven microservices and functions. With an event backbone providing the connectivity between the capabilities, we can visualize a reference Event Driven Architecture as below: Where: Event sources : generates events and event streams from sources such as IoT devices, web app, mobile app, microservices\u2026 IBM Event Streams : Provides an Event Backbone supporting Pub/Sub communication, an event log, and simple event stream processing based on Apache Kafka . IBM Cloud Functions : Provides a simplified programming model to take action on an event through a \"serverless\" function-based compute model. Streaming Analytics : Provides continuous ingest and analytical processing across multiple event streams. Decision Server Insights: Provides the means to take action on events and event streams through business rules. Event Stores: Provide optimized persistence (data stores), for event sourcing, Command Query Response Separation (CQRS) and analytical use cases. Event Driven Microservices : Applications that run as serverless functions or containerized workloads which are connected via pub/sub event communication through the event backbone.","title":"Reference Architecture"},{"location":"architecture/#extended-architecture","text":"The event-driven reference architecture provides the framework to support event-driven applications and solutions. The extended architecture provides the connections for: Integration with legacy apps and data resources Integration with analytics or machine learning to derive real-time insights The diagram below shows how these capabilities fit together to form an extended event-driven architecture. In 7. the AI workbench includes tools to do data analysis and visualization, build training and test sets from any datasource and in particular Event Store, and develop models. Models are pushed to streaming analytics component.","title":"Extended Architecture"},{"location":"architecture/#integration-with-analytics-and-machine-learning","text":"The extended architecture extends the basic EDA reference architecture with concepts showing how data science, artificial intelligence and machine learning can be incorporated into an event-driven solution. The starting point for data scientists to be able to derive machine learning models or analyze data for trends and behaviors is the existence of the data in a form that they can be consumed. For real-time intelligent solutions, data scientists typically inspect event histories and decision or action records from a system. Then, they reduce this data to some simplified model that scores new event data as it arrives.","title":"Integration with analytics and machine learning"},{"location":"architecture/#getting-the-data-for-the-data-scientist","text":"With real-time event streams, the challenge is in handling unbounded data or a continuous flow of events. To make this consumable for the data scientist you need to capture the relevant data and store it so that it can be pulled into the analysis and model-building process as required. Following our event-driven reference architecture the event stream would be a Kafka topic on the event backbone. From here there are two possibilities for making that event data available and consumable to the data scientist: The event stream or event log can be accessed directly through Kafka and pulled into the analysis process The event stream can be pre-processed by the streaming analytics system and stored for future use in the analysis process. You have a choice of store type to use. Within public IBM cloud object storage Cloud Object Store can be used as a cost-effective historical store. Both approaches are valid, pre-processing through streaming analytics provides opportunity for greater manipulation of the data, or storing data over time windows for complex event processing. However, the more interesting distinction is where you use a predictive (ML model) to score arriving events or stream data in real time. In this case you may use streaming analytics to extract and save the event data for analysis, model building, and model training and also for scoring (executing) a derived model in line in the real time against arriving event data. The event and decision or action data is made available in cloud object storage for model building through streaming analytics. Models may be developed by tuning and parameter fitting, standard form fitting, classification techniques, and text analytics methods. Increasingly artificial intelligence (AI) and machine learning (ML) frameworks are used to discover and train useful predictive models as an alternative to parameterizing existing model types manually. These techniques lead to process and data flows where the predictive model is trained offline using event histories from the event and the decision or action store possibly augmented with some supervisory outcome labelling, as illustrated by the paths from the Event Backbone and Stream Processing store into Learn/Analyze . A model trained in this way includes some \u201cscoring\u201d API that can be invoked with fresh event data to generate a model-based prediction for future behavior and event properties of that specific context. The scoring function is then easily reincorporated into the streaming analytics processing to generate predictions and insights. These combined techniques can lead to the creation of real-time intelligent applications: 1. Event-driven architecture 2. Identification of predictive insights using event storming methodology 3. Developing models for these insights using machine learning 4. Real-time scoring of the insight models using a streaming analytics processing framework These are scalable easily extensible, and adaptable applications responding in near real time to new situations. There are easily extended to build out and evolve from an initial minimal viable product (MVP) because of the loose coupling in the event-driven architecture, , and streams process domains.","title":"Getting the data for the data scientist:"},{"location":"architecture/#data-scientist-workbench","text":"To complete the extended architecture for integration with analytics and machine learning, consider the toolset and frameworks that the data scientist can use to derive the models. Watson Studio provides tools for data scientists, application developers, and subject matter experts to collaboratively and easily work with data to build and train models at scale. For more information see Getting started with Watson Studio.","title":"Data scientist workbench"},{"location":"architecture/#legacy-integration","text":"While you create new digital business applications as self-contained systems, you likely need to integrate legacy apps and databases into the event-driven system. Two ways of coming directly into the event-driven architecture are as follows: Where legacy applications are connected with MQ. You can connect directly from MQ to the Kafka in the event backbone. See IBM Event Streams getting started with MQ article . Where databases support the capture of changes to data, you can publish changes as events to Kafka and hence into the event infrastructure. See the confluent blog for more details","title":"Legacy integration"},{"location":"compendium/","text":"Compendium IBM Event Streams Event Streams presentation Event Streams Samples Validating Event Streams deployment with sample app. Install Event Streams on ICP IBM Event Streams product based on Kafka delivered in ICP catalog Kafka Start by reading Kafka introduction - a must read! Another introduction from Confluent, one of the main contributors of the open source. Kafka summary and deployment on IBM Cloud Private Planning event streams installation Develop Stream Application using Kafka Tutorial on access control, user authentication and authorization from IBM. Kafka on Kubernetes using stateful sets IBM Developer article - learn kafka Spark and Kafka with direct stream, and persistence considerations and best practices Example in scala for processing Tweets with Kafka Streams Microservices and event-driven patterns API for declaring messaging handlers using Reactive Streams Microservice patterns - Chris Richardson Service mesh Stream Analytics Getting started with IBM Streaming Analytics on IBM Cloud Serverless and cloud function Using Cloud functions with event trigger in Kafka https://github.com/IBM/ibm-cloud-functions-message-hub-trigger Serverless IBM Cloud Functions product offering https://www.ibm.com/cloud/functions","title":"Compendium"},{"location":"compendium/#compendium","text":"","title":"Compendium"},{"location":"compendium/#ibm-event-streams","text":"Event Streams presentation Event Streams Samples Validating Event Streams deployment with sample app. Install Event Streams on ICP IBM Event Streams product based on Kafka delivered in ICP catalog","title":"IBM Event Streams"},{"location":"compendium/#kafka","text":"Start by reading Kafka introduction - a must read! Another introduction from Confluent, one of the main contributors of the open source. Kafka summary and deployment on IBM Cloud Private Planning event streams installation Develop Stream Application using Kafka Tutorial on access control, user authentication and authorization from IBM. Kafka on Kubernetes using stateful sets IBM Developer article - learn kafka Spark and Kafka with direct stream, and persistence considerations and best practices Example in scala for processing Tweets with Kafka Streams","title":"Kafka"},{"location":"compendium/#microservices-and-event-driven-patterns","text":"API for declaring messaging handlers using Reactive Streams Microservice patterns - Chris Richardson Service mesh","title":"Microservices and event-driven patterns"},{"location":"compendium/#stream-analytics","text":"Getting started with IBM Streaming Analytics on IBM Cloud","title":"Stream Analytics"},{"location":"compendium/#serverless-and-cloud-function","text":"Using Cloud functions with event trigger in Kafka https://github.com/IBM/ibm-cloud-functions-message-hub-trigger Serverless IBM Cloud Functions product offering https://www.ibm.com/cloud/functions","title":"Serverless and cloud function"},{"location":"eda-skill-journey/","text":"EDA Skill Journey Implementing cloud native, event-driven solution with microservices deployed on kubernetes involves a broad skill set. We are proposing here a learning journey for developer with good programming background. This project includes best practices and basic knowledge on the technologies used inthe solution implementation, which represent the most used ones in modern architecture. The EDA reference solution implementation in this project includes a set of technology we are using that represent the modern landscape of cloud native applications (Kafka, maven, java, microprofile, kafka API, Kafka Stream API, Spring boot, Python, Nodejs, and Postgresql) but also some specific analytics and AI components like Streams analytics and machine learning with Jupyter notebook. A developer who wants to consume this content does not need to know everything at the expert level. You can progress by steps and it will take a good month to digest everything. We are also proposing a bootcamp to build, deploy and re-implement part of the \"Reefer container shipment solution\" . Note We expect you have some good knowledge around the following technologies. Nodejs / Javascript / Typescripts Java 1.8 amd microprofile architecture Python 3.6 Angular 7, HTML, CSS - This is for the user interface but this is more optional. Maven, npm, bash WebSphere Liberty or OpenLiberty Docker Docker compose Helm Kubernetes Apache Kafka, Kafka API Getting started around the core technologies used in EDA From the list above, the following getting started and tutorials can be studied to get a good pre-requisite knowledge: From zero to hero in Java 1.8 - an infoworld good article Open Liberty getting started application Getting started with Apache Maven Getting started Nodejs and npm Getting started with Apache Kafka and the Confluent blog for getting started with Kafka Angular tutorial - This is for the user interface but this is more optional. Docker getting started Getting started with Open Liberty Kubernetes and IBM developer learning path for Kubernetes and the Garage course Kubernetes 101 . Use the \"Develop a Kubernetes app with Helm\" toolchain on IBM Cloud Getting started in Python Applying a test driven practice for angular application Event Driven Specifics Now the event driven microservice involve specific technologies and practice. The following links should be studied in the proposed order: Why Event Driven Architecture now? EDA fundamentals Key concepts Reference architecture with event backbone, microservices and real time analytics Extended Reference Architecture with machine learning and AI integrated with real time analytics with machine learning workbench and event sourcing as data source, and real time analytics for deployment. Event sources - as event producers Event backbone where Kafka is the main implementation High availability and disaster recovery with IBM Event Streams or Kafka Architecture Considerations Event driven microservice development Event driven design patterns for microservice with CQRS, event sourcing and saga patterns. Processing continuous streaming events Event-driven cloud-native applications Kafka producer API documentation with some of our event producers best practices Kafka consumer API documentation with some of our own Event consumers best practices Kafka Stream APIs , Java or Scala based API to implement functional processing as a chain of operation to consumer events and generating new event stream. Act on events with IBM Cloud Functions IBM Event Streams - stream analytics app Event detection on continuous feed using Streaming Analytics in IBM Cloud. Kafka monitoring Kafka Python API and some examples in our integration tests project Kafka Nodejs API used in the voyage microservice Methodology Event storming methodology A concrete example to apply event storming, for a container shipment use case. Domain design driven implementation from event storming Kubernetes, docker, microprofile As we can use docker compose to control the dependencies between microservices and run all the solution as docker containers, it is important to read the Docker compose - getting started article. Kafka is Getting started with IBM Cloud Event Streams the IBM product based on Kafka on public cloud IBM Cloud Private Event Streams the IBM product based on Kafka for private cloud Understand docker networking as we use docker compose to run the reference implementation locally. The evolving hybrid integration reference architecture : How to ensure your integration landscape keeps pace with digital transformation Java microprofile application Deploy MicroProfile-based Java microservices on Kubernetes Knative introduction How to deploy, manage, and secure your container-based workloads on IKS and part 2 Hands-on labs The next steps beyond getting started and reading our technical point of view, you can try to deeper dive into the solution implementation and deployment: The source of this virtual bootcamp is the \"Reefer container shipment solution\" . Note At the end of this training you should have the following solution up and running (See detailed description here ): Understand the event storming analysis and derived design For those who are interested by the methodology applied we propose to review: The solution introduction to get a sense of the goals of this application. (7 minutes read) Followed by the event storming analysis report (15 minutes read). and the design derived from this analysis. (15 minutes reading) Prepare a local environment Here you have two options: running with docker compose, or running within Minikude, or the kubernetes single note running with docker (At least on Mac it works!). Get a local Kafka backbone environment, using docker compose, up and running to facilitate your development and testing. Get an event backbone up and running on your laptop in less than 3 minutes . Or use Minikube to get kafka, zookeeper and poastgreSQl up and running on kubernetes Build and run the solution Goals Build and run the solution so you can understand the maven, nodejs build process with docker stage build. Build and deploy the solution locally using docker compose Or build and deploy the solution locally using Minikube Execute the integration tests to validate the solution end to end. [Optional] Execute the demonstration script Review event driven patterns Review the Event sourcing explanations Review the CQRS pattern and the implementation in the order microservice . Data replication with Kafka One of the common usage of using Kafka is to combine it with a Change Data Capture component to get update from a \"legacy\" data base to the new microservice runtime environment. We are detailing an approach in this article . Other deployments Deploying the solution on IBM Cloud Kubernetes Service Deploying the solution on IBM Cloud Private Develop a toolchain for one of the container manager service Our Kubernetes troubleshooting notes Real time analytics and Machine learning IBM Cloud Streaming Analytics introduction and getting started Apply predictive analytics on container metrics for predictive maintenance use case","title":"Skill Journey"},{"location":"eda-skill-journey/#eda-skill-journey","text":"Implementing cloud native, event-driven solution with microservices deployed on kubernetes involves a broad skill set. We are proposing here a learning journey for developer with good programming background. This project includes best practices and basic knowledge on the technologies used inthe solution implementation, which represent the most used ones in modern architecture. The EDA reference solution implementation in this project includes a set of technology we are using that represent the modern landscape of cloud native applications (Kafka, maven, java, microprofile, kafka API, Kafka Stream API, Spring boot, Python, Nodejs, and Postgresql) but also some specific analytics and AI components like Streams analytics and machine learning with Jupyter notebook. A developer who wants to consume this content does not need to know everything at the expert level. You can progress by steps and it will take a good month to digest everything. We are also proposing a bootcamp to build, deploy and re-implement part of the \"Reefer container shipment solution\" . Note We expect you have some good knowledge around the following technologies. Nodejs / Javascript / Typescripts Java 1.8 amd microprofile architecture Python 3.6 Angular 7, HTML, CSS - This is for the user interface but this is more optional. Maven, npm, bash WebSphere Liberty or OpenLiberty Docker Docker compose Helm Kubernetes Apache Kafka, Kafka API","title":"EDA Skill Journey"},{"location":"eda-skill-journey/#getting-started-around-the-core-technologies-used-in-eda","text":"From the list above, the following getting started and tutorials can be studied to get a good pre-requisite knowledge: From zero to hero in Java 1.8 - an infoworld good article Open Liberty getting started application Getting started with Apache Maven Getting started Nodejs and npm Getting started with Apache Kafka and the Confluent blog for getting started with Kafka Angular tutorial - This is for the user interface but this is more optional. Docker getting started Getting started with Open Liberty Kubernetes and IBM developer learning path for Kubernetes and the Garage course Kubernetes 101 . Use the \"Develop a Kubernetes app with Helm\" toolchain on IBM Cloud Getting started in Python Applying a test driven practice for angular application","title":"Getting started around the core technologies used in EDA"},{"location":"eda-skill-journey/#event-driven-specifics","text":"Now the event driven microservice involve specific technologies and practice. The following links should be studied in the proposed order: Why Event Driven Architecture now?","title":"Event Driven Specifics"},{"location":"eda-skill-journey/#eda-fundamentals","text":"Key concepts Reference architecture with event backbone, microservices and real time analytics Extended Reference Architecture with machine learning and AI integrated with real time analytics with machine learning workbench and event sourcing as data source, and real time analytics for deployment. Event sources - as event producers Event backbone where Kafka is the main implementation High availability and disaster recovery with IBM Event Streams or Kafka Architecture Considerations","title":"EDA fundamentals"},{"location":"eda-skill-journey/#event-driven-microservice-development","text":"Event driven design patterns for microservice with CQRS, event sourcing and saga patterns. Processing continuous streaming events Event-driven cloud-native applications Kafka producer API documentation with some of our event producers best practices Kafka consumer API documentation with some of our own Event consumers best practices Kafka Stream APIs , Java or Scala based API to implement functional processing as a chain of operation to consumer events and generating new event stream. Act on events with IBM Cloud Functions IBM Event Streams - stream analytics app Event detection on continuous feed using Streaming Analytics in IBM Cloud. Kafka monitoring Kafka Python API and some examples in our integration tests project Kafka Nodejs API used in the voyage microservice","title":"Event driven microservice development"},{"location":"eda-skill-journey/#methodology","text":"Event storming methodology A concrete example to apply event storming, for a container shipment use case. Domain design driven implementation from event storming","title":"Methodology"},{"location":"eda-skill-journey/#kubernetes-docker-microprofile","text":"As we can use docker compose to control the dependencies between microservices and run all the solution as docker containers, it is important to read the Docker compose - getting started article. Kafka is Getting started with IBM Cloud Event Streams the IBM product based on Kafka on public cloud IBM Cloud Private Event Streams the IBM product based on Kafka for private cloud Understand docker networking as we use docker compose to run the reference implementation locally. The evolving hybrid integration reference architecture : How to ensure your integration landscape keeps pace with digital transformation Java microprofile application Deploy MicroProfile-based Java microservices on Kubernetes Knative introduction How to deploy, manage, and secure your container-based workloads on IKS and part 2","title":"Kubernetes, docker, microprofile"},{"location":"eda-skill-journey/#hands-on-labs","text":"The next steps beyond getting started and reading our technical point of view, you can try to deeper dive into the solution implementation and deployment: The source of this virtual bootcamp is the \"Reefer container shipment solution\" . Note At the end of this training you should have the following solution up and running (See detailed description here ):","title":"Hands-on labs"},{"location":"eda-skill-journey/#understand-the-event-storming-analysis-and-derived-design","text":"For those who are interested by the methodology applied we propose to review: The solution introduction to get a sense of the goals of this application. (7 minutes read) Followed by the event storming analysis report (15 minutes read). and the design derived from this analysis. (15 minutes reading)","title":"Understand the event storming analysis and derived design"},{"location":"eda-skill-journey/#prepare-a-local-environment","text":"Here you have two options: running with docker compose, or running within Minikude, or the kubernetes single note running with docker (At least on Mac it works!). Get a local Kafka backbone environment, using docker compose, up and running to facilitate your development and testing. Get an event backbone up and running on your laptop in less than 3 minutes . Or use Minikube to get kafka, zookeeper and poastgreSQl up and running on kubernetes","title":"Prepare a local environment"},{"location":"eda-skill-journey/#build-and-run-the-solution","text":"Goals Build and run the solution so you can understand the maven, nodejs build process with docker stage build. Build and deploy the solution locally using docker compose Or build and deploy the solution locally using Minikube Execute the integration tests to validate the solution end to end. [Optional] Execute the demonstration script","title":"Build and run the solution"},{"location":"eda-skill-journey/#review-event-driven-patterns","text":"Review the Event sourcing explanations Review the CQRS pattern and the implementation in the order microservice .","title":"Review event driven patterns"},{"location":"eda-skill-journey/#data-replication-with-kafka","text":"One of the common usage of using Kafka is to combine it with a Change Data Capture component to get update from a \"legacy\" data base to the new microservice runtime environment. We are detailing an approach in this article .","title":"Data replication with Kafka"},{"location":"eda-skill-journey/#other-deployments","text":"Deploying the solution on IBM Cloud Kubernetes Service Deploying the solution on IBM Cloud Private Develop a toolchain for one of the container manager service Our Kubernetes troubleshooting notes","title":"Other deployments"},{"location":"eda-skill-journey/#real-time-analytics-and-machine-learning","text":"IBM Cloud Streaming Analytics introduction and getting started Apply predictive analytics on container metrics for predictive maintenance use case","title":"Real time analytics and Machine learning"},{"location":"concepts/","text":"Concepts Events Events are notifications of change of state. Notifications are issued, or published and interested parties can subscribe and take action on the events. Typically, the issuer of the notification has no knowledge of what action is taken and receives no corresponding feedback that the notification has been processed. Events are notifications of change of state. Typically, events represent the change of state of something of interest to the business. Events are records of something that has happened. Events can't be changed, that is, they are immutable. (We can't change something that has happened in the past). Event streams An event stream is a continuous unbounded series of events. The start of the stream may have occurred before we started to process the stream. The end of the stream is at some unknown point in the future. Events are ordered by the point in time at which each event occurred. When developing event driven solutions, you will typically see two types of event streams: Event streams whose events are defined and published into a stream as part of a solution. Event streams that connect to a real-time event stream, for example from an IOT device, a voice stream from a telephone system, a video stream, or ship or plane locations from global positioning systems. Command A command , is an instruction to do something . Typically, commands are directed to a particular consumer. The consumer runs the required command or process, and passes back a confirmation to the issuer stating that the command has been processed. Events and Messages There is a long history of messaging in IT systems. You can easily see an event driven solution and events in the context of messaging systems and messages. However, there are different characteristics that are worth considering: Messaging: Messages transport a payload and messages are persisted until consumed. Message consumers are typically directly targeted and related to the producer who cares that the message has been delivered and processed. Events: Events are persisted as a replayable stream history. Event consumers are not tied to the producer. An event is a record of something that has happened and so can't be changed. (You can't change history.) Loose coupling Loose coupling is one of the main benefits of event-driven processing. It allows event producers to emit events without any knowledge about who is going to consume those events. Likewise, event consumers don't need to be aware of the event emitters. Because of this, event consuming modules and event producer modules can be implemented in different languages or use technologies that are different and appropriate for specific jobs. Loosely coupled modules are better suited to evolve independently and, when implemented correctly, result in a significant decrease in system complexity. Loose coupling, however, does not mean \u201cno coupling\u201d. An event consumer consumes events that are useful in achieving its goals and in doing so establishes what data it needs and the type and format of that data. The event producer emits events that it hopes are understood and useful to consumers thus establishing an implicit contract with potential consumers. For example, an event notification in XML format must conform to a certain schema that must be known by both the consumer and the producer. One of the most important things that you can do to reduce coupling in an event-driven system is to reduce the number of distinct event types that flow between modules. To do this you must pay attention to the cohesiveness of those modules. Cohesion Cohesion is the degree to which related things are encapsulated together in the same software module. For the purposes of this EDA discussion, a module is defined as an independently deployable software unit that has high cohesion. Cohesion is strongly related to coupling in the sense that a highly cohesive module communicates less with other modules, thus reducing the number of events most importantly, the number of event types in the system. The less frequently modules interact with each other, the less coupled they are. Achieving cohesion in software while optimizing module size for flexibility and adaptability is difficult, but something to strive for. Designing for cohesion starts with a holistic understanding of the problem domain and good analysis work. Sometimes it must also take into account the constraints of the supporting software environment. Monolithic implementations and implementations that are excessively fine-grained must be avoided.","title":"Concepts"},{"location":"concepts/#concepts","text":"","title":"Concepts"},{"location":"concepts/#events","text":"Events are notifications of change of state. Notifications are issued, or published and interested parties can subscribe and take action on the events. Typically, the issuer of the notification has no knowledge of what action is taken and receives no corresponding feedback that the notification has been processed. Events are notifications of change of state. Typically, events represent the change of state of something of interest to the business. Events are records of something that has happened. Events can't be changed, that is, they are immutable. (We can't change something that has happened in the past).","title":"Events"},{"location":"concepts/#event-streams","text":"An event stream is a continuous unbounded series of events. The start of the stream may have occurred before we started to process the stream. The end of the stream is at some unknown point in the future. Events are ordered by the point in time at which each event occurred. When developing event driven solutions, you will typically see two types of event streams: Event streams whose events are defined and published into a stream as part of a solution. Event streams that connect to a real-time event stream, for example from an IOT device, a voice stream from a telephone system, a video stream, or ship or plane locations from global positioning systems.","title":"Event streams"},{"location":"concepts/#command","text":"A command , is an instruction to do something . Typically, commands are directed to a particular consumer. The consumer runs the required command or process, and passes back a confirmation to the issuer stating that the command has been processed.","title":"Command"},{"location":"concepts/#events-and-messages","text":"There is a long history of messaging in IT systems. You can easily see an event driven solution and events in the context of messaging systems and messages. However, there are different characteristics that are worth considering: Messaging: Messages transport a payload and messages are persisted until consumed. Message consumers are typically directly targeted and related to the producer who cares that the message has been delivered and processed. Events: Events are persisted as a replayable stream history. Event consumers are not tied to the producer. An event is a record of something that has happened and so can't be changed. (You can't change history.)","title":"Events and Messages"},{"location":"concepts/#loose-coupling","text":"Loose coupling is one of the main benefits of event-driven processing. It allows event producers to emit events without any knowledge about who is going to consume those events. Likewise, event consumers don't need to be aware of the event emitters. Because of this, event consuming modules and event producer modules can be implemented in different languages or use technologies that are different and appropriate for specific jobs. Loosely coupled modules are better suited to evolve independently and, when implemented correctly, result in a significant decrease in system complexity. Loose coupling, however, does not mean \u201cno coupling\u201d. An event consumer consumes events that are useful in achieving its goals and in doing so establishes what data it needs and the type and format of that data. The event producer emits events that it hopes are understood and useful to consumers thus establishing an implicit contract with potential consumers. For example, an event notification in XML format must conform to a certain schema that must be known by both the consumer and the producer. One of the most important things that you can do to reduce coupling in an event-driven system is to reduce the number of distinct event types that flow between modules. To do this you must pay attention to the cohesiveness of those modules.","title":"Loose coupling"},{"location":"concepts/#cohesion","text":"Cohesion is the degree to which related things are encapsulated together in the same software module. For the purposes of this EDA discussion, a module is defined as an independently deployable software unit that has high cohesion. Cohesion is strongly related to coupling in the sense that a highly cohesive module communicates less with other modules, thus reducing the number of events most importantly, the number of event types in the system. The less frequently modules interact with each other, the less coupled they are. Achieving cohesion in software while optimizing module size for flexibility and adaptability is difficult, but something to strive for. Designing for cohesion starts with a holistic understanding of the problem domain and good analysis work. Sometimes it must also take into account the constraints of the supporting software environment. Monolithic implementations and implementations that are excessively fine-grained must be avoided.","title":"Cohesion"},{"location":"deployments/eventstreams/","text":"Install IBM Event Streams on ICP (Tested on June 2019 on ibm-eventstreams-dev helm chart 1.2.0 on ICP 3.1.2) You can use the ibm-eventstreams-dev or ibm-eventstreams-prod Helm chart from ICP catalog. The product installation instructions can be found here . Note If you need to upload the tar file for the event streams production (downloaded from IBM passport advantage or other support sites) use the following command: cloudctl catalog load-archive --archive eventstreams.pak.tar.gz As we do not want to rewrite the product documentation, we just want to highlight what was done for our deployment. Our cluster has the following characteristics: Three masters also running ETCD cluster on 3 nodes Three management nodes Three proxy Six worker nodes For worker nodes we need good CPUs and hard disk space. We allocated 12 CPUs - 32 Gb RAM per worker nodes. You need to decide if persistence should be enabled for ZooKeepers and Kafka brokers. Pre allocate one Persistence Volume per Kafka broker and one per ZooKeeper server. If you use dynamic persistence volume provisioning, ensure the expected volumes are present at installation time. Configuration Parameters The following parameters were changed from default settings: Parameter Description Value Kafka.autoCreateTopicsEnable Enable auto-creation of topics true persistence.enabled enable persistent storage for the Kafka brokers true persistence.useDynamicProvisioning dynamically create persistent volume claims true zookeeper.persistence.enabled use persistent storage for the ZooKeeper nodes true zookeeper.persistence.useDynamicProvisioning dynamically create persistent volume claims for the ZooKeeper nodes true proxy.externalAccessEnabled allow external access to Kafka from outside the Kubernetes cluster true The matching server.properties file is under the deployments/eventstreams folder. See parameters description in the product documentation You can get the details of the release with: helm list 'green-events-streams' --tls or access helm detail via ICP console: Here is the helm release details: The figure above shows the following elements: * ConfigMaps for UI, Kafka proxy * The five deployments for each major components: UI, REST, proxy and access controller. Next is the job list which shows what was run during installation. The panel lists also the current network policies: A network policy is a specification of how groups of pods are allowed to communicate with each other and other network endpoints. As soon as there are policies defined, pods will reject connections not allowed by any policies. The pods running in the platform. (One pod was a job) As we can see there are 3 kafka brokers, 3 zookeepers, 2 proxies, 2 access controllers. You can see the pods running on a node using the command: kubectl get pods --all-namespaces --field-selector=spec.nodeName=172.16.50.219 The figure below is for roles, rolebinding and secret as part of the Role Based Access Control settings. The figure below shows the services for zookeeper, Kafka and Event Stream REST api and user interface: The services expose capabilities to external world via nodePort type: * The IBM Event Streams admin console is visible at the port 31253 on the k8s proxy IP address: 172.16.50.227 * The REST api port 30121 * stream proxy port bootstrap: 31348, broker 0: 32489... You get access to the Event Streams admin console by using the IP address of the master / proxy node and the port number of the service, which you can get using the kubectl get service command like: kubectl get svc -n streaming \"green-events-streams-ibm-es-ui-svc\" -o 'jsonpath={.spec.ports[?(@.name==\"admin-ui-https\")].nodePort}' kubectl cluster-info | grep \"catalog\" | awk 'match($0, /([0-9]{1,3}\\.){3}[0-9]{1,3}/) { print substr( $0, RSTART, RLENGTH )}' Here is the admin console home page: To connect an application or tool to this cluster, you will need the address of a bootstrap server, a certificate and an API key. The page to access this information, is on the top right corner: Connect to this cluster : Download certificate and Java truststore files, and the generated API key. A key can apply to all groups or being specific to a group. In Java to leverage the api key the code needs to set the some properties: properties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\"); properties.put(SaslConfigs.SASL_MECHANISM, \"PLAIN\"); properties.put(SaslConfigs.SASL_JAAS_CONFIG, \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"token\\\" password=\\\"\" + env.get(\"KAFKA_APIKEY\") + \"\\\";\"); properties.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\"); properties.put(SslConfigs.SSL_ENABLED_PROTOCOLS_CONFIG, \"TLSv1.2\"); properties.put(SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG, \"HTTPS\"); See code example in ApplicationConfig.java . Some challenges during the installation As presented in the high availability discussion in this note , normally we need 6 worker nodes to avoid allocating zookeeper and kafka servers on the same kubernetes nodes. The community edition installation is permissive on that constraint, so both products could co-exist but in that case, ensure to have enough physical resources. We have seen some Kafka brokers that could not be scheduled because some nodes have taints (can't meet the specs for the stateful set) and the remaining worker nodes don't have enough memory. Getting started application Use the Event Stream Toolbox to download a getting started application we can use to test the deployment and as code base for future Kafka consumer / producer development. One example of the generated app is in this repository under gettingStarted/EDAIEWStarterApp folder, and a description on how to compile, package and run it: see the ./gettingStarted/EDAIEWStarterApp/README.md. The application runs in Liberty at the URL: http://localhost:9080/EDAIESStarterApp/ and delivers a simple user interface splitted into two panels: producer and consumer. The figure below illustrates the fact that the connetion to the broker was not working for a short period of time, so the producer has error, but because of the buffering capabilities, it was able to pace and then as soon as the connection was re-established the consumer started to get the messages. No messages were lost!. We have two solution implementations using Kafka and Event Streams the manufacturing asset analytics and the most recent KC container shipment solution . We recommend using the second implementation. Verifying ICP Kafka installation Once connected to the cluster with kubectl, get the list of pods for the namespace you used to install Kafka or IBM Event Streams: $ kubectl get pods -n streaming NAME READY STATUS RESTARTS green-even-c353-ibm-es-elas-ad8d-0 1/1 Running 0 3d green-even-c353-ibm-es-elas-ad8d-1 1/1 Running 0 3d green-even-c353-ibm-es-kafka-sts-0 4/4 Running 2 3d green-even-c353-ibm-es-kafka-sts-1 4/4 Running 2 3d green-even-c353-ibm-es-kafka-sts-2 4/4 Running 5 3d green-even-c353-ibm-es-zook-c4c0-0 1/1 Running 0 3d green-even-c353-ibm-es-zook-c4c0-1 1/1 Running 0 3d green-even-c353-ibm-es-zook-c4c0-2 1/1 Running 0 3d green-events-streams-ibm-es-access-controller-deploy-7cbf8jjs9n 2/2 Running 0 3d green-events-streams-ibm-es-access-controller-deploy-7cbf8st95z 2/2 Running 0 3d green-events-streams-ibm-es-indexmgr-deploy-6ff759779-c8ddc 1/1 Running 0 3d green-events-streams-ibm-es-proxy-deploy-777d6cf76c-bxjtq 1/1 Running 0 3d green-events-streams-ibm-es-proxy-deploy-777d6cf76c-p8rkc 1/1 Running 0 3d green-events-streams-ibm-es-rest-deploy-547cc6f9b-774xx 3/3 Running 0 3d green-events-streams-ibm-es-ui-deploy-7f9b9c6c6f-kvvs2 3/3 Running 0 3d Select the first pod: green-even-c353-ibm-es-kafka-sts-0 , then execute a bash shell so you can access the Kafka tools: $ kubectl exec green-even-c353-ibm-es-kafka-sts-0 -itn streaming -- bash bash-3.4# cd /opt/Kafka/bin Now you have access to the kafka tools. The most important thing is to get the hostname and port number of the zookeeper server. To do so use the kubectl command: $ kubectl describe pods green-even-c353-ibm-es-zook-c4c0-0 --namespace streaming In the long result get the client port ( ZK_CLIENT_PORT: 2181) information and IP address (IP: 192.168.76.235). Using this information, in the bash shell within the Kafka broker server we can do the following command to get the topics configured. $ ./Kafka-topics.sh --list -zookeeper 192.168.76.235:2181 # We can also use the service name of zookeeper and let k8s DNS resolve the IP address $ ./Kafka-topics.sh --list -zookeeper green-even-c353-ibm-es-zook-c4c0-0.streaming.svc.cluster.local:2181 Using the Event Stream CLI If not done already, you can install the Event Stream CLI on top of IBM cloud CLI by first downloading it from the Event Stream console and then running this command: $ cloudctl plugin install ./es-plugin Here is a simple summary of the possible cloudctl es commands: # Connect to the cluster cloudctl es init # create a topic - default is 3 replicas cloudctl es topic-create streams-plaintext-input cloudctl es topic-create streams-wordcount-output --replication-factor 1 --partitions 1 # list topics cloudctl es topics # delete topic cloudctl es topic-delete streams-plaintext-input Further Readings IBM Event Streams main page IBM Event Streams Product Documentation","title":"Event Streams ICP deployment"},{"location":"deployments/eventstreams/#install-ibm-event-streams-on-icp","text":"(Tested on June 2019 on ibm-eventstreams-dev helm chart 1.2.0 on ICP 3.1.2) You can use the ibm-eventstreams-dev or ibm-eventstreams-prod Helm chart from ICP catalog. The product installation instructions can be found here . Note If you need to upload the tar file for the event streams production (downloaded from IBM passport advantage or other support sites) use the following command: cloudctl catalog load-archive --archive eventstreams.pak.tar.gz As we do not want to rewrite the product documentation, we just want to highlight what was done for our deployment. Our cluster has the following characteristics: Three masters also running ETCD cluster on 3 nodes Three management nodes Three proxy Six worker nodes For worker nodes we need good CPUs and hard disk space. We allocated 12 CPUs - 32 Gb RAM per worker nodes. You need to decide if persistence should be enabled for ZooKeepers and Kafka brokers. Pre allocate one Persistence Volume per Kafka broker and one per ZooKeeper server. If you use dynamic persistence volume provisioning, ensure the expected volumes are present at installation time.","title":"Install IBM Event Streams on ICP"},{"location":"deployments/eventstreams/#configuration-parameters","text":"The following parameters were changed from default settings: Parameter Description Value Kafka.autoCreateTopicsEnable Enable auto-creation of topics true persistence.enabled enable persistent storage for the Kafka brokers true persistence.useDynamicProvisioning dynamically create persistent volume claims true zookeeper.persistence.enabled use persistent storage for the ZooKeeper nodes true zookeeper.persistence.useDynamicProvisioning dynamically create persistent volume claims for the ZooKeeper nodes true proxy.externalAccessEnabled allow external access to Kafka from outside the Kubernetes cluster true The matching server.properties file is under the deployments/eventstreams folder. See parameters description in the product documentation You can get the details of the release with: helm list 'green-events-streams' --tls or access helm detail via ICP console: Here is the helm release details: The figure above shows the following elements: * ConfigMaps for UI, Kafka proxy * The five deployments for each major components: UI, REST, proxy and access controller. Next is the job list which shows what was run during installation. The panel lists also the current network policies: A network policy is a specification of how groups of pods are allowed to communicate with each other and other network endpoints. As soon as there are policies defined, pods will reject connections not allowed by any policies. The pods running in the platform. (One pod was a job) As we can see there are 3 kafka brokers, 3 zookeepers, 2 proxies, 2 access controllers. You can see the pods running on a node using the command: kubectl get pods --all-namespaces --field-selector=spec.nodeName=172.16.50.219 The figure below is for roles, rolebinding and secret as part of the Role Based Access Control settings. The figure below shows the services for zookeeper, Kafka and Event Stream REST api and user interface: The services expose capabilities to external world via nodePort type: * The IBM Event Streams admin console is visible at the port 31253 on the k8s proxy IP address: 172.16.50.227 * The REST api port 30121 * stream proxy port bootstrap: 31348, broker 0: 32489... You get access to the Event Streams admin console by using the IP address of the master / proxy node and the port number of the service, which you can get using the kubectl get service command like: kubectl get svc -n streaming \"green-events-streams-ibm-es-ui-svc\" -o 'jsonpath={.spec.ports[?(@.name==\"admin-ui-https\")].nodePort}' kubectl cluster-info | grep \"catalog\" | awk 'match($0, /([0-9]{1,3}\\.){3}[0-9]{1,3}/) { print substr( $0, RSTART, RLENGTH )}' Here is the admin console home page: To connect an application or tool to this cluster, you will need the address of a bootstrap server, a certificate and an API key. The page to access this information, is on the top right corner: Connect to this cluster : Download certificate and Java truststore files, and the generated API key. A key can apply to all groups or being specific to a group. In Java to leverage the api key the code needs to set the some properties: properties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\"); properties.put(SaslConfigs.SASL_MECHANISM, \"PLAIN\"); properties.put(SaslConfigs.SASL_JAAS_CONFIG, \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"token\\\" password=\\\"\" + env.get(\"KAFKA_APIKEY\") + \"\\\";\"); properties.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\"); properties.put(SslConfigs.SSL_ENABLED_PROTOCOLS_CONFIG, \"TLSv1.2\"); properties.put(SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG, \"HTTPS\"); See code example in ApplicationConfig.java .","title":"Configuration Parameters"},{"location":"deployments/eventstreams/#some-challenges-during-the-installation","text":"As presented in the high availability discussion in this note , normally we need 6 worker nodes to avoid allocating zookeeper and kafka servers on the same kubernetes nodes. The community edition installation is permissive on that constraint, so both products could co-exist but in that case, ensure to have enough physical resources. We have seen some Kafka brokers that could not be scheduled because some nodes have taints (can't meet the specs for the stateful set) and the remaining worker nodes don't have enough memory.","title":"Some challenges during the installation"},{"location":"deployments/eventstreams/#getting-started-application","text":"Use the Event Stream Toolbox to download a getting started application we can use to test the deployment and as code base for future Kafka consumer / producer development. One example of the generated app is in this repository under gettingStarted/EDAIEWStarterApp folder, and a description on how to compile, package and run it: see the ./gettingStarted/EDAIEWStarterApp/README.md. The application runs in Liberty at the URL: http://localhost:9080/EDAIESStarterApp/ and delivers a simple user interface splitted into two panels: producer and consumer. The figure below illustrates the fact that the connetion to the broker was not working for a short period of time, so the producer has error, but because of the buffering capabilities, it was able to pace and then as soon as the connection was re-established the consumer started to get the messages. No messages were lost!. We have two solution implementations using Kafka and Event Streams the manufacturing asset analytics and the most recent KC container shipment solution . We recommend using the second implementation.","title":"Getting started application"},{"location":"deployments/eventstreams/#verifying-icp-kafka-installation","text":"Once connected to the cluster with kubectl, get the list of pods for the namespace you used to install Kafka or IBM Event Streams: $ kubectl get pods -n streaming NAME READY STATUS RESTARTS green-even-c353-ibm-es-elas-ad8d-0 1/1 Running 0 3d green-even-c353-ibm-es-elas-ad8d-1 1/1 Running 0 3d green-even-c353-ibm-es-kafka-sts-0 4/4 Running 2 3d green-even-c353-ibm-es-kafka-sts-1 4/4 Running 2 3d green-even-c353-ibm-es-kafka-sts-2 4/4 Running 5 3d green-even-c353-ibm-es-zook-c4c0-0 1/1 Running 0 3d green-even-c353-ibm-es-zook-c4c0-1 1/1 Running 0 3d green-even-c353-ibm-es-zook-c4c0-2 1/1 Running 0 3d green-events-streams-ibm-es-access-controller-deploy-7cbf8jjs9n 2/2 Running 0 3d green-events-streams-ibm-es-access-controller-deploy-7cbf8st95z 2/2 Running 0 3d green-events-streams-ibm-es-indexmgr-deploy-6ff759779-c8ddc 1/1 Running 0 3d green-events-streams-ibm-es-proxy-deploy-777d6cf76c-bxjtq 1/1 Running 0 3d green-events-streams-ibm-es-proxy-deploy-777d6cf76c-p8rkc 1/1 Running 0 3d green-events-streams-ibm-es-rest-deploy-547cc6f9b-774xx 3/3 Running 0 3d green-events-streams-ibm-es-ui-deploy-7f9b9c6c6f-kvvs2 3/3 Running 0 3d Select the first pod: green-even-c353-ibm-es-kafka-sts-0 , then execute a bash shell so you can access the Kafka tools: $ kubectl exec green-even-c353-ibm-es-kafka-sts-0 -itn streaming -- bash bash-3.4# cd /opt/Kafka/bin Now you have access to the kafka tools. The most important thing is to get the hostname and port number of the zookeeper server. To do so use the kubectl command: $ kubectl describe pods green-even-c353-ibm-es-zook-c4c0-0 --namespace streaming In the long result get the client port ( ZK_CLIENT_PORT: 2181) information and IP address (IP: 192.168.76.235). Using this information, in the bash shell within the Kafka broker server we can do the following command to get the topics configured. $ ./Kafka-topics.sh --list -zookeeper 192.168.76.235:2181 # We can also use the service name of zookeeper and let k8s DNS resolve the IP address $ ./Kafka-topics.sh --list -zookeeper green-even-c353-ibm-es-zook-c4c0-0.streaming.svc.cluster.local:2181","title":"Verifying ICP Kafka installation"},{"location":"deployments/eventstreams/#using-the-event-stream-cli","text":"If not done already, you can install the Event Stream CLI on top of IBM cloud CLI by first downloading it from the Event Stream console and then running this command: $ cloudctl plugin install ./es-plugin Here is a simple summary of the possible cloudctl es commands: # Connect to the cluster cloudctl es init # create a topic - default is 3 replicas cloudctl es topic-create streams-plaintext-input cloudctl es topic-create streams-wordcount-output --replication-factor 1 --partitions 1 # list topics cloudctl es topics # delete topic cloudctl es topic-delete streams-plaintext-input","title":"Using the Event Stream CLI"},{"location":"deployments/eventstreams/#further-readings","text":"IBM Event Streams main page IBM Event Streams Product Documentation","title":"Further Readings"},{"location":"deployments/eventstreams/Install_Ceph_on_ICP/","text":"Install and configure Ceph for IBM Cloud Private Ceph is open source software designed to provide highly scalable object, block and file-based storage under a unified system. Ceph provides a POSIX-compliant network file system (CephFS) that aims for high performance, large data storage, and maximum compatibility with legacy applications. Rook is an open source orchestrator for distributed storage systems running in cloud native environments. Rook turns storage software into self-managing, self-scaling, and self-healing storage services. It does this by automating deployment, bootstrapping, configuration, provisioning, scaling, upgrading, migration, disaster recovery, monitoring, and resource management. Rook uses the facilities provided by the underlying cloud-native container management, scheduling and orchestration platform to perform its duties. Note The Helm chart ibm-rook-rbd-cluster is used for setting up Ceph Cluster in IBM Cloud Private. Environment A typical IBM Cloud Private Environment includes Boot node, Master node, Management node, Proxy node and Worker nodes. When the Ceph RBD Cluster is used for providing storage for API Connect, any three worker nodes should be configured to have additional raw disks. The following set of systems can be used as reference for building development (non-HA) environment that runs IBM API Connect workload on IBM Cloud Private. Node type Number of nodes CPU Memory (GB) Disk (GB) Boot (FTP Server) 1 8 32 2048 Master 1 8 32 300 Management 1 8 32 300 Proxy 1 4 16 300 Worker 3 8 32 300+500(disk2) Total 7 52 208 3848+1500(disk2) The following set of systems can be used as reference for building production (HA) environment that runs IBM API Connect workload on IBM Cloud Private. Node type Number of nodes CPU Memory (GB) Disk (GB) Boot (FTP Server) 1 8 32 2048 Master 3 8 32 300 Management 2 8 32 300 Proxy 3 4 16 300 Worker 3 16 64 300+750(disk2) Total 12 108 432 5348+2250(disk2) Note Additional worker nodes will be required when there is a a need to run workloads other than IBM API Connect on IBM Cloud Private. Setup This document covers the setup of Ceph storage using Rook . The following tasks are performed for setting up the Ceph Cluster. Download the required setup files Logon to IBM Cloud Private Cluster Setup Ceph Cluster Verify Ceph cluster Troubleshooting Ceph setup 1. Download the required setup files Note The following files are required for installing ibm-rook-rbd-cluster chart and setting up Ceph cluster login.sh - Utility for logging onto IBM Cloud Private ibm-rook-rbd-cluster-0.8.3.tgz - IBM Chart for Rook RBD Cluster ceph-values.yaml - Sample values.yaml for installing Ceph Cluster rook-ceph-cluster-role-binding.yaml - ClusterRoleBinding for the service account rook-ceph-cluster rook-ceph-operator-values.yaml - Sample values.yaml for installing rook operator rook-cluster-role.yaml - ClusterRole for the resource rook-privileged rook-pod-security-policy.yaml - Define PodSecurityPolicy rook-privileged setup.sh - Utility for setting up Ceph Cluster status.sh - Utility for verifying Ceph Cluster cleanup.sh - Utility for cleaning up Ceph Cluster 2. Logon to IBM Cloud Private Cluster The script login.sh can be run to login to IBM Cloud Private Cluster. !! note The script should be updated to include the correct value for CLUSTER_NAME . Sample run of the login script is as follows: 3. Setup Ceph Cluster Step #1 Update the ceph-values.yaml to match your environment. The file ceph-values.yaml needs to be updated to list the IP address of the storage node within the IBM Cloud Private cluster. ... # # UPDATE VARIABLES TO MATCH THE ENVIRONMENT # nodes: - name: \"X.X.X.X\" devices: - name: \"DISK_NAME\" - name: \"Y.Y.Y.Y\" devices: - name: \"DISK_NAME\" - name: \"Z.Z.Z.Z\" devices: - name: \"DISK_NAME\" ... ... Step #2 Modify and run the setup script to install Rook Operator chart and the IBM Rook RBD Cluster chart The contents of the script setup.sh is as follows: # # UPDATE VARIABLES TO MATCH THE ENVIRONMENT # # Define the location of images IMAGE_DIR=/DIRECTORY_HAVING_IMAGES ... Note The script should be updated to include the correct location for IMAGE_DIR that has the location where the chart ibm-rook-rbd-cluster-0.8.3.tgz is downloaded and unzipped. The output of Ceph install is listed below for reference: ceph_install.log 4. Verify Ceph cluster The script status.sh can be run to check if Ceph cluster is working as expected. The contents of the script status.sh is as follows: ./deployments/ceph/status.sh Expected output is listed below. 5. Troubleshooting Ceph setup 5.1 Steps for reseting an used disk It is possible that sometimes OSD pods does't start up even though the OSD prepare jobs have completed successfully. It could happen when the device you have specified does not have a raw disk and the device name you have listed was used for other storage like GlusterFS cluster. In such case the following commands can be run to collect the Logical Volume group ID and Physical volume and remove it fully so that the raw disk is made available for the Ceph cluster. pvs pvdisplay vgremove LOGIOCAL_VOLUME_GROUP_ID -y pvremove PHYSICAL_VOLUME The output of the aforesaid commands is listed below. [root@rsun-rhel-glusterfs03 ~]# pvs PV VG Fmt Attr PSize PFree /dev/sda2 rhel lvm2 a-- 39.00g 0 /dev/sdb vg_687894352b254c630b291bf094a8d43d lvm2 a-- 499.87g 499.87g /dev/sdc rhel lvm2 a-- 500.00g 0 [root@rsun-rhel-glusterfs03 ~]# pvdisplay --- Physical volume --- PV Name /dev/sdb VG Name vg_687894352b254c630b291bf094a8d43d PV Size 500.00 GiB / not usable 132.00 MiB Allocatable yes PE Size 4.00 MiB Total PE 127967 Free PE 127967 Allocated PE 0 PV UUID v6xOuh-M2ot-oXfl-IWyf-TnYL-nX3a-kzqizN --- Physical volume --- PV Name /dev/sda2 VG Name rhel PV Size 39.00 GiB / not usable 3.00 MiB Allocatable yes (but full) PE Size 4.00 MiB Total PE 9983 Free PE 0 Allocated PE 9983 PV UUID tNjUif-RlBT-kdDn-PWwE-LHlq-3w9O-65Hlph --- Physical volume --- PV Name /dev/sdc VG Name rhel PV Size 500.00 GiB / not usable 4.00 MiB Allocatable yes (but full) PE Size 4.00 MiB Total PE 127999 Free PE 0 Allocated PE 127999 PV UUID 7CXpz5-95hb-0WAC-3Efe-XrY1-s6E6-dqLasC [root@rsun-rhel-glusterfs03 ~]# vgremove vg_687894352b254c630b291bf094a8d43d -y Volume group \"vg_687894352b254c630b291bf094a8d43d\" successfully removed [root@rsun-rhel-glusterfs03 ~]# pvremove /dev/sdb Labels on physical volume \"/dev/sdb\" successfully wiped. 5.2 Steps for uninstalling the rook-ceph setup Step #1 The script cleanup.sh can be run to remove the Ceph setup completely. ./deployments/ceph/cleanup.sh Step #2 Remove the contents of the temporary directory used by rook: /var/lib/rook The following command should run on all the worker nodes: rm -fr /var/lib/rook Ceph Cluster Management The following links has additional details on how to diagnose, troubleshoot, monitor and report Ceph cluster storage: https://github.com/rook/rook/tree/master/Documentation https://github.com/rook/rook/blob/master/Documentation/common-issues.md#troubleshooting-techniques https://sysdig.com/blog/monitor-ceph-top-5-metrics-watch/ https://tracker.ceph.com/projects/ceph/wiki/10_Commands_Every_Ceph_Administrator_Should_Know https://sabaini.at/pages/ceph-cheatsheet.html The Ceph Monitor pod can be attached using the following command: kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l \"app=rook-ceph-mon\" -o jsonpath='{.items[0].metadata.name}') bash After being attached to the Ceph Monitor pod, the following commands can be run which provides status and statistics of the Ceph Cluster . ceph health ceph status ceph df ceph osd stat ceph osd tree ceph osd df ceph osd df tree ceph osd perf ceph osd pool stats ceph osd status ceph osd utilization ceph auth list ceph quorum_status ceph mon_status ceph mon dump ceph pg dump ceph pg stat The following link has details on how to add and remove Ceph storage: https://github.com/rook/rook/blob/master/design/cluster-update.md The following link can be used as reference for backing up and restoring the images stored in the Ceph Pool. https://nicksabine.com/post/ceph-backup/ Related commands are: rbd ls -p replicapool rbd export rbd import The aforesaid commands can be run after being attached to the Ceph Monitor pod.","title":"Ceph deployment on ICP"},{"location":"deployments/eventstreams/Install_Ceph_on_ICP/#install-and-configure-ceph-for-ibm-cloud-private","text":"Ceph is open source software designed to provide highly scalable object, block and file-based storage under a unified system. Ceph provides a POSIX-compliant network file system (CephFS) that aims for high performance, large data storage, and maximum compatibility with legacy applications. Rook is an open source orchestrator for distributed storage systems running in cloud native environments. Rook turns storage software into self-managing, self-scaling, and self-healing storage services. It does this by automating deployment, bootstrapping, configuration, provisioning, scaling, upgrading, migration, disaster recovery, monitoring, and resource management. Rook uses the facilities provided by the underlying cloud-native container management, scheduling and orchestration platform to perform its duties. Note The Helm chart ibm-rook-rbd-cluster is used for setting up Ceph Cluster in IBM Cloud Private.","title":"Install and configure Ceph for IBM Cloud Private"},{"location":"deployments/eventstreams/Install_Ceph_on_ICP/#environment","text":"A typical IBM Cloud Private Environment includes Boot node, Master node, Management node, Proxy node and Worker nodes. When the Ceph RBD Cluster is used for providing storage for API Connect, any three worker nodes should be configured to have additional raw disks. The following set of systems can be used as reference for building development (non-HA) environment that runs IBM API Connect workload on IBM Cloud Private. Node type Number of nodes CPU Memory (GB) Disk (GB) Boot (FTP Server) 1 8 32 2048 Master 1 8 32 300 Management 1 8 32 300 Proxy 1 4 16 300 Worker 3 8 32 300+500(disk2) Total 7 52 208 3848+1500(disk2) The following set of systems can be used as reference for building production (HA) environment that runs IBM API Connect workload on IBM Cloud Private. Node type Number of nodes CPU Memory (GB) Disk (GB) Boot (FTP Server) 1 8 32 2048 Master 3 8 32 300 Management 2 8 32 300 Proxy 3 4 16 300 Worker 3 16 64 300+750(disk2) Total 12 108 432 5348+2250(disk2) Note Additional worker nodes will be required when there is a a need to run workloads other than IBM API Connect on IBM Cloud Private.","title":"Environment"},{"location":"deployments/eventstreams/Install_Ceph_on_ICP/#setup","text":"This document covers the setup of Ceph storage using Rook . The following tasks are performed for setting up the Ceph Cluster. Download the required setup files Logon to IBM Cloud Private Cluster Setup Ceph Cluster Verify Ceph cluster Troubleshooting Ceph setup","title":"Setup"},{"location":"deployments/eventstreams/Install_Ceph_on_ICP/#1-download-the-required-setup-files","text":"Note The following files are required for installing ibm-rook-rbd-cluster chart and setting up Ceph cluster login.sh - Utility for logging onto IBM Cloud Private ibm-rook-rbd-cluster-0.8.3.tgz - IBM Chart for Rook RBD Cluster ceph-values.yaml - Sample values.yaml for installing Ceph Cluster rook-ceph-cluster-role-binding.yaml - ClusterRoleBinding for the service account rook-ceph-cluster rook-ceph-operator-values.yaml - Sample values.yaml for installing rook operator rook-cluster-role.yaml - ClusterRole for the resource rook-privileged rook-pod-security-policy.yaml - Define PodSecurityPolicy rook-privileged setup.sh - Utility for setting up Ceph Cluster status.sh - Utility for verifying Ceph Cluster cleanup.sh - Utility for cleaning up Ceph Cluster","title":"1. Download the required setup files"},{"location":"deployments/eventstreams/Install_Ceph_on_ICP/#2-logon-to-ibm-cloud-private-cluster","text":"The script login.sh can be run to login to IBM Cloud Private Cluster. !! note The script should be updated to include the correct value for CLUSTER_NAME . Sample run of the login script is as follows:","title":"2. Logon to IBM Cloud Private Cluster"},{"location":"deployments/eventstreams/Install_Ceph_on_ICP/#3-setup-ceph-cluster","text":"Step #1 Update the ceph-values.yaml to match your environment. The file ceph-values.yaml needs to be updated to list the IP address of the storage node within the IBM Cloud Private cluster. ... # # UPDATE VARIABLES TO MATCH THE ENVIRONMENT # nodes: - name: \"X.X.X.X\" devices: - name: \"DISK_NAME\" - name: \"Y.Y.Y.Y\" devices: - name: \"DISK_NAME\" - name: \"Z.Z.Z.Z\" devices: - name: \"DISK_NAME\" ... ... Step #2 Modify and run the setup script to install Rook Operator chart and the IBM Rook RBD Cluster chart The contents of the script setup.sh is as follows: # # UPDATE VARIABLES TO MATCH THE ENVIRONMENT # # Define the location of images IMAGE_DIR=/DIRECTORY_HAVING_IMAGES ... Note The script should be updated to include the correct location for IMAGE_DIR that has the location where the chart ibm-rook-rbd-cluster-0.8.3.tgz is downloaded and unzipped. The output of Ceph install is listed below for reference: ceph_install.log","title":"3. Setup Ceph Cluster"},{"location":"deployments/eventstreams/Install_Ceph_on_ICP/#4-verify-ceph-cluster","text":"The script status.sh can be run to check if Ceph cluster is working as expected. The contents of the script status.sh is as follows: ./deployments/ceph/status.sh Expected output is listed below.","title":"4. Verify Ceph cluster"},{"location":"deployments/eventstreams/Install_Ceph_on_ICP/#5-troubleshooting-ceph-setup","text":"","title":"5. Troubleshooting Ceph setup"},{"location":"deployments/eventstreams/Install_Ceph_on_ICP/#51-steps-for-reseting-an-used-disk","text":"It is possible that sometimes OSD pods does't start up even though the OSD prepare jobs have completed successfully. It could happen when the device you have specified does not have a raw disk and the device name you have listed was used for other storage like GlusterFS cluster. In such case the following commands can be run to collect the Logical Volume group ID and Physical volume and remove it fully so that the raw disk is made available for the Ceph cluster. pvs pvdisplay vgremove LOGIOCAL_VOLUME_GROUP_ID -y pvremove PHYSICAL_VOLUME The output of the aforesaid commands is listed below. [root@rsun-rhel-glusterfs03 ~]# pvs PV VG Fmt Attr PSize PFree /dev/sda2 rhel lvm2 a-- 39.00g 0 /dev/sdb vg_687894352b254c630b291bf094a8d43d lvm2 a-- 499.87g 499.87g /dev/sdc rhel lvm2 a-- 500.00g 0 [root@rsun-rhel-glusterfs03 ~]# pvdisplay --- Physical volume --- PV Name /dev/sdb VG Name vg_687894352b254c630b291bf094a8d43d PV Size 500.00 GiB / not usable 132.00 MiB Allocatable yes PE Size 4.00 MiB Total PE 127967 Free PE 127967 Allocated PE 0 PV UUID v6xOuh-M2ot-oXfl-IWyf-TnYL-nX3a-kzqizN --- Physical volume --- PV Name /dev/sda2 VG Name rhel PV Size 39.00 GiB / not usable 3.00 MiB Allocatable yes (but full) PE Size 4.00 MiB Total PE 9983 Free PE 0 Allocated PE 9983 PV UUID tNjUif-RlBT-kdDn-PWwE-LHlq-3w9O-65Hlph --- Physical volume --- PV Name /dev/sdc VG Name rhel PV Size 500.00 GiB / not usable 4.00 MiB Allocatable yes (but full) PE Size 4.00 MiB Total PE 127999 Free PE 0 Allocated PE 127999 PV UUID 7CXpz5-95hb-0WAC-3Efe-XrY1-s6E6-dqLasC [root@rsun-rhel-glusterfs03 ~]# vgremove vg_687894352b254c630b291bf094a8d43d -y Volume group \"vg_687894352b254c630b291bf094a8d43d\" successfully removed [root@rsun-rhel-glusterfs03 ~]# pvremove /dev/sdb Labels on physical volume \"/dev/sdb\" successfully wiped.","title":"5.1 Steps for reseting an used disk"},{"location":"deployments/eventstreams/Install_Ceph_on_ICP/#52-steps-for-uninstalling-the-rook-ceph-setup","text":"Step #1 The script cleanup.sh can be run to remove the Ceph setup completely. ./deployments/ceph/cleanup.sh Step #2 Remove the contents of the temporary directory used by rook: /var/lib/rook The following command should run on all the worker nodes: rm -fr /var/lib/rook","title":"5.2 Steps for uninstalling the rook-ceph setup"},{"location":"deployments/eventstreams/Install_Ceph_on_ICP/#ceph-cluster-management","text":"The following links has additional details on how to diagnose, troubleshoot, monitor and report Ceph cluster storage: https://github.com/rook/rook/tree/master/Documentation https://github.com/rook/rook/blob/master/Documentation/common-issues.md#troubleshooting-techniques https://sysdig.com/blog/monitor-ceph-top-5-metrics-watch/ https://tracker.ceph.com/projects/ceph/wiki/10_Commands_Every_Ceph_Administrator_Should_Know https://sabaini.at/pages/ceph-cheatsheet.html The Ceph Monitor pod can be attached using the following command: kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l \"app=rook-ceph-mon\" -o jsonpath='{.items[0].metadata.name}') bash After being attached to the Ceph Monitor pod, the following commands can be run which provides status and statistics of the Ceph Cluster . ceph health ceph status ceph df ceph osd stat ceph osd tree ceph osd df ceph osd df tree ceph osd perf ceph osd pool stats ceph osd status ceph osd utilization ceph auth list ceph quorum_status ceph mon_status ceph mon dump ceph pg dump ceph pg stat The following link has details on how to add and remove Ceph storage: https://github.com/rook/rook/blob/master/design/cluster-update.md The following link can be used as reference for backing up and restoring the images stored in the Ceph Pool. https://nicksabine.com/post/ceph-backup/ Related commands are: rbd ls -p replicapool rbd export rbd import The aforesaid commands can be run after being attached to the Ceph Monitor pod.","title":"Ceph Cluster Management"},{"location":"deployments/kafka/","text":"Kafka Deployment We are proposing three deployment approaches: * Using IBM Event Streams (See separate note ) * Using Kafka on development environment, mostly developer workstation * Using IBM Cloud private for production We are defining two types of manifests, one set for development environment and one for production. The manifests and scripts are under each deployment folders. Development For kafka the manifests are in this project under the deployments/kafka/dev folder. We are using the google image: gcr.io/google_samples/k8skafka:v1 . We tested on MacOS with Docker Edge and Kubernetes. We are also providing scripts to deploy Kafka: $ pwd > deployments/kafka $ ./deployKafka.sh $ kubectl get pods -n greencompute NAME READY STATUS RESTARTS AGE gc-kafka-0 1/1 Running 0 2m gc-zookeeper-57dc5679bb-bh29q 1/1 Running 0 10m Verifying Kafka is connected to zookeeper The goal is to connect to the kafka running container and use the scripts inside kafka bin folder: # connect to the running container: $ kubectl exec -ti gc-kafka-0 /bin/bash -n greencompute # next is the prompt inside the container: kafka@gc-kafka-0:/$ cd /opt/kafka/bin # for example create a topic for testing kafka@gc-kafka-0:/$./kafka-topics.sh --create --zookeeper gc-client-zookeeper-svc.greencompute.svc.cluster.local:2181 --replication-factor 1 --partitions 1 --topic text-topic This previous command create a text-topic and to verify the configured existing topics use the command (inside the container): kafka@gc-kafka-0:/$./kafka-topics.sh --list --zookeeper gc-client-zookeeper-svc.greencompute.svc.cluster.local:2181 The URL of the zookeeper matches the hostname defined when deploying zookeeper service (see installing zookeeper note ): kubectl describe svc gc-client-zookeeper-svc Verifying pub/sub works with text messages Two scripts exist in the scripts folder in this repository. Those scripts are using kafkacat tool from Confluent. You need to add the following in your hostname resolution configuration (DNS or /etc/hosts), matching you IP address of your laptop. 192.168.1.89 gc-kafka-0.gc-kafka-hl-svc.greencompute.svc.cluster.local Start the consumer in a terminal window ./scripts/consumetext.sh And start the producer in a second terminal: ./script/producetext.sh You should see the text: try to send some text to the text-topic Let see... % Reached end of topic text-topic [0] at offset 3 Run Kafka in Docker On Linux If you run on a linux operating system, you can use the Spotify Kafka image from dockerhub as it includes Zookeeper and Kafka in a single image. It is started in background (-d), named \" Kafka \" and mounting scripts folder to /scripts: docker run -d -p 2181:2181 -p 9092:9092 -v `pwd`:/scripts --env ADVERTISED_HOST=`docker-machine ip \\`docker-machine active\\`` --name kafka --env ADVERTISED_PORT=9092 spotify/kafka Then remote connect to the docker container to open a bash shell: docker exec -ti kafka/bin/bash Create a topic: it uses zookeeper as a backend to persist partition within the topic. In this deployment zookeeper and Kafka are running on the localhost inside the container. So port 2181 is the client port for zookeeper. cd /opt/kafka/bin ./kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic mytopic ./kafka-topics.sh --list --zookeeper localhost:2181 We have done shell scripts for you to do those command and test your local Kafka . The scripts are under ../scripts/kafka createtopic.sh listtopic.sh sendText.sh Send a multiple lines message on mytopic topic- open this one in one terminal. consumeMessage.sh Connect to the topic to get messages. and this second in another terminal. Considerations One major requirement to address which impacts kubernetes Kafka Services configuration and Kafka Broker server configuration is to assess remote access need: do we need to have applications not deployed on Kubernetes that should push or consume message to/from topics defined in the Kafka Brokers running in pods. Normally the answer should be yes as all deployments are Hybrid cloud per nature. As the current client API is doing its own load balancing between brokers we will not be able to use ingress or dynamic node port allocation. Let explain by starting to review Java code to access brokers. The properties needed to access public static String BOOTSTRAP_SERVERS = \"172.16.40.133:32224,172.16.40.137:32224,172.16.40.135:32224\"; Properties properties = new Properties(); properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS); kafkaProducer = new KafkaProducer<>(properties); .... To connect to broker their addresses and port numbers need to be specified. This information should come from external properties file, but the code above is for illustration. The problem is that once deployed in Kubernetes, Kafka broker runs as pod so have dynamic port numbers if we expose a service using NodePort, and the IP address may change overtime while pod are scheduled to Node. The list of brokers need to be in the format: : , : , : . So host list, without port number will not work, forbidden the use of virtual host name defined with Ingress manifest and managed by Kubernetes ingress proxy. An external load balancer will not work too. Here is an example of return message when the broker list is not set right: Connection to node -1 could not be established. Broker may not be available . There are two options to support remote connection: implement a proxy, deployed inside the Kubernetes cluster, with 3 or 5 hostnames and port to expose the brokers, or use static NodePort. As of now for development we used NodePort: apiVersion: v1 kind: Service metadata: labels: app: gc-kafka name: gc-kafka-svc spec: type: NodePort ports: - name: kafka-port port: 32224 nodePort: 32224 targetPort: 32224 selector: app: gc-kafka So we use a port number for internal and external communication. In statefulset we use a google created tool to start the kafka server and set parameters to override the default the conf/server.properties . command: - \"exec kafka-server-start.sh /opt/kafka/config/server.properties --override broker.id=${HOSTNAME##*-} \\ --override listeners=PLAINTEXT://:32224 \\ When consumer or producer connect to a broker in the list there are some messages exchanged, like getting the cluster ID and the endpoint to be used which corresponds to a virtual DNS name of the exposed service: gc-kafka-0.gc-kafka-hl-svc.greencompute.svc.cluster.local : INFO org.apache.kafka.clients.Metadata - Cluster ID: 4qlnD1e-S8ONpOkIOGE8mg INFO o.a.k.c.c.i.AbstractCoordinator - [Consumer clientId=consumer-1, groupId=b6e69280-aa7f-47d2-95f5-f69a8f86b967] Discovered group coordinator gc-kafka-0.gc-kafka-hl-svc.greencompute.svc.cluster.local:32224 (id: 2147483647 rack: null) So the code may not have this entry defined in the DNS. I used /etc/hosts to map it to K8s Proxy IP address. Also the port number return is the one specified in the server configuration, it has to be one Kubernetes and Calico set in the accepted range and exposed on each host of the cluster. With that connection can be established. Verifying deployment We can use the tools delivered with Kafka by using the very helpful kubectl exec command. Validate the list of topics from the developer's workstation using the command: $ kubectl exec -ti gc-Kafka-0 -- bash -c \"kafka-topics.sh --list --zookeeper gc-srv-zookeeper-svc.greencompute.svc.cluster.local:2181 \" or Kafka-topics.sh --describe --topic text-topic --zookeeper gc-srv-zookeeper-svc.greencompute.svc.cluster.local:2181 start the consumer from the developer's workstation kubectl get pods | grep gc-Kafka kubectl exec gc-Kafka-0 -- bash -c \"Kafka-console-consumer.sh --bootstrap-server localhost:9093 --topic test-topic --from-beginning\" the script deployment/Kafka/consumetext.sh executes those commands. As we run in the Kafka broker the host is localhost and the port number is the headless service one. start a text producer Using the same approach we can use broker tool: $ kubectl exec gc-Kafka-0 -- bash -c \"/opt/Kafka/bin/Kafka-console-producer.sh --broker-list localhost:9093 --topic test-topic << EOB this is a message for you and this one too but this one... I m not sure EOB\" Next steps... do pub/sub message using remote IP and port from remote server. The code is in this project . Troubleshooting For ICP troubleshooting see this centralized note Assess the list of Topics # remote connect to the Kafka pod and open a bash: kubectl exec -ti Kafka-786975b994-9m8n2 bash bash-4.4# ./Kafka-topics.sh --zookeeper 192.168.1.89:30181 --list Purge a topic with bad message: delete and recreate it ./Kafka-topics.sh --zookeeper 192.168.1.89:30181 --delete --topic test-topic ./Kafka-topics.sh --zookeeper 192.168.1.89:30181 --create --replication-factor 1 --partitions 1 --topic test-topic Timeout while sending message to topic The error message may look like: Error when sending message to topic test-topic with key: null, value: 12 bytes with error: (org.apache.Kafka.clients.producer.internals.ErrorLoggingCallback) org.apache.Kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms. This can be linked to a lot of different issues, but it is a communication problem. Assess the following: port number exposed match the broker's one. host name known by the server running the producer or consumer code.","title":"Kafka deployment"},{"location":"deployments/kafka/#kafka-deployment","text":"We are proposing three deployment approaches: * Using IBM Event Streams (See separate note ) * Using Kafka on development environment, mostly developer workstation * Using IBM Cloud private for production We are defining two types of manifests, one set for development environment and one for production. The manifests and scripts are under each deployment folders.","title":"Kafka Deployment"},{"location":"deployments/kafka/#development","text":"For kafka the manifests are in this project under the deployments/kafka/dev folder. We are using the google image: gcr.io/google_samples/k8skafka:v1 . We tested on MacOS with Docker Edge and Kubernetes. We are also providing scripts to deploy Kafka: $ pwd > deployments/kafka $ ./deployKafka.sh $ kubectl get pods -n greencompute NAME READY STATUS RESTARTS AGE gc-kafka-0 1/1 Running 0 2m gc-zookeeper-57dc5679bb-bh29q 1/1 Running 0 10m","title":"Development"},{"location":"deployments/kafka/#verifying-kafka-is-connected-to-zookeeper","text":"The goal is to connect to the kafka running container and use the scripts inside kafka bin folder: # connect to the running container: $ kubectl exec -ti gc-kafka-0 /bin/bash -n greencompute # next is the prompt inside the container: kafka@gc-kafka-0:/$ cd /opt/kafka/bin # for example create a topic for testing kafka@gc-kafka-0:/$./kafka-topics.sh --create --zookeeper gc-client-zookeeper-svc.greencompute.svc.cluster.local:2181 --replication-factor 1 --partitions 1 --topic text-topic This previous command create a text-topic and to verify the configured existing topics use the command (inside the container): kafka@gc-kafka-0:/$./kafka-topics.sh --list --zookeeper gc-client-zookeeper-svc.greencompute.svc.cluster.local:2181 The URL of the zookeeper matches the hostname defined when deploying zookeeper service (see installing zookeeper note ): kubectl describe svc gc-client-zookeeper-svc","title":"Verifying Kafka is connected to zookeeper"},{"location":"deployments/kafka/#verifying-pubsub-works-with-text-messages","text":"Two scripts exist in the scripts folder in this repository. Those scripts are using kafkacat tool from Confluent. You need to add the following in your hostname resolution configuration (DNS or /etc/hosts), matching you IP address of your laptop. 192.168.1.89 gc-kafka-0.gc-kafka-hl-svc.greencompute.svc.cluster.local Start the consumer in a terminal window ./scripts/consumetext.sh And start the producer in a second terminal: ./script/producetext.sh You should see the text: try to send some text to the text-topic Let see... % Reached end of topic text-topic [0] at offset 3","title":"Verifying pub/sub works with text messages"},{"location":"deployments/kafka/#run-kafka-in-docker-on-linux","text":"If you run on a linux operating system, you can use the Spotify Kafka image from dockerhub as it includes Zookeeper and Kafka in a single image. It is started in background (-d), named \" Kafka \" and mounting scripts folder to /scripts: docker run -d -p 2181:2181 -p 9092:9092 -v `pwd`:/scripts --env ADVERTISED_HOST=`docker-machine ip \\`docker-machine active\\`` --name kafka --env ADVERTISED_PORT=9092 spotify/kafka Then remote connect to the docker container to open a bash shell: docker exec -ti kafka/bin/bash Create a topic: it uses zookeeper as a backend to persist partition within the topic. In this deployment zookeeper and Kafka are running on the localhost inside the container. So port 2181 is the client port for zookeeper. cd /opt/kafka/bin ./kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic mytopic ./kafka-topics.sh --list --zookeeper localhost:2181 We have done shell scripts for you to do those command and test your local Kafka . The scripts are under ../scripts/kafka createtopic.sh listtopic.sh sendText.sh Send a multiple lines message on mytopic topic- open this one in one terminal. consumeMessage.sh Connect to the topic to get messages. and this second in another terminal.","title":"Run Kafka in Docker On Linux"},{"location":"deployments/kafka/#considerations","text":"One major requirement to address which impacts kubernetes Kafka Services configuration and Kafka Broker server configuration is to assess remote access need: do we need to have applications not deployed on Kubernetes that should push or consume message to/from topics defined in the Kafka Brokers running in pods. Normally the answer should be yes as all deployments are Hybrid cloud per nature. As the current client API is doing its own load balancing between brokers we will not be able to use ingress or dynamic node port allocation. Let explain by starting to review Java code to access brokers. The properties needed to access public static String BOOTSTRAP_SERVERS = \"172.16.40.133:32224,172.16.40.137:32224,172.16.40.135:32224\"; Properties properties = new Properties(); properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS); kafkaProducer = new KafkaProducer<>(properties); .... To connect to broker their addresses and port numbers need to be specified. This information should come from external properties file, but the code above is for illustration. The problem is that once deployed in Kubernetes, Kafka broker runs as pod so have dynamic port numbers if we expose a service using NodePort, and the IP address may change overtime while pod are scheduled to Node. The list of brokers need to be in the format: : , : , : . So host list, without port number will not work, forbidden the use of virtual host name defined with Ingress manifest and managed by Kubernetes ingress proxy. An external load balancer will not work too. Here is an example of return message when the broker list is not set right: Connection to node -1 could not be established. Broker may not be available . There are two options to support remote connection: implement a proxy, deployed inside the Kubernetes cluster, with 3 or 5 hostnames and port to expose the brokers, or use static NodePort. As of now for development we used NodePort: apiVersion: v1 kind: Service metadata: labels: app: gc-kafka name: gc-kafka-svc spec: type: NodePort ports: - name: kafka-port port: 32224 nodePort: 32224 targetPort: 32224 selector: app: gc-kafka So we use a port number for internal and external communication. In statefulset we use a google created tool to start the kafka server and set parameters to override the default the conf/server.properties . command: - \"exec kafka-server-start.sh /opt/kafka/config/server.properties --override broker.id=${HOSTNAME##*-} \\ --override listeners=PLAINTEXT://:32224 \\ When consumer or producer connect to a broker in the list there are some messages exchanged, like getting the cluster ID and the endpoint to be used which corresponds to a virtual DNS name of the exposed service: gc-kafka-0.gc-kafka-hl-svc.greencompute.svc.cluster.local : INFO org.apache.kafka.clients.Metadata - Cluster ID: 4qlnD1e-S8ONpOkIOGE8mg INFO o.a.k.c.c.i.AbstractCoordinator - [Consumer clientId=consumer-1, groupId=b6e69280-aa7f-47d2-95f5-f69a8f86b967] Discovered group coordinator gc-kafka-0.gc-kafka-hl-svc.greencompute.svc.cluster.local:32224 (id: 2147483647 rack: null) So the code may not have this entry defined in the DNS. I used /etc/hosts to map it to K8s Proxy IP address. Also the port number return is the one specified in the server configuration, it has to be one Kubernetes and Calico set in the accepted range and exposed on each host of the cluster. With that connection can be established.","title":"Considerations"},{"location":"deployments/kafka/#verifying-deployment","text":"We can use the tools delivered with Kafka by using the very helpful kubectl exec command. Validate the list of topics from the developer's workstation using the command: $ kubectl exec -ti gc-Kafka-0 -- bash -c \"kafka-topics.sh --list --zookeeper gc-srv-zookeeper-svc.greencompute.svc.cluster.local:2181 \" or Kafka-topics.sh --describe --topic text-topic --zookeeper gc-srv-zookeeper-svc.greencompute.svc.cluster.local:2181 start the consumer from the developer's workstation kubectl get pods | grep gc-Kafka kubectl exec gc-Kafka-0 -- bash -c \"Kafka-console-consumer.sh --bootstrap-server localhost:9093 --topic test-topic --from-beginning\" the script deployment/Kafka/consumetext.sh executes those commands. As we run in the Kafka broker the host is localhost and the port number is the headless service one. start a text producer Using the same approach we can use broker tool: $ kubectl exec gc-Kafka-0 -- bash -c \"/opt/Kafka/bin/Kafka-console-producer.sh --broker-list localhost:9093 --topic test-topic << EOB this is a message for you and this one too but this one... I m not sure EOB\" Next steps... do pub/sub message using remote IP and port from remote server. The code is in this project .","title":"Verifying deployment"},{"location":"deployments/kafka/#troubleshooting","text":"For ICP troubleshooting see this centralized note","title":"Troubleshooting"},{"location":"deployments/kafka/#assess-the-list-of-topics","text":"# remote connect to the Kafka pod and open a bash: kubectl exec -ti Kafka-786975b994-9m8n2 bash bash-4.4# ./Kafka-topics.sh --zookeeper 192.168.1.89:30181 --list Purge a topic with bad message: delete and recreate it ./Kafka-topics.sh --zookeeper 192.168.1.89:30181 --delete --topic test-topic ./Kafka-topics.sh --zookeeper 192.168.1.89:30181 --create --replication-factor 1 --partitions 1 --topic test-topic","title":"Assess the list of Topics"},{"location":"deployments/kafka/#timeout-while-sending-message-to-topic","text":"The error message may look like: Error when sending message to topic test-topic with key: null, value: 12 bytes with error: (org.apache.Kafka.clients.producer.internals.ErrorLoggingCallback) org.apache.Kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms. This can be linked to a lot of different issues, but it is a communication problem. Assess the following: port number exposed match the broker's one. host name known by the server running the producer or consumer code.","title":"Timeout while sending message to topic"},{"location":"deployments/postgresql/","text":"Deploying Postgresql to ICP Update 05/10/2019 - ICP 3.2.1 Pre-requisites Access to an ICP cluster with an up to date catalog Once logged to the admin consoler (something like: https://172.16.254.80:8443) go to the Command Line Tools menu and download the IBM Cloud Private CLI. Rename the downloaded file to cloudctl and move it to a folder in your $PATH (e.g. /usr/local/bin/cloudctl) Download the kubeclt CLI that match ICP version. Rename and move the tool to /usr/local/bin/ Download the kubeclt CLI that match ICP version. Rename and move the tool to /usr/local/bin/ Get psql to access the postgresql. Steps Login to the cluster: cloudctl login -a https://172.16.254.80:8443 -u admin -p <passwordyoushouldknow> --skip-ssl-validation When selecting the postgresql tile in the database category of the catalog (https://172.16.254.80:8443/catalog/) the Overview gives some steps to follow, but those are from the product documentation and they may need some update. Below are the specifics we did: For the namespace we use greencompute , so the secret was something like: $ kubectl create secret generic postgresql-pwd-secret --from-literal='postgresql-password=<>' --namespace greencompute secret \"postgresql-pwd-secret\" created Create a persistence volume. You can use HostPath for development purpose, or if you have a NFS or ceph cluster available adapt the CRD file apiVersion: v1, kind: PersistentVolume, metadata: name: posgresql-pv, spec: capacity: storage: 10Gi hostPath: path: /bitnami/postgresql, type: \"\" accessModes: ReadWriteOnce persistentVolumeReclaimPolicy: Retain For NFS use the following changes: spec: nfs: server: path: /bitnami/postgresql As we deploy postgres in a namespace scope, we need to specify an image policy to authorize access to docker.io repository: apiVersion: securityenforcement.admission.cloud.ibm.com/v1beta1 kind: ImagePolicy namespace: greencompute metadata: name: postgresql-image-policy spec: repositories: - name: docker.io/* policy: va: enabled: false save the file as securitypolicies.yml and then run: $ kubectl apply -f securitypolicies.yml -n greencompute $ kubectl describe ImagePolicy postgresql-image-policy -n greencompute Use helm to install the release. Here is an example $ export PSWD=$(k get secret postgresql-pwd-secret -n greencompute -o jsonpath=\"{.data.postgresql-password}\" | base64 --decode; echo) $ helm install stable/postgresql --name postgresql --namespace greencompute --set postgresqlPassword=$PSWD,postgresqlDatabase=postgres --tls Access to the database with psql running locally on your computer In one terminal start a port forwarding using: kubectl port-forward postgresql-postgresql-0 5432:5432 &>> /dev/null & . Now we can connect our local psql CLI to the remote server via a command like: $ psql \"dbname=postgres host=127.0.0.1 user=postgres port=5432 password=$PSWD\" postgres=# \\d containers id | character varying(255) | | not null | brand | character varying(255) | | | capacity | integer | | not null | created_at | timestamp without time zone | | not null | current_city | character varying(255) | | | latitude | double precision | | not null | longitude | double precision | | not null | status | integer | | | type | character varying(255) | | | updated_at | timestamp without time zone | | not null | For more information about the psql tool see this note. Troubleshooting admission webhook \"trust.hooks.securityenforcement.admission.cloud.ibm.com\" denied the request: Deny \"docker.io/bitnami/postgresql:10.7.0\", no matching repositories in ClusterImagePolicy and no ImagePolicies in the \"greencompute\" namespace Be sure to use a ImagePolicy and not a cluster policy when using namespace deployment. Error: release postgresql failed: Internal error occurred: admission webhook \"trust.hooks.securityenforcement.admission.cloud.ibm.com\" denied the request: Deny \"docker.io/bitnami/postgresql:10.7.0\", no matching repositories in the ImagePolicies Be sure to authorize docker.io/* in the ImagePolicy. More Readings ICP 2.1 Postgresql install recipe: older recipeusing the configuration user interface in the ICP console. postgresql helm chart explanation and configuration : a must read. Installing postgresql via Helm Reefer container management microservice using Springboot, kafka and postgresql","title":"Postgresql"},{"location":"deployments/postgresql/#deploying-postgresql-to-icp","text":"Update 05/10/2019 - ICP 3.2.1","title":"Deploying Postgresql to ICP"},{"location":"deployments/postgresql/#pre-requisites","text":"Access to an ICP cluster with an up to date catalog Once logged to the admin consoler (something like: https://172.16.254.80:8443) go to the Command Line Tools menu and download the IBM Cloud Private CLI. Rename the downloaded file to cloudctl and move it to a folder in your $PATH (e.g. /usr/local/bin/cloudctl) Download the kubeclt CLI that match ICP version. Rename and move the tool to /usr/local/bin/ Download the kubeclt CLI that match ICP version. Rename and move the tool to /usr/local/bin/ Get psql to access the postgresql.","title":"Pre-requisites"},{"location":"deployments/postgresql/#steps","text":"Login to the cluster: cloudctl login -a https://172.16.254.80:8443 -u admin -p <passwordyoushouldknow> --skip-ssl-validation When selecting the postgresql tile in the database category of the catalog (https://172.16.254.80:8443/catalog/) the Overview gives some steps to follow, but those are from the product documentation and they may need some update. Below are the specifics we did: For the namespace we use greencompute , so the secret was something like: $ kubectl create secret generic postgresql-pwd-secret --from-literal='postgresql-password=<>' --namespace greencompute secret \"postgresql-pwd-secret\" created Create a persistence volume. You can use HostPath for development purpose, or if you have a NFS or ceph cluster available adapt the CRD file apiVersion: v1, kind: PersistentVolume, metadata: name: posgresql-pv, spec: capacity: storage: 10Gi hostPath: path: /bitnami/postgresql, type: \"\" accessModes: ReadWriteOnce persistentVolumeReclaimPolicy: Retain For NFS use the following changes: spec: nfs: server: path: /bitnami/postgresql As we deploy postgres in a namespace scope, we need to specify an image policy to authorize access to docker.io repository: apiVersion: securityenforcement.admission.cloud.ibm.com/v1beta1 kind: ImagePolicy namespace: greencompute metadata: name: postgresql-image-policy spec: repositories: - name: docker.io/* policy: va: enabled: false save the file as securitypolicies.yml and then run: $ kubectl apply -f securitypolicies.yml -n greencompute $ kubectl describe ImagePolicy postgresql-image-policy -n greencompute Use helm to install the release. Here is an example $ export PSWD=$(k get secret postgresql-pwd-secret -n greencompute -o jsonpath=\"{.data.postgresql-password}\" | base64 --decode; echo) $ helm install stable/postgresql --name postgresql --namespace greencompute --set postgresqlPassword=$PSWD,postgresqlDatabase=postgres --tls Access to the database with psql running locally on your computer In one terminal start a port forwarding using: kubectl port-forward postgresql-postgresql-0 5432:5432 &>> /dev/null & . Now we can connect our local psql CLI to the remote server via a command like: $ psql \"dbname=postgres host=127.0.0.1 user=postgres port=5432 password=$PSWD\" postgres=# \\d containers id | character varying(255) | | not null | brand | character varying(255) | | | capacity | integer | | not null | created_at | timestamp without time zone | | not null | current_city | character varying(255) | | | latitude | double precision | | not null | longitude | double precision | | not null | status | integer | | | type | character varying(255) | | | updated_at | timestamp without time zone | | not null | For more information about the psql tool see this note.","title":"Steps"},{"location":"deployments/postgresql/#troubleshooting","text":"admission webhook \"trust.hooks.securityenforcement.admission.cloud.ibm.com\" denied the request: Deny \"docker.io/bitnami/postgresql:10.7.0\", no matching repositories in ClusterImagePolicy and no ImagePolicies in the \"greencompute\" namespace Be sure to use a ImagePolicy and not a cluster policy when using namespace deployment. Error: release postgresql failed: Internal error occurred: admission webhook \"trust.hooks.securityenforcement.admission.cloud.ibm.com\" denied the request: Deny \"docker.io/bitnami/postgresql:10.7.0\", no matching repositories in the ImagePolicies Be sure to authorize docker.io/* in the ImagePolicy.","title":"Troubleshooting"},{"location":"deployments/postgresql/#more-readings","text":"ICP 2.1 Postgresql install recipe: older recipeusing the configuration user interface in the ICP console. postgresql helm chart explanation and configuration : a must read. Installing postgresql via Helm Reefer container management microservice using Springboot, kafka and postgresql","title":"More Readings"},{"location":"deployments/zookeeper/","text":"Zookeeper Deployment Development deployment uses one zookeeper server. For production the replicas is set to 5 to tolerate one planned and one unplanned failure. The service defines 3 ports: one for the inter-server communication, one for client access and one for leader-election. Persistence volumes are needed to provide durable storage. Better to use network storage like NFS or glusterfs. The zookeeper manifests are defined in this project under the deployments/zookeeper/dev folder. We are using our own docker images and the Dockerfile to build this image is in deployments/zookeeper . The image is already pushed to the Docker Hub under ibmcase account. We are providing a script to install zookeeper as a kubernetes environment. First be sure to be connected to your kubernetes cluster then run the following command: $ pwd > refarch-eda/deployments/zookeeper $ ./deployZoopeeker.sh $ kubectl get pods -n greencompute NAME READY STATUS RESTARTS AGE gc-zookeeper-57dc5679bb-bh29q 1/1 Running 0 1m It creates volume, services and deployment or statefulset. If you want to deploy it in more resilient deployment we provide other manifests under the prod folder. To install: $ ./deployZoopeeker.sh prod Once installed you do not need to reinstall it. We are also delivering a script to remove zookeeper when you are done using it. (./removeZookeeper.sh) When running in production it is better to use separate zookeeper ensemble for each Kafka cluster. Each server should have at least 2 GiB of heap with at least 4 GiB of reserved memory","title":"Zookeeper deployment"},{"location":"deployments/zookeeper/#zookeeper-deployment","text":"Development deployment uses one zookeeper server. For production the replicas is set to 5 to tolerate one planned and one unplanned failure. The service defines 3 ports: one for the inter-server communication, one for client access and one for leader-election. Persistence volumes are needed to provide durable storage. Better to use network storage like NFS or glusterfs. The zookeeper manifests are defined in this project under the deployments/zookeeper/dev folder. We are using our own docker images and the Dockerfile to build this image is in deployments/zookeeper . The image is already pushed to the Docker Hub under ibmcase account. We are providing a script to install zookeeper as a kubernetes environment. First be sure to be connected to your kubernetes cluster then run the following command: $ pwd > refarch-eda/deployments/zookeeper $ ./deployZoopeeker.sh $ kubectl get pods -n greencompute NAME READY STATUS RESTARTS AGE gc-zookeeper-57dc5679bb-bh29q 1/1 Running 0 1m It creates volume, services and deployment or statefulset. If you want to deploy it in more resilient deployment we provide other manifests under the prod folder. To install: $ ./deployZoopeeker.sh prod Once installed you do not need to reinstall it. We are also delivering a script to remove zookeeper when you are done using it. (./removeZookeeper.sh) When running in production it is better to use separate zookeeper ensemble for each Kafka cluster. Each server should have at least 2 GiB of heap with at least 4 GiB of reserved memory","title":"Zookeeper Deployment"},{"location":"dsi/","text":"ODM Decision Service Insight Solution Implementation. The entities are: * Application has a status and a name * Application has one to many services * A service has a status and a name. * A user is uniquely identified by his email address. When one of the service is degraded the application is degraded. Events: * service degraded: with unique service name * service is back on line with unique service name * user y is impacted by application x UNDER construction!","title":"ODM Decision Service Insight Solution Implementation."},{"location":"dsi/#odm-decision-service-insight-solution-implementation","text":"The entities are: * Application has a status and a name * Application has one to many services * A service has a status and a name. * A user is uniquely identified by his email address. When one of the service is degraded the application is degraded. Events: * service degraded: with unique service name * service is back on line with unique service name * user y is impacted by application x UNDER construction!","title":"ODM Decision Service Insight Solution Implementation."},{"location":"eda-usecases/","text":"Event Driven Use Cases Before we look to specific use cases which naturally lead to an event driven approach, we should step back and remind ourselves what being event driven means: Event driven applications and Event Driven Architectures define a style for developing applications or IT solutions Event driven applications are reactive to things which happen or change in the business Event driven applications make decisions and take actions when things happen/change in the business Event driven applications are responsive Event driven application provide a notification interaction style ( Inform uses when something happens ) Event driven applications process continuous streams of events in real time for insights and actions Event Driven applications can be intelligent applications applying AI models in real time with the event stream An Event Driven Architecture and with event driven microservices provide the greatest opportunity for loose coupling which enables greater business agility However being event driven also brings challenges Event driven applications are harder to visualize, trace and debug Event driven application programming is less familiar as is often perceived to be complex Event driven systems are typically not transactional or consistent. With these characteristics in mind, we can look for the right situations where an event driven approaches provides the best fit for the business problem or opportunity. High level Use cases At a high level the following business drivers are typically suited an event driven approach: Use case Why Event Driven Digital Business Transformation The Digital business need to be both reactive and responsive to situations which occur in the business. It must detect, understand and act on events Maximizing customer engagement The success of a digital business requires that customer ( users of digital business applications ) are properly engaged with the business through the digital channel. Engagement comes from providing the most appropriate information, at the most appropriate time, in the most appropriate way. The responsive nature of event driven systems provides an excellent foundation for this engagement. Realizing Situational Business Opportunities Respond in real time to business opportunities which become available just for the moment when certain events or combinations of events ( situations ) occur. Delivering value with connected devices/IOT Connected devices are typically the source of continuous event streams from which we can derive insights and intelligence and can take actions on through applications.The Event driven architecture and event driven applications enable us to derive insights/intelligence from the event streams and to deliver the informed view into the hands of business users in real time. Application Modernization Application modernization typically follows a lift and shift approach to moving the application to a cloud native run times and an extension approach where new capability may be added in a pure cloud native way. By event enabling the legacy application we can develop extensions as event driven cloud native applications with minimal impact and coupling. Intelligent apps, realizing the value of AI The application of AI with techniques such as Machine Learning provides the opportunity to learn and make predictions, and recommendations. Integrating machine learning with an event driven approach means that we can bring that intelligence into our event driven applications and deliver those intelligent insights, recommendations, predictions into the hands of business users in real time at the right time. Business Agility Agility has perhaps become the overruling characteristic the business demands of app development in these days of the digital business. Being able to rapidly and safely change or extend an application to better fit the business opportunity or to extend reach to new opportunities becomes critical. Following an event driven approach and adopting event driven microservices provides the ultimate approach to loose coupling and none disruptive application extension. Reactive, Responsive, and Intelligent Looking across the high level uses cases we can see three themes which really define the characteristics which event driven can bring to our applications: Now lets consider some of these characteristics in practice, with Zoom Air application below: To deliver this capability with minimal impact on existing systems, clearly benefits from an event-driven approach. The unpredictable nature of the events that lead to disruption, become the triggers to take actions ( Reactive ), notifying the passengers of the change in travel plans when they need it ( responsive ) Achieve business agility with event driven microservices Agility has perhaps become the overriding characteristic the digital business demands of modern app development. Being able to rapidly and safely change or extend an application to better fit the business opportunity or to extend reach to new opportunities becomes critical. Following an event driven microservices can provides the ultimate in agility with: Loose coupling between component services None disruptive application extension. Fine grained service scaling With event driven microservices the services become produce and consumer events (notifications ) , where the event payload is the representation of something which has happened which is of significance in the business domain. With this style a micro-service produces events, but it does not need to have knowledge of if or when it will be processed. We are loosely coupled. We can add a new subscriber to an event to take different and additional action which may start a complete new path and raft of new capabilities for the application. We have none disruptive application extension. Events can only enable us to increase agility if we can get the right event to the right place ( consuming micro-service ) at the right time. The cloud native event driven architecture provides the means to do this, to connect events from mobile apps, web apps, legacy apps, IOT devices, directly into the microservices platform and event driven microservices. Events for shared State and AI Events and following an event driven approach requires a change of thinking compared to the data first and data centric view which has underpinned the majority of traditional applications. An event driven approach with an Event Backbone providing a pub/sub communication capability for micro service communication and an event log which enables shared state can bring significant advantages: Where events are notifications of changes in a business domain, with an event driven approach we can easily add new subscriber microservices to add new capabilities to the application as discussed above We can also easily and none intrusively add handlers to feed the changes in the business domain into our big data analytics and machine learning With the event log and event stores we also build up a time sequenced history of changes of state. This complete history can provide greater insights for data scientists when develloping machine learning models","title":"Event Driven Use Cases"},{"location":"eda-usecases/#event-driven-use-cases","text":"Before we look to specific use cases which naturally lead to an event driven approach, we should step back and remind ourselves what being event driven means: Event driven applications and Event Driven Architectures define a style for developing applications or IT solutions Event driven applications are reactive to things which happen or change in the business Event driven applications make decisions and take actions when things happen/change in the business Event driven applications are responsive Event driven application provide a notification interaction style ( Inform uses when something happens ) Event driven applications process continuous streams of events in real time for insights and actions Event Driven applications can be intelligent applications applying AI models in real time with the event stream An Event Driven Architecture and with event driven microservices provide the greatest opportunity for loose coupling which enables greater business agility However being event driven also brings challenges Event driven applications are harder to visualize, trace and debug Event driven application programming is less familiar as is often perceived to be complex Event driven systems are typically not transactional or consistent. With these characteristics in mind, we can look for the right situations where an event driven approaches provides the best fit for the business problem or opportunity.","title":"Event Driven Use Cases"},{"location":"eda-usecases/#high-level-use-cases","text":"At a high level the following business drivers are typically suited an event driven approach: Use case Why Event Driven Digital Business Transformation The Digital business need to be both reactive and responsive to situations which occur in the business. It must detect, understand and act on events Maximizing customer engagement The success of a digital business requires that customer ( users of digital business applications ) are properly engaged with the business through the digital channel. Engagement comes from providing the most appropriate information, at the most appropriate time, in the most appropriate way. The responsive nature of event driven systems provides an excellent foundation for this engagement. Realizing Situational Business Opportunities Respond in real time to business opportunities which become available just for the moment when certain events or combinations of events ( situations ) occur. Delivering value with connected devices/IOT Connected devices are typically the source of continuous event streams from which we can derive insights and intelligence and can take actions on through applications.The Event driven architecture and event driven applications enable us to derive insights/intelligence from the event streams and to deliver the informed view into the hands of business users in real time. Application Modernization Application modernization typically follows a lift and shift approach to moving the application to a cloud native run times and an extension approach where new capability may be added in a pure cloud native way. By event enabling the legacy application we can develop extensions as event driven cloud native applications with minimal impact and coupling. Intelligent apps, realizing the value of AI The application of AI with techniques such as Machine Learning provides the opportunity to learn and make predictions, and recommendations. Integrating machine learning with an event driven approach means that we can bring that intelligence into our event driven applications and deliver those intelligent insights, recommendations, predictions into the hands of business users in real time at the right time. Business Agility Agility has perhaps become the overruling characteristic the business demands of app development in these days of the digital business. Being able to rapidly and safely change or extend an application to better fit the business opportunity or to extend reach to new opportunities becomes critical. Following an event driven approach and adopting event driven microservices provides the ultimate approach to loose coupling and none disruptive application extension.","title":"High level Use cases"},{"location":"eda-usecases/#reactive-responsive-and-intelligent","text":"Looking across the high level uses cases we can see three themes which really define the characteristics which event driven can bring to our applications: Now lets consider some of these characteristics in practice, with Zoom Air application below: To deliver this capability with minimal impact on existing systems, clearly benefits from an event-driven approach. The unpredictable nature of the events that lead to disruption, become the triggers to take actions ( Reactive ), notifying the passengers of the change in travel plans when they need it ( responsive )","title":"Reactive, Responsive, and  Intelligent"},{"location":"eda-usecases/#achieve-business-agility-with-event-driven-microservices","text":"Agility has perhaps become the overriding characteristic the digital business demands of modern app development. Being able to rapidly and safely change or extend an application to better fit the business opportunity or to extend reach to new opportunities becomes critical. Following an event driven microservices can provides the ultimate in agility with: Loose coupling between component services None disruptive application extension. Fine grained service scaling With event driven microservices the services become produce and consumer events (notifications ) , where the event payload is the representation of something which has happened which is of significance in the business domain. With this style a micro-service produces events, but it does not need to have knowledge of if or when it will be processed. We are loosely coupled. We can add a new subscriber to an event to take different and additional action which may start a complete new path and raft of new capabilities for the application. We have none disruptive application extension. Events can only enable us to increase agility if we can get the right event to the right place ( consuming micro-service ) at the right time. The cloud native event driven architecture provides the means to do this, to connect events from mobile apps, web apps, legacy apps, IOT devices, directly into the microservices platform and event driven microservices.","title":"Achieve business agility with event driven microservices"},{"location":"eda-usecases/#events-for-shared-state-and-ai","text":"Events and following an event driven approach requires a change of thinking compared to the data first and data centric view which has underpinned the majority of traditional applications. An event driven approach with an Event Backbone providing a pub/sub communication capability for micro service communication and an event log which enables shared state can bring significant advantages: Where events are notifications of changes in a business domain, with an event driven approach we can easily add new subscriber microservices to add new capabilities to the application as discussed above We can also easily and none intrusively add handlers to feed the changes in the business domain into our big data analytics and machine learning With the event log and event stores we also build up a time sequenced history of changes of state. This complete history can provide greater insights for data scientists when develloping machine learning models","title":"Events for shared State and AI"},{"location":"evt-action/","text":"Taking An Action with Cloud Functions IBM Cloud Functions is a \"Serverless\" compute offering. While one of the appeals of serverless computing is the provision of cost-effective compute time, it also provides a simplified event-driven programming model which is very valuable for event-driven solutions. valuable for event-driven solutions. With Cloud Functions, the process is as follows: Developers write functional logic called actions . Actions can be written in many supported languages including Java, Python, Node, Swift, Go, or other languages. Actions are triggered from events being published to Kafka topics (the event backbone). Cloud Functions brings up the required compute to run the action. Cloud Functions shuts down the server when the action is complete. Cloud Functions automatically scales for event volume and velocity. For event-driven systems, this simple event driven programming model is powerful. It abstracts the complications of event handling and load balancing to ensure that you have enough subscribing consumers ready to handle the velocity of events published through the system. Developers write the code which executes the required business logic. Supporting products IBM Cloud Functions is a commercial service offering version of the Apache Openwhisk project IBM Cloud Functions product offering https://www.ibm.com/cloud/functions Suggested reading Using Cloud functions with event trigger in Kafka https://github.com/IBM/ibm-cloud-functions-message-hub-trigger","title":"Event Actions"},{"location":"evt-action/#taking-an-action-with-cloud-functions","text":"IBM Cloud Functions is a \"Serverless\" compute offering. While one of the appeals of serverless computing is the provision of cost-effective compute time, it also provides a simplified event-driven programming model which is very valuable for event-driven solutions. valuable for event-driven solutions. With Cloud Functions, the process is as follows: Developers write functional logic called actions . Actions can be written in many supported languages including Java, Python, Node, Swift, Go, or other languages. Actions are triggered from events being published to Kafka topics (the event backbone). Cloud Functions brings up the required compute to run the action. Cloud Functions shuts down the server when the action is complete. Cloud Functions automatically scales for event volume and velocity. For event-driven systems, this simple event driven programming model is powerful. It abstracts the complications of event handling and load balancing to ensure that you have enough subscribing consumers ready to handle the velocity of events published through the system. Developers write the code which executes the required business logic.","title":"Taking An Action with Cloud Functions"},{"location":"evt-action/#supporting-products","text":"IBM Cloud Functions is a commercial service offering version of the Apache Openwhisk project IBM Cloud Functions product offering https://www.ibm.com/cloud/functions","title":"Supporting products"},{"location":"evt-action/#suggested-reading","text":"Using Cloud functions with event trigger in Kafka https://github.com/IBM/ibm-cloud-functions-message-hub-trigger","title":"Suggested reading"},{"location":"evt-backbone/","text":"Event Backbone The event backbone is the communication layer in the event driven architecture. It provides the connection between event driven capabilities and in the Cloud Native it becomes the Pub/Sub communication layer for event driven microservices. At this high level we would consider two types of relevant technologies for the event backbone, Message Brokers and Event Logs . Both technology types could be used to achieve the event communication style, with the \"Publish and subscribe\" model however, it is also important to consider other capabilities which are frequently used within event driven solutions: Keeping an Event Log as a time sequenced as it happened recording of events (Source of the truth). Enabling direct replay of events. Enabling Event Sourcing as a way of recording state changes in distributed systems. Enabling programmatic access to the continuous event stream . When viewed across these wider event driven capabilities, an event log style technology can provide a central component which can support all of these capabilities, whereas a message broker would have to be extended with other components. Defining the Event Backbone for the event driven reference architecture For the event driven architecture we defined the following characteristics to be essential for the event backbone Pub/Sub communication. Facilitate many consumers: Shared central \u201csource of truth\u201d. Capability to store events for a given period of time (event log). Provide replay of events from history for evolving application instances. Provide programmatic access to continuous event stream data. Must be highly scalable and resilient to cloud deployment levels. Looking across these capabilities, the potential technologies, the amount of adoption and community activity around the technologies lead us to selecting Kafka as the Open Source technology base for the event backbone. You can read more about Apache Kafka project here https://kafka.apache.org Supporting products The IBM Event Streams offering provides a Kafka service for the Event Backbone. The service is available as a fully managed service within Public cloud and as a supported build for IBM Cloud Private. IBM Event Streams Public Cloud IBM Event Streams Private Cloud See also our own Kafka study article on how to support high availability and how to deploy to your local environment or to a kubernetes cluster like IBM Cloud Private. Deployments In term of event backbone deployment we propose different approaches: IBM Cloud with the Event Streams service . Deployment discussions for the KC solution are in this note IBM Cloud Private Event Streams deployment . Zookeeper deployment and Kafka deployment for ICP. Running locally with docker compose. See this note for details.","title":"Event Backbone"},{"location":"evt-backbone/#event-backbone","text":"The event backbone is the communication layer in the event driven architecture. It provides the connection between event driven capabilities and in the Cloud Native it becomes the Pub/Sub communication layer for event driven microservices. At this high level we would consider two types of relevant technologies for the event backbone, Message Brokers and Event Logs . Both technology types could be used to achieve the event communication style, with the \"Publish and subscribe\" model however, it is also important to consider other capabilities which are frequently used within event driven solutions: Keeping an Event Log as a time sequenced as it happened recording of events (Source of the truth). Enabling direct replay of events. Enabling Event Sourcing as a way of recording state changes in distributed systems. Enabling programmatic access to the continuous event stream . When viewed across these wider event driven capabilities, an event log style technology can provide a central component which can support all of these capabilities, whereas a message broker would have to be extended with other components.","title":"Event Backbone"},{"location":"evt-backbone/#defining-the-event-backbone-for-the-event-driven-reference-architecture","text":"For the event driven architecture we defined the following characteristics to be essential for the event backbone Pub/Sub communication. Facilitate many consumers: Shared central \u201csource of truth\u201d. Capability to store events for a given period of time (event log). Provide replay of events from history for evolving application instances. Provide programmatic access to continuous event stream data. Must be highly scalable and resilient to cloud deployment levels. Looking across these capabilities, the potential technologies, the amount of adoption and community activity around the technologies lead us to selecting Kafka as the Open Source technology base for the event backbone. You can read more about Apache Kafka project here https://kafka.apache.org","title":"Defining the Event Backbone for the event driven reference architecture"},{"location":"evt-backbone/#supporting-products","text":"The IBM Event Streams offering provides a Kafka service for the Event Backbone. The service is available as a fully managed service within Public cloud and as a supported build for IBM Cloud Private. IBM Event Streams Public Cloud IBM Event Streams Private Cloud See also our own Kafka study article on how to support high availability and how to deploy to your local environment or to a kubernetes cluster like IBM Cloud Private.","title":"Supporting products"},{"location":"evt-backbone/#deployments","text":"In term of event backbone deployment we propose different approaches: IBM Cloud with the Event Streams service . Deployment discussions for the KC solution are in this note IBM Cloud Private Event Streams deployment . Zookeeper deployment and Kafka deployment for ICP. Running locally with docker compose. See this note for details.","title":"Deployments"},{"location":"evt-dashboard/","text":"Event Dashboards Supporting Products Code Examples Angular App to present real time event Dashboard BFF UNDER construction!","title":"Event Dashboards"},{"location":"evt-dashboard/#event-dashboards","text":"","title":"Event Dashboards"},{"location":"evt-dashboard/#supporting-products","text":"","title":"Supporting Products"},{"location":"evt-dashboard/#code-examples","text":"Angular App to present real time event Dashboard BFF UNDER construction!","title":"Code Examples"},{"location":"evt-microservices/","text":"Event-driven cloud native apps On cloud-native platforms, microservices are the application architecture of choice. As businesses become event-driven, event driven pattern needs to extend into our microservices application space. This means that your microservices are still doing REST calls to well-known microservice but they must respond to and send out events, or in event-driven terms they need to be both event producers and consumers to enforce strong decoupling. Event backbone - Pub/Sub communication and data sharing for microservices With the adoption of microservices, the focus on synchronous communication between services has increased. Service mesh packages such as Istio help with the management of communication, service discovery, load balancing, and visibility in this synchronous communication environment. With event-driven microservices, the communication point becomes the Pub/Sub layer of the event backbone. By adopting an event-based approach for intercommunication between microservices, the microservices applications are naturally responsive (event-driven). This approach enhances the loose coupling nature of microservices because it decouples producers and consumers. Further, it enables the sharing of data across microservices through the event log. These event style characteristics are increasingly important considerations when you develop microservices style applications. In practical terms microservices applications are a combination of synchronous API-driven, and asynchronous event-driven communication styles. For the implementation point of view a set of established patterns are used, such as Database per Service, Event Sourcing, Command Query Responsibility Segregation, Saga, ... Read more on patterns Supporting products and suggested reading Event backbone IBM Cloud Functions/Openwhisk programming model Using Cloud functions with event trigger in Kafka IBM Cloud Functions product offering Getting Started with Cloud Functions Event driven apps with containers While the serverless approach with Cloud Functions provides a simplified event-based programming model, the majority of microservices applications today are developed for and deployed to a container-based cloud-native stack. Within the cloud-native landscape, Kubernetes is the standard platform for container orchestration, and therefore becomes the base for the container platform in the event-driven architecture. As before, the event backbone is the Pub/Sub communication provider and event log for shared data for the microservices. In this context microservices are developed as direct consumers and producers of events on the backbone via topics. The extra work in this environment is in managing consumer instances to respond to the demand of the event stream. You must determine how many consumer instances need to be running to keep pace with, or always be immediately available to execute, the microservice in response to an arriving event. Supporting products and suggested reading IBM Cloud Private - Kubernetes base container platform IBM Cloud Kubernetes Service Deploy a microservices application on Kubernetes IBM Cloud Kubernetes Service: Manage apps in containers and clusters on cloud","title":"Microservices"},{"location":"evt-microservices/#event-driven-cloud-native-apps","text":"On cloud-native platforms, microservices are the application architecture of choice. As businesses become event-driven, event driven pattern needs to extend into our microservices application space. This means that your microservices are still doing REST calls to well-known microservice but they must respond to and send out events, or in event-driven terms they need to be both event producers and consumers to enforce strong decoupling.","title":"Event-driven cloud native apps"},{"location":"evt-microservices/#event-backbone-pubsub-communication-and-data-sharing-for-microservices","text":"With the adoption of microservices, the focus on synchronous communication between services has increased. Service mesh packages such as Istio help with the management of communication, service discovery, load balancing, and visibility in this synchronous communication environment. With event-driven microservices, the communication point becomes the Pub/Sub layer of the event backbone. By adopting an event-based approach for intercommunication between microservices, the microservices applications are naturally responsive (event-driven). This approach enhances the loose coupling nature of microservices because it decouples producers and consumers. Further, it enables the sharing of data across microservices through the event log. These event style characteristics are increasingly important considerations when you develop microservices style applications. In practical terms microservices applications are a combination of synchronous API-driven, and asynchronous event-driven communication styles. For the implementation point of view a set of established patterns are used, such as Database per Service, Event Sourcing, Command Query Responsibility Segregation, Saga, ... Read more on patterns","title":"Event backbone - Pub/Sub communication and data sharing for microservices"},{"location":"evt-microservices/#supporting-products-and-suggested-reading","text":"Event backbone IBM Cloud Functions/Openwhisk programming model Using Cloud functions with event trigger in Kafka IBM Cloud Functions product offering Getting Started with Cloud Functions","title":"Supporting products and suggested reading"},{"location":"evt-microservices/#event-driven-apps-with-containers","text":"While the serverless approach with Cloud Functions provides a simplified event-based programming model, the majority of microservices applications today are developed for and deployed to a container-based cloud-native stack. Within the cloud-native landscape, Kubernetes is the standard platform for container orchestration, and therefore becomes the base for the container platform in the event-driven architecture. As before, the event backbone is the Pub/Sub communication provider and event log for shared data for the microservices. In this context microservices are developed as direct consumers and producers of events on the backbone via topics. The extra work in this environment is in managing consumer instances to respond to the demand of the event stream. You must determine how many consumer instances need to be running to keep pace with, or always be immediately available to execute, the microservice in response to an arriving event.","title":"Event driven apps with containers"},{"location":"evt-microservices/#supporting-products-and-suggested-reading_1","text":"IBM Cloud Private - Kubernetes base container platform IBM Cloud Kubernetes Service Deploy a microservices application on Kubernetes IBM Cloud Kubernetes Service: Manage apps in containers and clusters on cloud","title":"Supporting products and suggested reading"},{"location":"evt-microservices/ED-patterns/","text":"Understanding event driven microservice patterns Abstract In this article, we are detailing some of the most import event-driven patterns to be used during your microservice implementation and when adopting kafka as an event backbone. Adopting messaging (Pub/Sub) as a microservice communication backbone involves using at least the following patterns: Decompose by subdomain , event driven microservices are still microservices, so we need to find them, and the domain-driven subdomains is a good approach to identify and classify business function and therefore microservices. With the event storming method, aggregates help to find those subdomain of responsibility. Database per service to enforce each service persists data privately and is accessible only via its API. Services are loosely coupled limiting impact to other service when database schema changes. The database technology is selected from business requirements. The implementation of transactions that span multiple services is complex and enforce using the Saga pattern. Queries that goes over multiple entities is a challenge and CQRS represents an interesting solution. Strangler pattern is used to incrementally migrate an existing, monolytic application by replacing a set of features to a microservice but keep both running in parallel. Saga pattern: Microservices publish events when something happens in the scope of their control like an update in the business entities they are responsible for. A microservice interested in other business entities, subscribe to those events and it can update its own states and business entities when receiving such events. Business entity keys needs to be unique, immutable. Event sourcing persists the state of a business entity such an Order as a sequence of state-changing events. Command Query Responsibility Segregation helps to separate queries from commands and help to address queries with cross-microservice boundary. Update 06/2019 - Author: Jerome Boyer Event sourcing Most business applications are state based persistent where any update changes the previous state of business entities. The database keeps the last committed update. But some business application needs to explain how it reaches its current state. It needs to keep history of business facts. Traditional domain oriented implementation builds domain data model mapped to a RDBMS. As an example, in the simple Order model below, the database record will keep the last state of the order, the different addresses and the last ordered items in separate tables. If you need to implement a query that looks at what happened to the order over a time period, you need to change the model and add historical records, basically building a log table. Designing a service to manage the life cycle of this order will, most of the time, add a \"delete operation\" to remove data. But most businesses do not remove data. For legal reason, a business ledger has to include new record(s) to compensate a previous transaction. There is no erasing of previously logged transactions. It is always possible to understand what was done in the past. Most business application needs to keep this capability. Event sourcing persists the state of a business entity, such an Order, as a sequence of state-changing events or \"facts\". When the state of a system changes, an application issues a notification event of the state change. Any interested parties can become consumers of the event and take required actions. The state-change event is immutable stored in an event log or event store in time order. The event log or store becomes the principal source of truth. The system state can be recreated from a point in time by reprocessing the events. The history of state changes becomes an audit record for the business and is often a useful source of data for data scientists to gain insights into the business. The previous order model changes to a time oriented immutable stream of events, organized by key (orderID): You can see the \"removing an item\" in the order is a new event. With this capability, we can count how often a specific product is removed. In some cases, the event sourcing pattern is implemented completely within the event backbone. With Kafka topic and partition are the building blocks for event sourcing. However, you can also consider implementing the pattern with an external event store, which provides optimizations for how the data may be accessed and used. For example IBM Db2 Event store can provide the handlers and event store connected to the backbone and can provide optimization for down stream analytical processing of the data. An event store only needs to store three pieces of information: The type of event or aggregate. The sequence number of the event. The data as a serialized entity. More data can be added to help with diagnosis and audit, but the core functionality only requires a narrow set of fields. This gives rise to a very simple data design that can be heavily optimized for appending and retrieving sequences of records. With a central event logs, producers append events to the log, and consumers read them from an offset (the last committed read). To get the final state of an entity, the consumer needs to replay all the events, which means replaying the changes to the state from the last committed offset or from the last snapshot. When replaying the event, it may be important to avoid generating side effects. A common side effect is to send a notification on state change to other consumers. Sometime it may be too long to replay hundreds of events. In that case we can use snapshot, to capture the current state of an entity, and then replay events from the most recent snapshot. This is an optimization technique not needed for all event sourcing implementations. When state change events are in low volume there is no need for snapshots. Kafka is supporting the event sourcing pattern with the topic and partition . In our reference implementation we are validating event sourcing with Kafka in the Order microservices and specially this set of test cases. The event sourcing pattern is well described in this article on microservices.io . It is a very important pattern for event-driven microservices to microservices data synchronization implementations. See also this event sourcing article from Martin Fowler, where he is also using ship movement examples. Our implementation differs as we are using Kafka topic as event store and use different entities to support the container shipping process: the Orders, ShipLocations, Containers entities... Another use case for event sourcing is related to developers who want to understand what data to fix after a service crashes, for example after having deployed a buggy code. Command sourcing Command sourcing is a similar pattern as the event sourcing one, but the commands that modify the states are persisted instead of the events. This allows commands to be processed asynchronously, which can be relevant when the command execution takes a lot of time. One derived challenge is that the command may be executed multiple times, especially in case of failure. Therefore, it has to be idempotent ( making multiple identical requests has the same effect as making a single request). Finally, there is a need also to perform validation of the command to avoid keeping wrong commands in queue. For example, AddItem command is becoming AddItemValidated , then once persisted to a database it becomes an event as ItemAdded . So mixing command and event sourcing is common. Business transactions are not ACID and span multiple services, they are more a serie of steps, each step is supported by a microservice responsible to update its own entity. We talk about \"eventual data consistency\". The event backbone needs to guarantee that events are delivered at least once and the microservices are responsible to manage their offset from the stream source and deal with inconsistency, by detecting duplicate events. At the microservice level, updating data and emitting event needs to be an atomic operation, to avoid inconsistency if the service crashes after the update to the datasource and before emitting the event. This can be done with an eventTable added to the microservice datasource and an event publisher that reads this table on a regular basis and change the state of the event once published. Another solution is to have a database transaction log reader or miner responsible to publish event on new row added to the log. One other approach to avoid the two-phase commit and inconsistency is to use an Event Store or Event Sourcing pattern to keep track of what is done on the business entity with enough information to rebuild the data state. Events are becoming facts describing state changes done on the business entity. Command Query Responsibility Segregation (CQRS) pattern When doing event sourcing and domain driven design, we event source the aggregates or root entities. Aggregate creates events that are persisted. On top of the simple create, update and read by ID operations, the business requirements want to perform complex queries that can't be answered by a single aggregate. By just using event sourcing to be able to respond to a query like \"what are the orders of a customer\", then we have to rebuild the history of all orders and filter per customer. It is a lot of computation. This is linked to the problem of having conflicting domain models between query and persistence. Command Query Responsibility Segregation, CQRS, separates the \"command\" operations, used to update application state (also named the 'write model'), from the \"query/read\" operations (the 'read model'). Updates are done as state notification events (change of state), and are persisted in the event log/store. On the \"read model\" side, you have the option of persisting the state in different stores optimized for how other applications may query/read the data. The CQRS application pattern is frequently associated with event sourcing. The following figure presents the high level principles: The service exposes CUD operations, some basic Read by Id and then queries APIs. The domain model is separated into write and read models. Combined with Event Sourcing (ES) the write model goes to the event store. Then we have a separate process that consumes those events and build a projection for future queries. The \"write\" part may persist in SQL while the read may use document oriented database with strong indexing and query capabilities. Or use in-memory database. They do not need to be in the same language. With CQRS and ES the projections are retroactive. New query equals implementing new projection and read the events from the beginning of time or the recent snapshot. Read and write models are strongly decoupled and can evolve independently. It is important to note that the 'Command' part can still handle simple queries, primary-key based, like get order by id, or queries that do not involve joins. With this structure, the Read model microservice will most likely consume events from multiple topics to build the data projection based on joining those data. A query, to assess if the cold-chain was respected on the fresh food order shipment, will go to the voyage, container metrics, and order to be able to answer this question. This is where CQRS shines. A second view of the previous diagram presents how we can separate the API definition and management in a API gateway, the Order command and write model has its own microservice, the event sourcing supported by a Kafka topic, and the query - read model as a set of different microservices or event functions as a service: The shipment order microservice is implementing this pattern. Some implementation items to consider: Consistency (ensure the data constraints are respected for each data transaction): CQRS without event sourcing has the same consistency guarantees as the database used to persist data and events. With Event Sourcing the consistency could be different, one for the \"Write\" model and one for the \"Read\" model. On write model strong consistency is important to ensure the current state of the system is correct, so it leverages transaction, lock and sharding. On read side, we need less consistency, as they mostly work on stale data. Locking data on the read operation is not reasonable. Scalability : Separating read and write as two different microservices allows for high availability. Caching at the \"read\" level can be used to increase performance response time, and can be deployed as multiple standalane instances (Pods in kubernetes). It is also possible to separate the query implementations between different services. Functions as service / serverless are good technology choices to implement complex queries. Availability : As the \"write\" model is often strongly consistent, it impacts availability. This is a fact. The read model is eventually consistent so high availability is possible. In case of failure the system disables the writing of data but still be able to read them as they are served by different databases and services. With CQRS the \"write\" model can evolve overtime without impacting the read model, unless the event model changes. It adds some cost by adding more tables to implement the query parts. It allows smaller model, easier to understand. CQRS results in an increased number of objects, with commands, operations, events,... and packaging in deployable components or containers. It adds potentially different type of data sources. It is more complex. Some challenges to always consider: How to support event version management? How much data to keep in the event store (history)? Design data duplication which results to synchronization issues. The CQRS pattern was introduced by Greg Young , and described in Martin Fowler's work on microservices. As soon as we see two arrows from the same component we have to ask ourselves how does it work: the write model has to persist Order in its own database and then sends OrderCreated event to the topic... Should those operations be atomic and controlled with transaction? We are detailing this in next section. The consistency challenge As introduced in previous section there is potentially a problem of data consistency: the command part saves the data into the database and is not able to send the event to the topic, then consumers do not see the new or updated data. With traditional Java service, using JPA and JMS, the save and send operations can be part of the same transaction and both succeed or both failed. With event sourcing pattern, the source of trust is the event source. It acts as a version control system. So the service should start by creating the event (1) and then persists the data into the database, it uses a topic consumer, get the payload from the event (2) and uses this data to save in its local datasource (3). It derives state solely from the events. If it fails to save, it can persist the event to an error log (4) and then it will be possible to trigger the replay, via an admin API and Command Line Interface (5,6), by searching in the topic using this order id to replay the save operation. Here is a diagram to illustrate that process: This implementation brings a problem on the createOrder(order): order operation, as the returned order was supposed to have the order id as unique key, so most likely, a key created by the database... To avoid this we can generate the key by code and enforce this key in the database if the underlying technology supports it. It is important to clearly study the Kafka consumer API and the different parameters on how to support the read offset. We are addressing those implementation best practices in our consumer note. CQRS and Change Data Capture There are other ways to support this dual operations level: There is the open source Debezium tool to help respond to insert, update and delete operations on database and generate event accordingly. It may not work on all database schema. Write the order to the database and in the same transaction write to an event table. Then use a polling to get the events to send to kafka from this event table and delete the row in the table once the event is sent. Use the Change Data Capture from the database transaction log and generate events from this log. The IBM Infosphere CDC product helps to implement this pattern. For more detail about this solution see this product tour . The CQRS implementation using CDC will look like in the following diagram: What is important to note is that the event needs to be flexible on the data payload. We are presenting a event model in the reference implementation. On the view side, updates to the view part need to be idempotent. Delay in the view There is a delay between the data persistence and the availability of the data in the Read model. For most business applications, it is perfectly acceptable. In web based data access most of the data are at stale. When there is a need for the client, calling the query operation, to know if the data is up-to-date, the service can define a versioning strategy. When the order data was entered in a form within a single page application like our kc- user interface , the \"create order\" operation should return the order with its unique key freshly created and the Single Page Application will have the last data. Here is an example of such operation: @POST public Response create(OrderCreate dto) { Order order = new Order(UUID.randomUUID().toString(), dto.getProductID(),...); // ... return Response.ok().entity(order).build() } Schema change What to do when we need to add attribute to event?. So we need to create a versioninig schema for event structure. You need to use flexible schema like json schema, Apache Avro or protocol buffer and may be, add an event adapter (as a function?) to translate between the different event structures. Saga pattern With the adoption of one data source per microservice, there is an interesting challenge on how to support long running transaction cross microservices. With event backbone two phase commit is not an option. Introduced in 1987 by Hector Garcaa-Molrna Kenneth Salem paper the Saga pattern help to support a long running transaction that can be broken up to a collection of sub transactions that can be interleaved any way with other transactions. With microservice each transaction updates data within a single service, each subsequent steps may be triggered by previous completion. The following figure, based on our solution implementation , illustrates those concepts for an order: When the order is created, it can be updated at any time by the user until he/she books it as the final order. As soon as the order is booked, the process needs to allocate the voyage, assigns containers and updates the list of containers to load on the ship. Those actions / commands are chained. The final state (in this schema not in the reality as the process has more steps) is the Order assigned state in the order microservice. SAGA pattern supports two types of implementation: Choreography and Orchestration. With Choreography each service produces and listens to other service\u2019s events and decides if an action should be taken or not. The first service executes a transaction and then publishes an event. It maintains the business entity status, (order.status) to the pending state until it is completed. This event is listened by one or more services which execute local transactions and publish new events. The distributed transaction ends when the last service executes its local transaction or when a service does not publish any events or the event published is not polled by any of the saga\u2019s participants. In case of failure, the source microservice is keeping state and timer to monitor for the completion event. Rolling back a distributed transaction does not come for free. Normally you have to implement another operation/transaction to compensate for what has been done before. With orchestration, one service is responsible to drive each participant on what to do and when. If anything fails, the orchestrator is also responsible for coordinating the rollback by sending commands to each participant to undo the previous operation. Orchestrator is a State Machine where each transformation corresponds to a command or message. Rollbacks are a lot easier when you have an orchestrator to coordinate everything. See also this article from Chris Richardson on the Saga pattern. Strangler pattern Problem How to migrate a monolytics application to microservice without doing a big bang, redeveloping the application from white page. Replacing and rewritting an existing application can be a huge investment. Rewritting a subset of business functions while running current application in parallel may be relevant and reduce risk and velocity of changes. Solution The approach is to use a \"strangler\" interface to dispatch request to new or old features. Existing features to migrate are selected by trying to isolate sub components. One of main challenge is to isolate data store and how the new microservices and the legacy application are accessing the shared data. Continuous data replication can be a solution to propagate write model to read model. Write model will most likely stays on the monolitic application, change data capture can be used, with event backbone to propagate change to read model. The facade needs to be scalable and not a single point of failure. It needs to support new APIs (RESTful) and old API (most likely SOAP). Code References The K Containers shipment use cases provides a supporting EDA example https://github.com/ibm-cloud-architecture/refarch-kc Within K Containers shipment the following are example microservices illustrating some of those patterns https://github.com/ibm-cloud-architecture/refarch-kc-ms https://github.com/ibm-cloud-architecture/refarch-kc-order-ms","title":"Event-driven patterns"},{"location":"evt-microservices/ED-patterns/#understanding-event-driven-microservice-patterns","text":"Abstract In this article, we are detailing some of the most import event-driven patterns to be used during your microservice implementation and when adopting kafka as an event backbone. Adopting messaging (Pub/Sub) as a microservice communication backbone involves using at least the following patterns: Decompose by subdomain , event driven microservices are still microservices, so we need to find them, and the domain-driven subdomains is a good approach to identify and classify business function and therefore microservices. With the event storming method, aggregates help to find those subdomain of responsibility. Database per service to enforce each service persists data privately and is accessible only via its API. Services are loosely coupled limiting impact to other service when database schema changes. The database technology is selected from business requirements. The implementation of transactions that span multiple services is complex and enforce using the Saga pattern. Queries that goes over multiple entities is a challenge and CQRS represents an interesting solution. Strangler pattern is used to incrementally migrate an existing, monolytic application by replacing a set of features to a microservice but keep both running in parallel. Saga pattern: Microservices publish events when something happens in the scope of their control like an update in the business entities they are responsible for. A microservice interested in other business entities, subscribe to those events and it can update its own states and business entities when receiving such events. Business entity keys needs to be unique, immutable. Event sourcing persists the state of a business entity such an Order as a sequence of state-changing events. Command Query Responsibility Segregation helps to separate queries from commands and help to address queries with cross-microservice boundary. Update 06/2019 - Author: Jerome Boyer","title":"Understanding event driven microservice patterns"},{"location":"evt-microservices/ED-patterns/#event-sourcing","text":"Most business applications are state based persistent where any update changes the previous state of business entities. The database keeps the last committed update. But some business application needs to explain how it reaches its current state. It needs to keep history of business facts. Traditional domain oriented implementation builds domain data model mapped to a RDBMS. As an example, in the simple Order model below, the database record will keep the last state of the order, the different addresses and the last ordered items in separate tables. If you need to implement a query that looks at what happened to the order over a time period, you need to change the model and add historical records, basically building a log table. Designing a service to manage the life cycle of this order will, most of the time, add a \"delete operation\" to remove data. But most businesses do not remove data. For legal reason, a business ledger has to include new record(s) to compensate a previous transaction. There is no erasing of previously logged transactions. It is always possible to understand what was done in the past. Most business application needs to keep this capability. Event sourcing persists the state of a business entity, such an Order, as a sequence of state-changing events or \"facts\". When the state of a system changes, an application issues a notification event of the state change. Any interested parties can become consumers of the event and take required actions. The state-change event is immutable stored in an event log or event store in time order. The event log or store becomes the principal source of truth. The system state can be recreated from a point in time by reprocessing the events. The history of state changes becomes an audit record for the business and is often a useful source of data for data scientists to gain insights into the business. The previous order model changes to a time oriented immutable stream of events, organized by key (orderID): You can see the \"removing an item\" in the order is a new event. With this capability, we can count how often a specific product is removed. In some cases, the event sourcing pattern is implemented completely within the event backbone. With Kafka topic and partition are the building blocks for event sourcing. However, you can also consider implementing the pattern with an external event store, which provides optimizations for how the data may be accessed and used. For example IBM Db2 Event store can provide the handlers and event store connected to the backbone and can provide optimization for down stream analytical processing of the data. An event store only needs to store three pieces of information: The type of event or aggregate. The sequence number of the event. The data as a serialized entity. More data can be added to help with diagnosis and audit, but the core functionality only requires a narrow set of fields. This gives rise to a very simple data design that can be heavily optimized for appending and retrieving sequences of records. With a central event logs, producers append events to the log, and consumers read them from an offset (the last committed read). To get the final state of an entity, the consumer needs to replay all the events, which means replaying the changes to the state from the last committed offset or from the last snapshot. When replaying the event, it may be important to avoid generating side effects. A common side effect is to send a notification on state change to other consumers. Sometime it may be too long to replay hundreds of events. In that case we can use snapshot, to capture the current state of an entity, and then replay events from the most recent snapshot. This is an optimization technique not needed for all event sourcing implementations. When state change events are in low volume there is no need for snapshots. Kafka is supporting the event sourcing pattern with the topic and partition . In our reference implementation we are validating event sourcing with Kafka in the Order microservices and specially this set of test cases. The event sourcing pattern is well described in this article on microservices.io . It is a very important pattern for event-driven microservices to microservices data synchronization implementations. See also this event sourcing article from Martin Fowler, where he is also using ship movement examples. Our implementation differs as we are using Kafka topic as event store and use different entities to support the container shipping process: the Orders, ShipLocations, Containers entities... Another use case for event sourcing is related to developers who want to understand what data to fix after a service crashes, for example after having deployed a buggy code.","title":"Event sourcing"},{"location":"evt-microservices/ED-patterns/#command-sourcing","text":"Command sourcing is a similar pattern as the event sourcing one, but the commands that modify the states are persisted instead of the events. This allows commands to be processed asynchronously, which can be relevant when the command execution takes a lot of time. One derived challenge is that the command may be executed multiple times, especially in case of failure. Therefore, it has to be idempotent ( making multiple identical requests has the same effect as making a single request). Finally, there is a need also to perform validation of the command to avoid keeping wrong commands in queue. For example, AddItem command is becoming AddItemValidated , then once persisted to a database it becomes an event as ItemAdded . So mixing command and event sourcing is common. Business transactions are not ACID and span multiple services, they are more a serie of steps, each step is supported by a microservice responsible to update its own entity. We talk about \"eventual data consistency\". The event backbone needs to guarantee that events are delivered at least once and the microservices are responsible to manage their offset from the stream source and deal with inconsistency, by detecting duplicate events. At the microservice level, updating data and emitting event needs to be an atomic operation, to avoid inconsistency if the service crashes after the update to the datasource and before emitting the event. This can be done with an eventTable added to the microservice datasource and an event publisher that reads this table on a regular basis and change the state of the event once published. Another solution is to have a database transaction log reader or miner responsible to publish event on new row added to the log. One other approach to avoid the two-phase commit and inconsistency is to use an Event Store or Event Sourcing pattern to keep track of what is done on the business entity with enough information to rebuild the data state. Events are becoming facts describing state changes done on the business entity.","title":"Command sourcing"},{"location":"evt-microservices/ED-patterns/#command-query-responsibility-segregation-cqrs-pattern","text":"When doing event sourcing and domain driven design, we event source the aggregates or root entities. Aggregate creates events that are persisted. On top of the simple create, update and read by ID operations, the business requirements want to perform complex queries that can't be answered by a single aggregate. By just using event sourcing to be able to respond to a query like \"what are the orders of a customer\", then we have to rebuild the history of all orders and filter per customer. It is a lot of computation. This is linked to the problem of having conflicting domain models between query and persistence. Command Query Responsibility Segregation, CQRS, separates the \"command\" operations, used to update application state (also named the 'write model'), from the \"query/read\" operations (the 'read model'). Updates are done as state notification events (change of state), and are persisted in the event log/store. On the \"read model\" side, you have the option of persisting the state in different stores optimized for how other applications may query/read the data. The CQRS application pattern is frequently associated with event sourcing. The following figure presents the high level principles: The service exposes CUD operations, some basic Read by Id and then queries APIs. The domain model is separated into write and read models. Combined with Event Sourcing (ES) the write model goes to the event store. Then we have a separate process that consumes those events and build a projection for future queries. The \"write\" part may persist in SQL while the read may use document oriented database with strong indexing and query capabilities. Or use in-memory database. They do not need to be in the same language. With CQRS and ES the projections are retroactive. New query equals implementing new projection and read the events from the beginning of time or the recent snapshot. Read and write models are strongly decoupled and can evolve independently. It is important to note that the 'Command' part can still handle simple queries, primary-key based, like get order by id, or queries that do not involve joins. With this structure, the Read model microservice will most likely consume events from multiple topics to build the data projection based on joining those data. A query, to assess if the cold-chain was respected on the fresh food order shipment, will go to the voyage, container metrics, and order to be able to answer this question. This is where CQRS shines. A second view of the previous diagram presents how we can separate the API definition and management in a API gateway, the Order command and write model has its own microservice, the event sourcing supported by a Kafka topic, and the query - read model as a set of different microservices or event functions as a service: The shipment order microservice is implementing this pattern. Some implementation items to consider: Consistency (ensure the data constraints are respected for each data transaction): CQRS without event sourcing has the same consistency guarantees as the database used to persist data and events. With Event Sourcing the consistency could be different, one for the \"Write\" model and one for the \"Read\" model. On write model strong consistency is important to ensure the current state of the system is correct, so it leverages transaction, lock and sharding. On read side, we need less consistency, as they mostly work on stale data. Locking data on the read operation is not reasonable. Scalability : Separating read and write as two different microservices allows for high availability. Caching at the \"read\" level can be used to increase performance response time, and can be deployed as multiple standalane instances (Pods in kubernetes). It is also possible to separate the query implementations between different services. Functions as service / serverless are good technology choices to implement complex queries. Availability : As the \"write\" model is often strongly consistent, it impacts availability. This is a fact. The read model is eventually consistent so high availability is possible. In case of failure the system disables the writing of data but still be able to read them as they are served by different databases and services. With CQRS the \"write\" model can evolve overtime without impacting the read model, unless the event model changes. It adds some cost by adding more tables to implement the query parts. It allows smaller model, easier to understand. CQRS results in an increased number of objects, with commands, operations, events,... and packaging in deployable components or containers. It adds potentially different type of data sources. It is more complex. Some challenges to always consider: How to support event version management? How much data to keep in the event store (history)? Design data duplication which results to synchronization issues. The CQRS pattern was introduced by Greg Young , and described in Martin Fowler's work on microservices. As soon as we see two arrows from the same component we have to ask ourselves how does it work: the write model has to persist Order in its own database and then sends OrderCreated event to the topic... Should those operations be atomic and controlled with transaction? We are detailing this in next section.","title":"Command Query Responsibility Segregation (CQRS) pattern"},{"location":"evt-microservices/ED-patterns/#the-consistency-challenge","text":"As introduced in previous section there is potentially a problem of data consistency: the command part saves the data into the database and is not able to send the event to the topic, then consumers do not see the new or updated data. With traditional Java service, using JPA and JMS, the save and send operations can be part of the same transaction and both succeed or both failed. With event sourcing pattern, the source of trust is the event source. It acts as a version control system. So the service should start by creating the event (1) and then persists the data into the database, it uses a topic consumer, get the payload from the event (2) and uses this data to save in its local datasource (3). It derives state solely from the events. If it fails to save, it can persist the event to an error log (4) and then it will be possible to trigger the replay, via an admin API and Command Line Interface (5,6), by searching in the topic using this order id to replay the save operation. Here is a diagram to illustrate that process: This implementation brings a problem on the createOrder(order): order operation, as the returned order was supposed to have the order id as unique key, so most likely, a key created by the database... To avoid this we can generate the key by code and enforce this key in the database if the underlying technology supports it. It is important to clearly study the Kafka consumer API and the different parameters on how to support the read offset. We are addressing those implementation best practices in our consumer note.","title":"The consistency challenge"},{"location":"evt-microservices/ED-patterns/#cqrs-and-change-data-capture","text":"There are other ways to support this dual operations level: There is the open source Debezium tool to help respond to insert, update and delete operations on database and generate event accordingly. It may not work on all database schema. Write the order to the database and in the same transaction write to an event table. Then use a polling to get the events to send to kafka from this event table and delete the row in the table once the event is sent. Use the Change Data Capture from the database transaction log and generate events from this log. The IBM Infosphere CDC product helps to implement this pattern. For more detail about this solution see this product tour . The CQRS implementation using CDC will look like in the following diagram: What is important to note is that the event needs to be flexible on the data payload. We are presenting a event model in the reference implementation. On the view side, updates to the view part need to be idempotent.","title":"CQRS and Change Data Capture"},{"location":"evt-microservices/ED-patterns/#delay-in-the-view","text":"There is a delay between the data persistence and the availability of the data in the Read model. For most business applications, it is perfectly acceptable. In web based data access most of the data are at stale. When there is a need for the client, calling the query operation, to know if the data is up-to-date, the service can define a versioning strategy. When the order data was entered in a form within a single page application like our kc- user interface , the \"create order\" operation should return the order with its unique key freshly created and the Single Page Application will have the last data. Here is an example of such operation: @POST public Response create(OrderCreate dto) { Order order = new Order(UUID.randomUUID().toString(), dto.getProductID(),...); // ... return Response.ok().entity(order).build() }","title":"Delay in the view"},{"location":"evt-microservices/ED-patterns/#schema-change","text":"What to do when we need to add attribute to event?. So we need to create a versioninig schema for event structure. You need to use flexible schema like json schema, Apache Avro or protocol buffer and may be, add an event adapter (as a function?) to translate between the different event structures.","title":"Schema change"},{"location":"evt-microservices/ED-patterns/#saga-pattern","text":"With the adoption of one data source per microservice, there is an interesting challenge on how to support long running transaction cross microservices. With event backbone two phase commit is not an option. Introduced in 1987 by Hector Garcaa-Molrna Kenneth Salem paper the Saga pattern help to support a long running transaction that can be broken up to a collection of sub transactions that can be interleaved any way with other transactions. With microservice each transaction updates data within a single service, each subsequent steps may be triggered by previous completion. The following figure, based on our solution implementation , illustrates those concepts for an order: When the order is created, it can be updated at any time by the user until he/she books it as the final order. As soon as the order is booked, the process needs to allocate the voyage, assigns containers and updates the list of containers to load on the ship. Those actions / commands are chained. The final state (in this schema not in the reality as the process has more steps) is the Order assigned state in the order microservice. SAGA pattern supports two types of implementation: Choreography and Orchestration. With Choreography each service produces and listens to other service\u2019s events and decides if an action should be taken or not. The first service executes a transaction and then publishes an event. It maintains the business entity status, (order.status) to the pending state until it is completed. This event is listened by one or more services which execute local transactions and publish new events. The distributed transaction ends when the last service executes its local transaction or when a service does not publish any events or the event published is not polled by any of the saga\u2019s participants. In case of failure, the source microservice is keeping state and timer to monitor for the completion event. Rolling back a distributed transaction does not come for free. Normally you have to implement another operation/transaction to compensate for what has been done before. With orchestration, one service is responsible to drive each participant on what to do and when. If anything fails, the orchestrator is also responsible for coordinating the rollback by sending commands to each participant to undo the previous operation. Orchestrator is a State Machine where each transformation corresponds to a command or message. Rollbacks are a lot easier when you have an orchestrator to coordinate everything. See also this article from Chris Richardson on the Saga pattern.","title":"Saga pattern"},{"location":"evt-microservices/ED-patterns/#strangler-pattern","text":"","title":"Strangler pattern"},{"location":"evt-microservices/ED-patterns/#problem","text":"How to migrate a monolytics application to microservice without doing a big bang, redeveloping the application from white page. Replacing and rewritting an existing application can be a huge investment. Rewritting a subset of business functions while running current application in parallel may be relevant and reduce risk and velocity of changes.","title":"Problem"},{"location":"evt-microservices/ED-patterns/#solution","text":"The approach is to use a \"strangler\" interface to dispatch request to new or old features. Existing features to migrate are selected by trying to isolate sub components. One of main challenge is to isolate data store and how the new microservices and the legacy application are accessing the shared data. Continuous data replication can be a solution to propagate write model to read model. Write model will most likely stays on the monolitic application, change data capture can be used, with event backbone to propagate change to read model. The facade needs to be scalable and not a single point of failure. It needs to support new APIs (RESTful) and old API (most likely SOAP).","title":"Solution"},{"location":"evt-microservices/ED-patterns/#code-references","text":"The K Containers shipment use cases provides a supporting EDA example https://github.com/ibm-cloud-architecture/refarch-kc Within K Containers shipment the following are example microservices illustrating some of those patterns https://github.com/ibm-cloud-architecture/refarch-kc-ms https://github.com/ibm-cloud-architecture/refarch-kc-order-ms","title":"Code References"},{"location":"evt-src/","text":"Event Sources When you consider an event-driven architecture, think about event producers and event consumers as the interaction points with events. As you develop event-driven applications following a microservices architecture, the microservices you develop play the role of both event producers and event consumers, with the events being passed as the communication payload between them. However, as you look at the wider opportunities that being event driven offers, you need to widen your view and consider event sources that come from beyond the application code you are writing. These are events that may be produced from outside our immediate system but have business relevance or enable us to gain valuable insights into things that are affecting your business. Here is a list of common event sources: IOT devices or sensors showing device status changes Click Stream data from web or mobile applications Mobile applications (HTTP to Back-end For Front-end service and then to topic) Geospacial data Weather alerts Social media feeds Real-time voice feeds Other messaging backbone IOT devices and sensors With IOT devices and sensors you typically have a gateway providing the connectivity for the device, and a level of event enrichment and filtering. In terms of domain driven design you would see the device and gateway as being the technical domain and the event-driven reference architecture as providing the infrastructure for the applications in a business domain. In practice, the IOT gateway or platform provides the connectivity and is the point of filtering and consolidation of events so that only business-relevant events are passed up to the business domain. The gateway can also be the point where the technical event is enhanced to relate to something recognizable at the business level. One example of this is to relate a device number or identifier in the event to something that the business recognizes. Clickstream data Clickstream data is often used to understand the behavior of users as they navigate their way through web or mobile apps. It provides a recording of the actions they take, such as the clicks, the mouse-movements, and the gestures. Analysis of the clickstream data can lead to a deep understanding of how users actually interact with the application. It enables you to detect where users struggle and to look for ways to improve the experience. Processing the clickstream in real time in an event-driven architecture can also give rise to the opportunity to take direct action in response to what a user is currently doing, or more accurately has just done. There are various \"collectors\" that enable collection of standard clickstream events and allow custom actions to be collected as events typically through tags in Javascript. Within the Apache Open Source communities the Divolte collector is an example of one of these collectors that directly publishes events to Kafka topics. Microservices as event producers and consumers The event-driven reference architecture provides support for event-driven microservices, this is microservices are connected and communicate via the pub/sub communication protocol within the Event Backbone. With Kafka as the event backbone and pub/sub messaging provider, microservices can use the Kafka API's to publish and listen for events. Event standards and schemas Where you have control as the producer of an event we should consider having an event schema and following a standard to provide the best opportunity for portability of the solutions across cloud environments. With a lack of formal standards, a working group under the Cloud Native Computing Foundation (CNCF) has recently been formed to define and propose Cloud Events as the standard. Our recommendation is to follow CloudEvents where we have the ability to define the event structure and so pass \"CloudEvents\" through the event backbone. Supporting products IBM Mobile IBM MQ IBM Internet of Things platform IBM Streaming Analytics Kafka Producer API for Java Weather Company Data Voice Agent with Watson Code references The following code repositories can be used for event sourcing inspiration: ship movements/ container metrics event producer as a microservice Container stream analytics Pump Simulator to send New Pump/ Asset event or Metric events to emulate intelligent IoT Electrical Pump. Simple text message producer As well as the starting application generated from IBM Event Streams. See such app in the folder gettingStarted and explanation in the starter App","title":"Event Sources"},{"location":"evt-src/#event-sources","text":"When you consider an event-driven architecture, think about event producers and event consumers as the interaction points with events. As you develop event-driven applications following a microservices architecture, the microservices you develop play the role of both event producers and event consumers, with the events being passed as the communication payload between them. However, as you look at the wider opportunities that being event driven offers, you need to widen your view and consider event sources that come from beyond the application code you are writing. These are events that may be produced from outside our immediate system but have business relevance or enable us to gain valuable insights into things that are affecting your business. Here is a list of common event sources: IOT devices or sensors showing device status changes Click Stream data from web or mobile applications Mobile applications (HTTP to Back-end For Front-end service and then to topic) Geospacial data Weather alerts Social media feeds Real-time voice feeds Other messaging backbone","title":"Event Sources"},{"location":"evt-src/#iot-devices-and-sensors","text":"With IOT devices and sensors you typically have a gateway providing the connectivity for the device, and a level of event enrichment and filtering. In terms of domain driven design you would see the device and gateway as being the technical domain and the event-driven reference architecture as providing the infrastructure for the applications in a business domain. In practice, the IOT gateway or platform provides the connectivity and is the point of filtering and consolidation of events so that only business-relevant events are passed up to the business domain. The gateway can also be the point where the technical event is enhanced to relate to something recognizable at the business level. One example of this is to relate a device number or identifier in the event to something that the business recognizes.","title":"IOT devices and sensors"},{"location":"evt-src/#clickstream-data","text":"Clickstream data is often used to understand the behavior of users as they navigate their way through web or mobile apps. It provides a recording of the actions they take, such as the clicks, the mouse-movements, and the gestures. Analysis of the clickstream data can lead to a deep understanding of how users actually interact with the application. It enables you to detect where users struggle and to look for ways to improve the experience. Processing the clickstream in real time in an event-driven architecture can also give rise to the opportunity to take direct action in response to what a user is currently doing, or more accurately has just done. There are various \"collectors\" that enable collection of standard clickstream events and allow custom actions to be collected as events typically through tags in Javascript. Within the Apache Open Source communities the Divolte collector is an example of one of these collectors that directly publishes events to Kafka topics.","title":"Clickstream data"},{"location":"evt-src/#microservices-as-event-producers-and-consumers","text":"The event-driven reference architecture provides support for event-driven microservices, this is microservices are connected and communicate via the pub/sub communication protocol within the Event Backbone. With Kafka as the event backbone and pub/sub messaging provider, microservices can use the Kafka API's to publish and listen for events.","title":"Microservices as event producers and consumers"},{"location":"evt-src/#event-standards-and-schemas","text":"Where you have control as the producer of an event we should consider having an event schema and following a standard to provide the best opportunity for portability of the solutions across cloud environments. With a lack of formal standards, a working group under the Cloud Native Computing Foundation (CNCF) has recently been formed to define and propose Cloud Events as the standard. Our recommendation is to follow CloudEvents where we have the ability to define the event structure and so pass \"CloudEvents\" through the event backbone.","title":"Event standards and schemas"},{"location":"evt-src/#supporting-products","text":"IBM Mobile IBM MQ IBM Internet of Things platform IBM Streaming Analytics Kafka Producer API for Java Weather Company Data Voice Agent with Watson","title":"Supporting products"},{"location":"evt-src/#code-references","text":"The following code repositories can be used for event sourcing inspiration: ship movements/ container metrics event producer as a microservice Container stream analytics Pump Simulator to send New Pump/ Asset event or Metric events to emulate intelligent IoT Electrical Pump. Simple text message producer As well as the starting application generated from IBM Event Streams. See such app in the folder gettingStarted and explanation in the starter App","title":"Code references"},{"location":"evt-state/","text":"Event managed state While the prime focus for an event-driven architecture is processing events, in certain cases you need to persist events for post processing and queries by other applications. The event backbone has a built-in event log that can be used to store and reply to events that are published to the backbone. However, considering the full scope of event-driven solutions, other use cases and types of store can be supported: Event stores optimized for analytics. Event sourcing as a pattern for recording state changes and updates across distributed systems. Command Query Response Separation (CQRS) as an optimization that separates updates and reads across different stores. Event sourcing When the state of a system changes, an application issues a notification event of the state change. We are detailing this pattern here >> Command Query Responsibility Segregation (CQRS) The event log leads to more work to support business query as it requires converting the events into the application state suitable to the query. We are detailing this pattern here >> See the following order management project for a detail explanation and implementation of the CQRS and event sourcing patterns. Event sourcing, CQRS and microservices With the adoption of microservices you have explicitly separated state, so that a microservice is bounded with its own state. Further, with the use of event sourcing, you create a history log that is not easy to query. The challenge now comes when you need to implement a query that requires a joining of data from multiple services. There are multiple choices to address service orchestration: API composition or the CQRS pattern. For API composition the query is supported by an operation which integrate with all other microservices and may do some data transformation to combine the results. With this pattern you need to assess for aggregation requirements as they may dramatically impact performance. You may need to assess where to put this API composition component. It can be an API gateway or part of a BFF or even its own microservices. The other answer is to implement a CQRS pattern where state changes are published as events by multiple related business objects. Each change is persisted in the event log or event store, and a higher level operation subscribes to each event and persists the data in a queryable data store. Fearther readings Read more on this pattern at https://microservices.io/patterns/data/cqrs.html and our reference implementation Supporting Products IBM Event Streams Public Cloud IBM Event Streams Private Cloud IBM Db2 Event store","title":"Event Managed States"},{"location":"evt-state/#event-managed-state","text":"While the prime focus for an event-driven architecture is processing events, in certain cases you need to persist events for post processing and queries by other applications. The event backbone has a built-in event log that can be used to store and reply to events that are published to the backbone. However, considering the full scope of event-driven solutions, other use cases and types of store can be supported: Event stores optimized for analytics. Event sourcing as a pattern for recording state changes and updates across distributed systems. Command Query Response Separation (CQRS) as an optimization that separates updates and reads across different stores.","title":"Event managed state"},{"location":"evt-state/#event-sourcing","text":"When the state of a system changes, an application issues a notification event of the state change. We are detailing this pattern here >>","title":"Event sourcing"},{"location":"evt-state/#command-query-responsibility-segregation-cqrs","text":"The event log leads to more work to support business query as it requires converting the events into the application state suitable to the query. We are detailing this pattern here >> See the following order management project for a detail explanation and implementation of the CQRS and event sourcing patterns.","title":"Command Query Responsibility Segregation (CQRS)"},{"location":"evt-state/#event-sourcing-cqrs-and-microservices","text":"With the adoption of microservices you have explicitly separated state, so that a microservice is bounded with its own state. Further, with the use of event sourcing, you create a history log that is not easy to query. The challenge now comes when you need to implement a query that requires a joining of data from multiple services. There are multiple choices to address service orchestration: API composition or the CQRS pattern. For API composition the query is supported by an operation which integrate with all other microservices and may do some data transformation to combine the results. With this pattern you need to assess for aggregation requirements as they may dramatically impact performance. You may need to assess where to put this API composition component. It can be an API gateway or part of a BFF or even its own microservices. The other answer is to implement a CQRS pattern where state changes are published as events by multiple related business objects. Each change is persisted in the event log or event store, and a higher level operation subscribes to each event and persists the data in a queryable data store.","title":"Event sourcing, CQRS and microservices"},{"location":"evt-state/#fearther-readings","text":"Read more on this pattern at https://microservices.io/patterns/data/cqrs.html and our reference implementation","title":"Fearther readings"},{"location":"evt-state/#supporting-products","text":"IBM Event Streams Public Cloud IBM Event Streams Private Cloud IBM Db2 Event store","title":"Supporting Products"},{"location":"kafka/FAQ/","text":"Frequently asked questions Kafka concepts? See this introduction How to support exactly once delivery? See the section in the producer implementation considerations note . Also it is important to note that the Kafka Stream API supports exactly once semantics with the config: processing.guarantee=exactly_once . Each task within a read-process-write flow may fail so this setting is important to be sure the right answer is delivered, even in case of task failure, and the process is executed exactly once.","title":"Kafka FAQ"},{"location":"kafka/FAQ/#frequently-asked-questions","text":"","title":"Frequently asked questions"},{"location":"kafka/FAQ/#kafka-concepts","text":"See this introduction","title":"Kafka concepts?"},{"location":"kafka/FAQ/#how-to-support-exactly-once-delivery","text":"See the section in the producer implementation considerations note . Also it is important to note that the Kafka Stream API supports exactly once semantics with the config: processing.guarantee=exactly_once . Each task within a read-process-write flow may fail so this setting is important to be sure the right answer is delivered, even in case of task failure, and the process is executed exactly once.","title":"How to support exactly once delivery?"},{"location":"kafka/arch/","text":"IBM Event Streams / Kafka Architecture Considerations If you need to know the key concepts read this article . High Availability As a distributed cluster, kafka brokers ensure high availability to process new events. Topic has replication factor to support not loosing data in case of broker failure. You need at least 3 brokers to ensure availability and a replication factor set to 3 for each topic, so no data will be lost. Partition enables data locality, elasticity, scalability, high performance, parallelism, and fault tolerance. Each partitition is replicated at least 3 times and allocated in different brokers. One replicas is the leader . In the case of broker failure (broker 1 in figure below), one of the existing partition in the remaining running brokers will take the leader role (e.g. red partition in broker 3): The keys in the data record determine the partitioning of data in Kafka . The records with the same key will be in the same partition. As kafka is keeping its cluster states in zookeeper, you also need to have at least a three node cluster for zookeeper. Writes to Zookeeper are only performed on changes to the membership of consumer groups or on changes to the Kafka cluster itself. Assuming you are using the most recent kafka version (after 0.9), it is possible to have a unique zookeeper cluster for multiple kafka clusters. But the latency between Kafka and zookeeper needs to be under few milliseconds (< 15ms) anyway. Zookeepers and Brokers should have high availability communication via dual network, and each broker and node allocated on different racks and blades. Consumers and producers are using a list of bootstrap server names (also named advertiser.listeners) to contact the cluster. The list is used for cluster discovery, it does not need to keep the full set of server names or ip addresses. A Kafka cluster has exactly one broker that acts as the controller. Per design Kafka aims to run within a single data center. But it is still recommended to use multiple racks connected with low laterncy dual networks. With multiple racks you will have better fault tolerance, as one rack failure will impact only one broker. There is a configuration property to assign kafka broker using rack awareness. (See this configuration from the product documentation). Always assess the latency requirements and consumers needs. Throughtput is linked to the number of partitions within a topic and having more consumers running in parallel. Consumers and producers should better run on separate servers than the brokers nodes. For high availability assess any potential single point of failure, such as server, rack, network, power supply... The figure below illustrates a kubernetes deployment, where zookeeper and kafka brokers are allocated to 3 worker nodes (We recommend 5 nodes) and event driven microservices are deployed in separate nodes. Those microservices are consumers and producers of events from one to many topics. Kafka may be used as event sourcing. We recommend reading this event stream article for planning installation on k8s. To add new broker, we can deploy the runtime to a new server / rack / blade, and give a unique ID. It will process new topic, but it is possible to use tool to migrate some existing topic/ partitions to the new server. The tool is used to reassign partitions across brokers. An ideal partition distribution would ensure even data load and partition sizes across all brokers. High Availability in the context of Kubernetes deployment The combination of kafka with kubernetes seems to be a sound approach, but it is not that easy to achieve. Kubernetes workloads prefer to be stateless, Kafka is stateful platform and manages its own brokers, and replications across known servers. It knows the underlying infrastructure. In kubernetes nodes and pods may change dynamically. For any Kubernetes deployment real high availability is constrained by the application / workload deployed on it. The Kubernetes platform supports high availability by having at least the following configuration: At least three master nodes (always an odd number of nodes). One is active at master, the others are in standby. The election of the master is using the quorum algorithm. Three proxy nodes. At least three worker nodes, but with zookeeper and Kafka clusters, we may need to define six nodes as we do not want to have zookeeper nodes with Kafka cluster broker on the same host. Externalize the management stack to three manager nodes Shared storage outside of the cluster to support private image registry, audit logs, and statefulset data persistence. Use etcd cluster: See recommendations from this article . The virtual IP manager assigns virtual IP addresses to master and proxy nodes and monitors the health of the cluster. It leverages etcd for storing information, so it is important that etcd is high available too and connected to low latency network below 10ms. For IBM Cloud private HA installation see the product documentation Traditionally disaster recovery and high availability were always consider separated subjects. Now active/active deployment where workloads are deployed in different data center, is more and more a common request. IBM Cloud Private is supporting federation cross data centers , but you need to ensure to have low latency network connections. Also not all deployment components of a solution are well suited for cross data center clustering. For Kafka context, the Confluent website presents an interesting article for Kafka production deployment . One of their recommendation is to avoid cluster that spans multiple data centers and specially long distance ones. But the semantic of the event processing may authorize some adaptations. For sure, you need multiple Kafka Brokers, which will connect to the same ZooKeeper ensemble running at least five nodes (you can tolerate the loss of one server during the planned maintenance of another server). One Zookeeper server acts as a lead and the two others as stand-by. The schema above illustrates the recommendations to separate Zookeeper from Kafka nodes for failover purpose as zookeeper keeps state of the Kafka cluster. We use Kubernetes anti-affinity to ensure they are scheduled onto separate worker nodes that the ones used by zookeeper. It uses the labels on pods with a rule like: Kafka pod should not run on same node as zookeeper pods. Here is an example of such spec: apiVersion: v1 kind: Pod metadata: name: with-pod-affinity spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: labelSelector: matchExpressions: - key: name operator: In values: - gc-zookeeper topologyKey: kubernetes.io/hostname We recommend doing the running zookeeper in k8s tutorial for understanding such configuration. Provision a fast storage class for persistence volume. Kafka uses the log.dirs property to configure the driver to persist logs. So you need to define multiple volumes/ drives to support log.dirs. Zookeeper should not be used by other applications deployed in k8s cluster, it has to be dedicated for one Kafka cluster only. In a multi-cluster configuration being used for disaster recovery purposes, messages sent between clusters will have different offsets in the two clusters. It is usual to use timestamps for position information when restarting applications for recovery after a disaster. We are addressing offset management in one of our consumer projects here . For configuring ICP for HA on VmWare read this note . For Kafka streaming with stateful processing like joins, event aggregation and correlation coming from multiple partitions, it is not easy to achieve high availability cross clusters: in the strictest case every event must be processed by the streaming service exactly once. Which means: producer emits data to different sites and be able to re-emit in case of failure. Brokers are known by producer via a list of hostnames and port numbers. communications between zookeepers and cluster nodes are redundant and safe for data losses consumers ensure idempotence... They have to tolerate data duplication and manage data integrity in their persistence layer. Within Kafka's boundary, data will not be lost, when doing proper configuration, also to support high availability the complexity moves to the producer and the consumer implementation. Kafka configuration is an art and you need to tune the parameters by use case: Partition replication for at least 3 replicas. Recall that in case of node failure, coordination of partition re-assignments is provided with ZooKeeper. End to end latency needs to be measured from producer (when a message is sent) to consumer when it is read. A consumer is able to get a message when the broker finishes replicating to all in-synch replicas. Use the producer buffering capability to pace the message to the broker. Can use memory or time based threshold. Define the number of partitions to drive consumer parallelism. More consumers running in parallel the higher is the throughput. Assess the retention hours to control when old messages in topic can be deleted Control the maximum message size the server can receive. Zookeeper is not CPU intensive and each server should have a least 2 GB of heap space and 4GB reserved. Two cpu per server should be sufficient. Servers keep their entire state machine in memory, and write every mutation to a durable WAL (Write Ahead Log) on persistent storage. To prevent the WAL from growing without bound, ZooKeeper servers periodically snapshot their in memory state to storage. Use fast and dynamically provisioned persistence storage for both WAL and snapshot. Kubernetes Operator It is important to note that the deployment and management of stateful application in Kubernetes should, now, use the proposed Operator Framework introduced by Red Hat and Google. One important contribution is the Strinmzi kafka operator that simplify the deployment of Kafka within k8s by adding a set of operators to deploy and manage kafka cluster, manage topics and manage users. Multi regions for disaster recovery With the current implementation it is recommended to have one cluster per data center / availability zone. Consumers and producers are co-located to the brokers cluster. When there are needs to keep some part of the data replicated in both data center, you need to assess what kind of data can be aggregated, and if Kafka mirroring tool can be used. The tool consumes from a source cluster, from a given topic, and produces to a destination cluster with the same named topic. It keeps the message key for partitioning, so order is preserved. The above diagram is using Kafka MirrorMaker with a master to slave deployment. Within the data center 2, the brokers are here to manage the topics and events. When there is no consumer running, nothing happen. Consumers and producers can be started when DC1 fails. This is the active/passive model. In fact, we could have consumers within the DC2 processing topics to manage a readonly model, keeping in memory their projection view, as presented in the CQRS pattern . The second solution is to use one mirror maker in each site, for each topic. This is an active - actice topology: consumers and producers are on both sites. But to avoid infinite loop, we need to use naming convention for the topic, or only produce in the cluster of the main topic. Consumers consume from the replicated topic. When you want to deploy solution that spreads over multiple regions to support global streaming, you need to address the following challenges: How do you make data available to applications across multiple data centers? How to serve data closer to the geography? How to be compliant on regulations, like GDPR? How to address no duplication of records? MQ integration IBM has created a pair of connectors, available as source code or as part of IBM Event Streams product. The Source Connector responsible to support the integration from MQ queue to Kafka topic is available in the github repository named ibm-messaging/kafka-connect-mq-source while the sink connector, from Kafka topic to MQ queue is at ibm-messaging/kafka-connect-mq-sink The following figure illustrates the high level components. It is important to note that the Kafka connectors is a cluster deployment for local high availability and scalability. We are proposing an MQ to Kafka implementation sample in the container inventory repository where we mockup the integration of a legacy DB managing shipment container inventory, it runs as a java appm jms producer and consumer on MQ queues. This solution is integrated in the global EDA reference solution implementation and specially the Reefer container management microservice.","title":"Kafka architecture"},{"location":"kafka/arch/#ibm-event-streams-kafka-architecture-considerations","text":"If you need to know the key concepts read this article .","title":"IBM Event Streams / Kafka Architecture Considerations"},{"location":"kafka/arch/#high-availability","text":"As a distributed cluster, kafka brokers ensure high availability to process new events. Topic has replication factor to support not loosing data in case of broker failure. You need at least 3 brokers to ensure availability and a replication factor set to 3 for each topic, so no data will be lost. Partition enables data locality, elasticity, scalability, high performance, parallelism, and fault tolerance. Each partitition is replicated at least 3 times and allocated in different brokers. One replicas is the leader . In the case of broker failure (broker 1 in figure below), one of the existing partition in the remaining running brokers will take the leader role (e.g. red partition in broker 3): The keys in the data record determine the partitioning of data in Kafka . The records with the same key will be in the same partition. As kafka is keeping its cluster states in zookeeper, you also need to have at least a three node cluster for zookeeper. Writes to Zookeeper are only performed on changes to the membership of consumer groups or on changes to the Kafka cluster itself. Assuming you are using the most recent kafka version (after 0.9), it is possible to have a unique zookeeper cluster for multiple kafka clusters. But the latency between Kafka and zookeeper needs to be under few milliseconds (< 15ms) anyway. Zookeepers and Brokers should have high availability communication via dual network, and each broker and node allocated on different racks and blades. Consumers and producers are using a list of bootstrap server names (also named advertiser.listeners) to contact the cluster. The list is used for cluster discovery, it does not need to keep the full set of server names or ip addresses. A Kafka cluster has exactly one broker that acts as the controller. Per design Kafka aims to run within a single data center. But it is still recommended to use multiple racks connected with low laterncy dual networks. With multiple racks you will have better fault tolerance, as one rack failure will impact only one broker. There is a configuration property to assign kafka broker using rack awareness. (See this configuration from the product documentation). Always assess the latency requirements and consumers needs. Throughtput is linked to the number of partitions within a topic and having more consumers running in parallel. Consumers and producers should better run on separate servers than the brokers nodes. For high availability assess any potential single point of failure, such as server, rack, network, power supply... The figure below illustrates a kubernetes deployment, where zookeeper and kafka brokers are allocated to 3 worker nodes (We recommend 5 nodes) and event driven microservices are deployed in separate nodes. Those microservices are consumers and producers of events from one to many topics. Kafka may be used as event sourcing. We recommend reading this event stream article for planning installation on k8s. To add new broker, we can deploy the runtime to a new server / rack / blade, and give a unique ID. It will process new topic, but it is possible to use tool to migrate some existing topic/ partitions to the new server. The tool is used to reassign partitions across brokers. An ideal partition distribution would ensure even data load and partition sizes across all brokers.","title":"High Availability"},{"location":"kafka/arch/#high-availability-in-the-context-of-kubernetes-deployment","text":"The combination of kafka with kubernetes seems to be a sound approach, but it is not that easy to achieve. Kubernetes workloads prefer to be stateless, Kafka is stateful platform and manages its own brokers, and replications across known servers. It knows the underlying infrastructure. In kubernetes nodes and pods may change dynamically. For any Kubernetes deployment real high availability is constrained by the application / workload deployed on it. The Kubernetes platform supports high availability by having at least the following configuration: At least three master nodes (always an odd number of nodes). One is active at master, the others are in standby. The election of the master is using the quorum algorithm. Three proxy nodes. At least three worker nodes, but with zookeeper and Kafka clusters, we may need to define six nodes as we do not want to have zookeeper nodes with Kafka cluster broker on the same host. Externalize the management stack to three manager nodes Shared storage outside of the cluster to support private image registry, audit logs, and statefulset data persistence. Use etcd cluster: See recommendations from this article . The virtual IP manager assigns virtual IP addresses to master and proxy nodes and monitors the health of the cluster. It leverages etcd for storing information, so it is important that etcd is high available too and connected to low latency network below 10ms. For IBM Cloud private HA installation see the product documentation Traditionally disaster recovery and high availability were always consider separated subjects. Now active/active deployment where workloads are deployed in different data center, is more and more a common request. IBM Cloud Private is supporting federation cross data centers , but you need to ensure to have low latency network connections. Also not all deployment components of a solution are well suited for cross data center clustering. For Kafka context, the Confluent website presents an interesting article for Kafka production deployment . One of their recommendation is to avoid cluster that spans multiple data centers and specially long distance ones. But the semantic of the event processing may authorize some adaptations. For sure, you need multiple Kafka Brokers, which will connect to the same ZooKeeper ensemble running at least five nodes (you can tolerate the loss of one server during the planned maintenance of another server). One Zookeeper server acts as a lead and the two others as stand-by. The schema above illustrates the recommendations to separate Zookeeper from Kafka nodes for failover purpose as zookeeper keeps state of the Kafka cluster. We use Kubernetes anti-affinity to ensure they are scheduled onto separate worker nodes that the ones used by zookeeper. It uses the labels on pods with a rule like: Kafka pod should not run on same node as zookeeper pods. Here is an example of such spec: apiVersion: v1 kind: Pod metadata: name: with-pod-affinity spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: labelSelector: matchExpressions: - key: name operator: In values: - gc-zookeeper topologyKey: kubernetes.io/hostname We recommend doing the running zookeeper in k8s tutorial for understanding such configuration. Provision a fast storage class for persistence volume. Kafka uses the log.dirs property to configure the driver to persist logs. So you need to define multiple volumes/ drives to support log.dirs. Zookeeper should not be used by other applications deployed in k8s cluster, it has to be dedicated for one Kafka cluster only. In a multi-cluster configuration being used for disaster recovery purposes, messages sent between clusters will have different offsets in the two clusters. It is usual to use timestamps for position information when restarting applications for recovery after a disaster. We are addressing offset management in one of our consumer projects here . For configuring ICP for HA on VmWare read this note . For Kafka streaming with stateful processing like joins, event aggregation and correlation coming from multiple partitions, it is not easy to achieve high availability cross clusters: in the strictest case every event must be processed by the streaming service exactly once. Which means: producer emits data to different sites and be able to re-emit in case of failure. Brokers are known by producer via a list of hostnames and port numbers. communications between zookeepers and cluster nodes are redundant and safe for data losses consumers ensure idempotence... They have to tolerate data duplication and manage data integrity in their persistence layer. Within Kafka's boundary, data will not be lost, when doing proper configuration, also to support high availability the complexity moves to the producer and the consumer implementation. Kafka configuration is an art and you need to tune the parameters by use case: Partition replication for at least 3 replicas. Recall that in case of node failure, coordination of partition re-assignments is provided with ZooKeeper. End to end latency needs to be measured from producer (when a message is sent) to consumer when it is read. A consumer is able to get a message when the broker finishes replicating to all in-synch replicas. Use the producer buffering capability to pace the message to the broker. Can use memory or time based threshold. Define the number of partitions to drive consumer parallelism. More consumers running in parallel the higher is the throughput. Assess the retention hours to control when old messages in topic can be deleted Control the maximum message size the server can receive. Zookeeper is not CPU intensive and each server should have a least 2 GB of heap space and 4GB reserved. Two cpu per server should be sufficient. Servers keep their entire state machine in memory, and write every mutation to a durable WAL (Write Ahead Log) on persistent storage. To prevent the WAL from growing without bound, ZooKeeper servers periodically snapshot their in memory state to storage. Use fast and dynamically provisioned persistence storage for both WAL and snapshot.","title":"High Availability in the context of Kubernetes deployment"},{"location":"kafka/arch/#kubernetes-operator","text":"It is important to note that the deployment and management of stateful application in Kubernetes should, now, use the proposed Operator Framework introduced by Red Hat and Google. One important contribution is the Strinmzi kafka operator that simplify the deployment of Kafka within k8s by adding a set of operators to deploy and manage kafka cluster, manage topics and manage users.","title":"Kubernetes Operator"},{"location":"kafka/arch/#multi-regions-for-disaster-recovery","text":"With the current implementation it is recommended to have one cluster per data center / availability zone. Consumers and producers are co-located to the brokers cluster. When there are needs to keep some part of the data replicated in both data center, you need to assess what kind of data can be aggregated, and if Kafka mirroring tool can be used. The tool consumes from a source cluster, from a given topic, and produces to a destination cluster with the same named topic. It keeps the message key for partitioning, so order is preserved. The above diagram is using Kafka MirrorMaker with a master to slave deployment. Within the data center 2, the brokers are here to manage the topics and events. When there is no consumer running, nothing happen. Consumers and producers can be started when DC1 fails. This is the active/passive model. In fact, we could have consumers within the DC2 processing topics to manage a readonly model, keeping in memory their projection view, as presented in the CQRS pattern . The second solution is to use one mirror maker in each site, for each topic. This is an active - actice topology: consumers and producers are on both sites. But to avoid infinite loop, we need to use naming convention for the topic, or only produce in the cluster of the main topic. Consumers consume from the replicated topic. When you want to deploy solution that spreads over multiple regions to support global streaming, you need to address the following challenges: How do you make data available to applications across multiple data centers? How to serve data closer to the geography? How to be compliant on regulations, like GDPR? How to address no duplication of records?","title":"Multi regions for disaster recovery"},{"location":"kafka/arch/#mq-integration","text":"IBM has created a pair of connectors, available as source code or as part of IBM Event Streams product. The Source Connector responsible to support the integration from MQ queue to Kafka topic is available in the github repository named ibm-messaging/kafka-connect-mq-source while the sink connector, from Kafka topic to MQ queue is at ibm-messaging/kafka-connect-mq-sink The following figure illustrates the high level components. It is important to note that the Kafka connectors is a cluster deployment for local high availability and scalability. We are proposing an MQ to Kafka implementation sample in the container inventory repository where we mockup the integration of a legacy DB managing shipment container inventory, it runs as a java appm jms producer and consumer on MQ queues. This solution is integrated in the global EDA reference solution implementation and specially the Reefer container management microservice.","title":"MQ integration"},{"location":"kafka/consumers/","text":"Consumers design and implementation considerations Important concepts Consumers belong to consumer groups . You specify the group name as part of the connection parameters. Consumer groups are grouping consumers to cooperate to consume messages from one or more topics. Consumers can run in separate hosts and separate processes. Organized in cluster the coordinator servers are responsible for assigning partitions to the consumers in the group. The rebalancing of partition to consumer is done when a new consumer join or leave the group or when a new partition is added to an existing topic. There is always at least one consumer per partition. Implementing a Topic consumer is using the kafka KafkaConsumer class which the API documentation is a must read. The implementation is simple for a single thread consumer, and the code structure looks like: * prepare the properties * create an instance of KafkaConsumer to connect to a topic and a partition * loop on polling events * process the ConsumerRecords and commit the offset by code or use the autocommit attibute of the consumer, Examples of Java consumers can be found in this project . Example of Javascript implementation is in this repository But the complexity comes from the offset management and multithreading needs. So the following important considerations need to be addressed while implementing a consumer: Assess number of consumer needed The KafkaConsumer is not thread safe so it is recommended to run in a unique thread. But if needed you can implement a multi-threads solution, but as each thread will open a TCP connection to the Kafka broker, be sure to close the connection to avoid memory leak. The alternate is to start n processus. If you need multiple consumers running in parallel to scale horizontally, you have to define multiple partitions while configuring the topic and use fine-grained control over offset persistence. You\u2019ll use one consumer per partition of a topic. This consumer-per-partition pattern maximizes throughput. When consumers run in parallel and you use multiple threads per soncumser you need to be sure the total number of threads across all instances do not exceed the total number of partitions in the topic. Also, a consumer can subscribe to multiple topics. The brokers are doing rebalancing of the assignment of topic-partition to a consumer that belong to a group. When creating a new consumer you can specify the group id in the options. Offset management Recall that offset is just a numeric identifier of a consumer position of the last record read within a partition. Consumers periodically need to commit the offsets of messages they have received to show they have processed the message and in case of failure from where they should reconnect. It is possible to commit by calling API or by setting some properties at the consumer creation level to enable autocommit offset. When doing manual offet, there are two types of manually committed: * offsets\u2014synchronous * asynchronous. When dealing with heavy load storing offset in zookeeper is non advisable. It is even now recognize as a bad practice. To manage offset use the new consumer API, and for example commits offset synchronously when a specified number of events are read from the topic and the persistence to the back end succeed. Assess if it is possible to lose messages from topic. If so, when a consumer restarts it will start consuming the topic from the end of the queue. Do the solution is fine with at-least-once delivery or exactly-once is a must have? As the operation to store a message and the storage of offsets are two separate operations, and in case of failure between them, it is possible to have stale offsets, which will introduce duplicate messages when consumers restart to process from last known committed offset. \"exactly-once\" means grouping record and offset persistence in an atomic operation. Repositories with consumer code Within the Container shipment solution we have a ship movement event consumer and a container metrics event consumer. Asset analytics asset consumers Nodejs kafka consumers and producers Kafka useful Consumer APIs KafkaConsumer a topic consumer which support: transparently handles brokers failure transparently adapt to partition migration within the cluster support grouping for load balancing among consumers maintains TCP connections to the necessary brokers to fetch data subscribe to multiple topics and being part of consumer groups each partition is assigned to exactly one consumer in the group if a process fails, the partitions assigned to it will be reassigned to other consumers in the same group ConsumerRecords holds the list ConsumerRecord per partition for a particular topic. ConsumerRecord A key/value pair to be received from Kafka. This also consists of a topic name and a partition number from which the record is being received, an offset that points to the record in a Kafka partition, and a timestamp References IBM Event Streams - Consuming messages KafkaConsumer class","title":"Kafka consumer"},{"location":"kafka/consumers/#consumers-design-and-implementation-considerations","text":"","title":"Consumers design and implementation considerations"},{"location":"kafka/consumers/#important-concepts","text":"Consumers belong to consumer groups . You specify the group name as part of the connection parameters. Consumer groups are grouping consumers to cooperate to consume messages from one or more topics. Consumers can run in separate hosts and separate processes. Organized in cluster the coordinator servers are responsible for assigning partitions to the consumers in the group. The rebalancing of partition to consumer is done when a new consumer join or leave the group or when a new partition is added to an existing topic. There is always at least one consumer per partition. Implementing a Topic consumer is using the kafka KafkaConsumer class which the API documentation is a must read. The implementation is simple for a single thread consumer, and the code structure looks like: * prepare the properties * create an instance of KafkaConsumer to connect to a topic and a partition * loop on polling events * process the ConsumerRecords and commit the offset by code or use the autocommit attibute of the consumer, Examples of Java consumers can be found in this project . Example of Javascript implementation is in this repository But the complexity comes from the offset management and multithreading needs. So the following important considerations need to be addressed while implementing a consumer:","title":"Important concepts"},{"location":"kafka/consumers/#assess-number-of-consumer-needed","text":"The KafkaConsumer is not thread safe so it is recommended to run in a unique thread. But if needed you can implement a multi-threads solution, but as each thread will open a TCP connection to the Kafka broker, be sure to close the connection to avoid memory leak. The alternate is to start n processus. If you need multiple consumers running in parallel to scale horizontally, you have to define multiple partitions while configuring the topic and use fine-grained control over offset persistence. You\u2019ll use one consumer per partition of a topic. This consumer-per-partition pattern maximizes throughput. When consumers run in parallel and you use multiple threads per soncumser you need to be sure the total number of threads across all instances do not exceed the total number of partitions in the topic. Also, a consumer can subscribe to multiple topics. The brokers are doing rebalancing of the assignment of topic-partition to a consumer that belong to a group. When creating a new consumer you can specify the group id in the options.","title":"Assess number of consumer needed"},{"location":"kafka/consumers/#offset-management","text":"Recall that offset is just a numeric identifier of a consumer position of the last record read within a partition. Consumers periodically need to commit the offsets of messages they have received to show they have processed the message and in case of failure from where they should reconnect. It is possible to commit by calling API or by setting some properties at the consumer creation level to enable autocommit offset. When doing manual offet, there are two types of manually committed: * offsets\u2014synchronous * asynchronous. When dealing with heavy load storing offset in zookeeper is non advisable. It is even now recognize as a bad practice. To manage offset use the new consumer API, and for example commits offset synchronously when a specified number of events are read from the topic and the persistence to the back end succeed. Assess if it is possible to lose messages from topic. If so, when a consumer restarts it will start consuming the topic from the end of the queue. Do the solution is fine with at-least-once delivery or exactly-once is a must have? As the operation to store a message and the storage of offsets are two separate operations, and in case of failure between them, it is possible to have stale offsets, which will introduce duplicate messages when consumers restart to process from last known committed offset. \"exactly-once\" means grouping record and offset persistence in an atomic operation.","title":"Offset management"},{"location":"kafka/consumers/#repositories-with-consumer-code","text":"Within the Container shipment solution we have a ship movement event consumer and a container metrics event consumer. Asset analytics asset consumers Nodejs kafka consumers and producers","title":"Repositories with consumer code"},{"location":"kafka/consumers/#kafka-useful-consumer-apis","text":"KafkaConsumer a topic consumer which support: transparently handles brokers failure transparently adapt to partition migration within the cluster support grouping for load balancing among consumers maintains TCP connections to the necessary brokers to fetch data subscribe to multiple topics and being part of consumer groups each partition is assigned to exactly one consumer in the group if a process fails, the partitions assigned to it will be reassigned to other consumers in the same group ConsumerRecords holds the list ConsumerRecord per partition for a particular topic. ConsumerRecord A key/value pair to be received from Kafka. This also consists of a topic name and a partition number from which the record is being received, an offset that points to the record in a Kafka partition, and a timestamp","title":"Kafka useful Consumer APIs"},{"location":"kafka/consumers/#references","text":"IBM Event Streams - Consuming messages KafkaConsumer class","title":"References"},{"location":"kafka/kafka-stream/","text":"Kafka Streaming Kafka Streams is a graph of processing nodes to implement the logic to process event streams. Each node process events from the parent node. We recommend reading this excellent introduction from Jay Kreps @confluent: Kafka stream made simple to get a good understanding of why Kafka stream was created. To summarize, Kafka Stream has the following capabilities: Stream processing is helpful for handling out-of-order data, reprocessing input as code changes, and performing stateful computations. It uses producer / consumer, stateful storage and consumer groups. It treats both past and future data the same way. Embedded library for your application to use. Integrate tables for state persistence with streams of events. Consumes continuous real time flows of records and publish new flows. Supports exactly-once processing semantics to guarantee that each record will be processed once and only once even when there is a failure. Stream APIs transform, aggregate and enrich data, per record with milli second latency, from one topic to another one. Supports stateful and windowing operations by processing one record at a time. Can be integrated in java application. No need for separate processing cluster. It is a Java API. But a Stream app is executed outside of the broker code, which is different than message flow in an ESB. Elastic, highly scalable, fault tolerance, it can recover from failure. An application's processor topology is scaled by breaking it into multiple tasks. Tasks can then instantiate their own processor topology based on the assigned partitions. It is a very important technology to process real-time data for analytics and event processing, developing stateless or stateful processing. In general code for processing event does the following: Set a properties object to specify which brokers to connect to and what kind of serialization to use. Define a stream client: if you want stream of record use KStream, if you want a changelog with the last value of a given key use KTable (Example of using KTable is to keep a user profile with userid as key). In the container shipment implementation we use KTable to keep the Reefer container inventory in memory. Create a topology of input source and sink target and the set of actions to perform in between. Start the stream client to consume records. Programming with KStream and Ktable is not easy at first, as there are a lot of concepts for data manipulations, serialization and operations chaining. It also uses function programming and chaining. A stateful operator uses the streaming Domain Specific Language, and is used for aggregation, join and time window operators. Stateful transformations require a state store associated with the stream processor. The code below comes from Kafka examples and is counting word occurrence in text: final StreamsBuilder builder = new StreamsBuilder(); final Pattern pattern = Pattern.compile(\"\\\\W+\"); KStream<String, String> textLines = builder.stream(source); KTable<String, Long> wordCounts = textLines .flatMapValues(textLine -> Arrays.asList(pattern.split(textLine.toLowerCase()))) .print(Printed.toSysOut() .groupBy((key, word) -> word) .count(Materialized.<String, Long, KeyValueStore<Bytes, byte[]>>as(\"counts-store\")); wordCounts.toStream().to(sink, Produced.with(Serdes.String(), Serdes.Long())); KafkaStreams streams = new KafkaStreams(builder.build(), props); streams.start(); KStream represents KeyValue records coming as event stream from the topic. flatMapValues() transforms the value of each record in \"this\" stream into zero or more values with the same key in a new KStream. So here the text line is split into words. The parameter is a ValueMapper which applies transformation on values but keeps the key. groupBy() Group the records of this KStream on a new key that is selected using the provided KeyValueMapper. So here it creates new KStream with the extracted word as key. count() counts the number of records in this stream by the grouped key. Materialized is an api to define a store to persist state. So here the state store is \"counts-store\". As store is a in-memory table. Produced defines how to provide the optional parameter types when producing to new topics. KTable is an abstraction of a changelog stream from a primary-keyed table. See this article from Confluent for deeper kafka stream architecture presentation. Example to run the Word Count application: Be sure to create the needed different topics once the Kafka broker is started (test-topic, streams-wordcount-output): docker exec -ti Kafka /bin/bash cd /scripts ./createtopics.sh Start a terminal window and execute the command to be ready to send message. $ docker exec -ti Kafka /bin/bash # can use the /scripts/openProducer.sh or... root> /opt/Kafka_2.11-0.10.1.0/bin/Kafka-console-producer.sh --broker-list localhost:9092 --topic streams-plaintext-input Start another terminal to listen to the output topic: $ docker exec -ti Kafka /bin/bash # can use the /scripts/consumeWordCount.sh or... root> /opt/Kafka_2.11-0.10.1.0/bin/Kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic streams-wordcount-output --from-beginning --formatter Kafka.tools.DefaultMessageFormatter --property print.key=true --property print.value=true --property key.deserializer=org.apache.Kafka.common.serialization.StringDeserializer --property value.deserializer=org.apache.Kafka.common.serialization.LongDeserializer Start the stream client to count word in the entered lines mvn exec:java -Dexec.mainClass=ibm.cte.Kafka.play.WordCount Outputs of the WordCount application is actually a continuous stream of updates, where each output record is an updated count of a single word. A KTable is counting the occurrence of word, and a KStream send the output message with updated count. Other examples We have implemented the container microservice of the Container Shipment solution using kstreams processing. See the presentation here , and go to the following code to see tests for the different process flow. Basic kstream processing on order events Further reading The API and product documentation . Deep dive explanation for the differences between KStream and KTable","title":"Event streaming processing"},{"location":"kafka/kafka-stream/#kafka-streaming","text":"Kafka Streams is a graph of processing nodes to implement the logic to process event streams. Each node process events from the parent node. We recommend reading this excellent introduction from Jay Kreps @confluent: Kafka stream made simple to get a good understanding of why Kafka stream was created. To summarize, Kafka Stream has the following capabilities: Stream processing is helpful for handling out-of-order data, reprocessing input as code changes, and performing stateful computations. It uses producer / consumer, stateful storage and consumer groups. It treats both past and future data the same way. Embedded library for your application to use. Integrate tables for state persistence with streams of events. Consumes continuous real time flows of records and publish new flows. Supports exactly-once processing semantics to guarantee that each record will be processed once and only once even when there is a failure. Stream APIs transform, aggregate and enrich data, per record with milli second latency, from one topic to another one. Supports stateful and windowing operations by processing one record at a time. Can be integrated in java application. No need for separate processing cluster. It is a Java API. But a Stream app is executed outside of the broker code, which is different than message flow in an ESB. Elastic, highly scalable, fault tolerance, it can recover from failure. An application's processor topology is scaled by breaking it into multiple tasks. Tasks can then instantiate their own processor topology based on the assigned partitions. It is a very important technology to process real-time data for analytics and event processing, developing stateless or stateful processing. In general code for processing event does the following: Set a properties object to specify which brokers to connect to and what kind of serialization to use. Define a stream client: if you want stream of record use KStream, if you want a changelog with the last value of a given key use KTable (Example of using KTable is to keep a user profile with userid as key). In the container shipment implementation we use KTable to keep the Reefer container inventory in memory. Create a topology of input source and sink target and the set of actions to perform in between. Start the stream client to consume records. Programming with KStream and Ktable is not easy at first, as there are a lot of concepts for data manipulations, serialization and operations chaining. It also uses function programming and chaining. A stateful operator uses the streaming Domain Specific Language, and is used for aggregation, join and time window operators. Stateful transformations require a state store associated with the stream processor. The code below comes from Kafka examples and is counting word occurrence in text: final StreamsBuilder builder = new StreamsBuilder(); final Pattern pattern = Pattern.compile(\"\\\\W+\"); KStream<String, String> textLines = builder.stream(source); KTable<String, Long> wordCounts = textLines .flatMapValues(textLine -> Arrays.asList(pattern.split(textLine.toLowerCase()))) .print(Printed.toSysOut() .groupBy((key, word) -> word) .count(Materialized.<String, Long, KeyValueStore<Bytes, byte[]>>as(\"counts-store\")); wordCounts.toStream().to(sink, Produced.with(Serdes.String(), Serdes.Long())); KafkaStreams streams = new KafkaStreams(builder.build(), props); streams.start(); KStream represents KeyValue records coming as event stream from the topic. flatMapValues() transforms the value of each record in \"this\" stream into zero or more values with the same key in a new KStream. So here the text line is split into words. The parameter is a ValueMapper which applies transformation on values but keeps the key. groupBy() Group the records of this KStream on a new key that is selected using the provided KeyValueMapper. So here it creates new KStream with the extracted word as key. count() counts the number of records in this stream by the grouped key. Materialized is an api to define a store to persist state. So here the state store is \"counts-store\". As store is a in-memory table. Produced defines how to provide the optional parameter types when producing to new topics. KTable is an abstraction of a changelog stream from a primary-keyed table. See this article from Confluent for deeper kafka stream architecture presentation.","title":"Kafka Streaming"},{"location":"kafka/kafka-stream/#example-to-run-the-word-count-application","text":"Be sure to create the needed different topics once the Kafka broker is started (test-topic, streams-wordcount-output): docker exec -ti Kafka /bin/bash cd /scripts ./createtopics.sh Start a terminal window and execute the command to be ready to send message. $ docker exec -ti Kafka /bin/bash # can use the /scripts/openProducer.sh or... root> /opt/Kafka_2.11-0.10.1.0/bin/Kafka-console-producer.sh --broker-list localhost:9092 --topic streams-plaintext-input Start another terminal to listen to the output topic: $ docker exec -ti Kafka /bin/bash # can use the /scripts/consumeWordCount.sh or... root> /opt/Kafka_2.11-0.10.1.0/bin/Kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic streams-wordcount-output --from-beginning --formatter Kafka.tools.DefaultMessageFormatter --property print.key=true --property print.value=true --property key.deserializer=org.apache.Kafka.common.serialization.StringDeserializer --property value.deserializer=org.apache.Kafka.common.serialization.LongDeserializer Start the stream client to count word in the entered lines mvn exec:java -Dexec.mainClass=ibm.cte.Kafka.play.WordCount Outputs of the WordCount application is actually a continuous stream of updates, where each output record is an updated count of a single word. A KTable is counting the occurrence of word, and a KStream send the output message with updated count.","title":"Example to run the Word Count application:"},{"location":"kafka/kafka-stream/#other-examples","text":"We have implemented the container microservice of the Container Shipment solution using kstreams processing. See the presentation here , and go to the following code to see tests for the different process flow. Basic kstream processing on order events","title":"Other examples"},{"location":"kafka/kafka-stream/#further-reading","text":"The API and product documentation . Deep dive explanation for the differences between KStream and KTable","title":"Further reading"},{"location":"kafka/monitoring/","text":"Monitoring Kafka with Prometheus and Grafana Author: Ana Giordano - IBM A comprehensive Kafka monitoring plan should collect metrics from the following components: Kafka Broker(s) Kafka Cluster (which should include ZooKeeper metrics as Kafka relies on it to maintain its state) Producer(s) / Consumer(s) Kafka Broker, Zookeeper and Java clients (producer/consumer) expose metrics via JMX (Java Management Extensions) and can be configured to report stats back to Prometheus using the JMX exporter maintained by Prometheus. There is also a number of exporters maintained by the community to explore. Some of them can be used in addition to the JMX export. To monitor Kafka, for example, the JMX exporter is often used to provide broker level metrics, while community exporters claim to provide more accurate cluster level metrics (e.g. Kafka exporter , Kafka Zookeeper Exporter by CloudFlare , and others). Alternatively, you can consider writing your own custom exporter . What to monitor A long list of metrics is made available by Kafka ( here ) and Zookeeper ( here ). The easiest way to see the available metrics is to fire up jconsole and point it at a running kafka client or Kafka/Prometheus server; this will allow browsing all metrics with JMX. But you are still left to figure out which ones you want to actively monitor and the ones that you want to be actively alerted. An simple way to get started would be to start with the Grafana\u2019s sample dashboards for the Prometheus exporters you chose to use and then modify them as you learn more about the available metrics and/or your environment on ICP. The Monitoring Kafka metrics article by DataDog and How to monitor Kafka by Server Density provides guidance on key Kafka and Prometheus metrics, reasoning to why you should care about them and suggestions on thresholds to trigger alerts. In the next section, we will demonstrate exactly that; we will start with sample dashboards and make few modifications to exemplify how to configure key Kafka metrics to display in the dashboard. Configuring server and agents For convenience and easy configuration, we will use Docker images from DockerHub and make few modifications to DockerFiles to include few additional steps to install, configure and start the servers and exporter agents locally. Kafka and Zookeeper servers with JMX Exporter We will start with the DockerFile of the Spotify kafka image from DockerHub as it includes Zookeeper and Kafka in a single image. The DockerFile was modified as shown below to download, install the Prometheus JMX exporter. The exporter can be configured to scrape and expose mBeans of a JMX target. It runs as a Java Agent, exposing a HTTP server and serving metrics of the JVM. In the DockerFile below, Kafka is started with JMX exporter agent on port 7071 and metrics will be expose in the /metrics endpoint. FROM java:openjdk-8-jre ENV DEBIAN_FRONTEND noninteractive ENV SCALA_VERSION 2.11 ENV KAFKA_VERSION 0.10.2.2 ENV KAFKA_HOME /opt/kafka_\"$SCALA_VERSION\"-\"$KAFKA_VERSION\" # Install Kafka, Zookeeper and other needed things RUN apt-get update && \\ apt-get install -y zookeeper wget supervisor dnsutils vim && \\ rm -rf /var/lib/apt/lists/* && \\ apt-get clean && \\ wget -q http://apache.mirrors.spacedump.net/kafka/\"$KAFKA_VERSION\"/kafka_\"$SCALA_VERSION\"-\"$KAFKA_VERSION\".tgz -O /tmp/kafka_\"$SCALA_VERSION\"-\"$KAFKA_VERSION\".tgz && \\ tar xfz /tmp/kafka_\"$SCALA_VERSION\"-\"$KAFKA_VERSION\".tgz -C /opt && \\ rm /tmp/kafka_\"$SCALA_VERSION\"-\"$KAFKA_VERSION\".tgz ADD scripts/start-kafka.sh /usr/bin/start-kafka.sh # ADD scripts/jmx_prometheus_javaagent-0.9.jar \"$KAFKA_HOME\"/jmx_prometheus_javaagent-0.9.jar # ADD scripts/kafka-0-8-2.yml \"$KAFKA_HOME\"/kafka-0-8-2.yml # Supervisor config ADD supervisor/kafka.conf supervisor/zookeeper.conf /etc/supervisor/conf.d/ # 2181 is zookeeper, 9092 is kafka EXPOSE 2181 9092 # ********** # start - modifications to run Prometheus JMX exporter and community Kafka exporter agents ENV KAFKA_OPTS \"-javaagent:$KAFKA_HOME/jmx_prometheus_javaagent-0.9.jar=7071:$KAFKA_HOME/kafka-0-8-2.yml\" RUN wget -q https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.9/jmx_prometheus_javaagent-0.9.jar -O \"$KAFKA_HOME\"/jmx_prometheus_javaagent-0.9.jar && \\ wget -q https://raw.githubusercontent.com/prometheus/jmx_exporter/master/example_configs/kafka-0-8-2.yml -O \"$KAFKA_HOME\"/kafka-0-8-2.yml EXPOSE 7071 # end - modifications # ********** CMD [\"supervisord\", \"-n\"] For your convenience, the modified DockerFile and scripts are available on this GitHub repository . You can run the following commands to create and run the container locally. download git repo with DockerFile and scripts mkdir /tmp/monitor git clone https://github.com/anagiordano/ibm-artifacts.git /tmp/monitor/. Build image from DockerFile docker build --tag kafka_i /tmp/monitor/kafka/. Create/Run Docker container docker run -d -p 2181:2181 -p 9092:9092 -p 7071:7071 --env ADVERTISED_PORT=9092 --name kafka_c kafka_i Create kafka topics docker exec -it kafka_c /bin/bash cd /opt/kafka*/bin export KAFKA_OPTS=\"\" ./kafka-topics.sh --create --zookeeper localhost:2181 --replication-fact 1 --partitions 1 --topic my-topic1 ./kafka-topics.sh --create --zookeeper localhost:2181 --replication-fact 1 --partitions 1 --topic my-topic2 ./kafka-topics.sh --list --zookeeper localhost:2181 (optional) Produce few message into topics from console and exit container ./kafka-console-producer.sh --broker-list localhost:9092 --topic my-topic1 ./kafka-console-producer.sh --broker-list localhost:9092 --topic my-topic2 exit Lastly you can validate that the /metrics endpoint is returning metrics from Kafka. On a browser, open the http://localhost:7071/metrics URL. Prometheus Server and scrape jobs Prometheus uses a configuration file in YAML format to define the scraping jobs and their instances . You can also use the configuration file to define recording rules and alerting rules : Recording rules allow you to precompute frequently needed or computationally expensive expressions and save their result as a new set of time series. Querying the precomputed result will then often be much faster than executing the original expression every time it is needed. This is especially useful for dashboards, which need to query the same expression repeatedly every time they refresh. Alerting rules allow you to define alert conditions based on Prometheus expression language expressions and to send notifications about firing alerts to an external service. Alerting rules in Prometheus servers send alerts to an Alertmanager. The Alertmanager then manages those alerts, including silencing, inhibition, aggregation and sending out notifications via methods such as email, PagerDuty and others. Below, we will go thru the steps to stand-up a local Prometheus server as a Docker container and to modify the configuration file to scrape Kafka metrics. Create/run a docker container using Prometheus official image from DockerHub docker run -d -p 9090:9090 prom/prometheus Obtain the IP address of the Kafka container docker inspect kafka_c | grep IPAddress Edit the prometheus.yml to add Kafka as a target docker exec -it prometheus_c \\sh vi /etc/prometheus/prometheus.yml Locate the scrape_configs section in the properties file and add the lines below to define the Kafka job, where the IP should be the IP of the kafka container - job_name: 'kafka' static_configs: - targets: ['172.17.0.4:7071'] Reload the configuration file ps -ef kill -HUP <prometheus PID> You can now verify that Kafka is listed as a target job in Prometheus. On a Browser, open the http://localhost:9090/targets URL. Grafana Server and dashboards We will use Grafana for visualization of the metrics scraped by Prometheus for that, we will need to: Stand-up a local Grafana server as a Docker container Configure Prometheus as a data source in Grafana Import sample dashboards provided by Grafana and/or community Modify the sample dashboards as we see fit Let\u2019s get started: Create a docker container using Prometheus official image from DockerHub docker run -d --name=grafana_c -p 3000:3000 grafana/grafana On a Browser, open the http://localhost:3000 URL. Login as admin/admin . You will be prompted to change the password. Once logged in, Grafana provides visual guidance on what the next steps are: a) Add data sources b) Create first dashboard and others Configure Prometheus as a data source: Enter a Name for the data source (e.g. Prometheus) Select Prometheus as Type Enter http://localhost:9090 for HTTP URL In our simple server configuration, select Browser for HTTP Access Click Save and Test to validate configuration Back to Home, click Dashboards -> Manage to import sample dashboards Click the +Import button and paste this URL https://grafana.com/dashboards/721 Make sure to select Prometheus as the data source. NOTE: You can also explore other sample dashboard options at https://grafana.com/dashboards. For instance, there is a Kubernetes Kafka resource metrics sample dashboard that you could use instead as the starting point when configuring Kafka monitoring on ICP. The six graphs displayed in the dashboard are configured as follows: NOTE: You might want to go back to your Kafka Docker container and push messages into the topics you have created above to see changes to the graph. Or, if you have already pushed messages, you can change the Quick Range from last 5 minutes to something else (e.g. last 6 hours ) on the top right hand corner of the dashboard. Graph Formula Format As CPU Usage rate(process_cpu_seconds_total{job=\"kafka\"}[1m]) Time Series JVM Memory Used sum without(area)(jvm_memory_bytes_used{job=\"kafka\"}) Time Series Time spent in GC sum without(gc)(rate(jvm_gc_collection_seconds_sum{job=\"kafka\"}[5m])) Time Series Messages In per Topic sum without(instance)(rate(kafka_server_brokertopicmetrics_messagesin_total{job=\"kafka\",topic!=\"\"}[5m])) Time Series Bytes In per Topic sum without(instance)(rate(kafka_server_brokertopicmetrics_bytesin_total{job=\"kafka\",topic!=\"\"}[5m])) Time Series Bytes Out per Topic sum without(instance)(rate(kafka_server_brokertopicmetrics_bytesout_total{job=\"kafka\",topic!=\"\"}[5m])) Time Series Prometheus provides a functional expression language that lets the user select and aggregate time series data in real time. Before proceeding review the information on these pages to gain basic understanding of: Prometheus Expression language - http://docs.grafana.org/features/datasources/prometheus/ Grafana Query Editor - http://docs.grafana.org/features/datasources/prometheus/ As you make modifications to the dashboard it is also important to understand the data returned by the scrape jobs in the first place. For two of the metrics above, this is what the Kafka JMX exportex returns. You can go to https://localhost:7071/metrics to inspect others returned in /metrics endpoint response: Messages in Per Topic Time spent in GC","title":"Kafka Monitoring"},{"location":"kafka/monitoring/#monitoring-kafka-with-prometheus-and-grafana","text":"Author: Ana Giordano - IBM A comprehensive Kafka monitoring plan should collect metrics from the following components: Kafka Broker(s) Kafka Cluster (which should include ZooKeeper metrics as Kafka relies on it to maintain its state) Producer(s) / Consumer(s) Kafka Broker, Zookeeper and Java clients (producer/consumer) expose metrics via JMX (Java Management Extensions) and can be configured to report stats back to Prometheus using the JMX exporter maintained by Prometheus. There is also a number of exporters maintained by the community to explore. Some of them can be used in addition to the JMX export. To monitor Kafka, for example, the JMX exporter is often used to provide broker level metrics, while community exporters claim to provide more accurate cluster level metrics (e.g. Kafka exporter , Kafka Zookeeper Exporter by CloudFlare , and others). Alternatively, you can consider writing your own custom exporter .","title":"Monitoring Kafka with Prometheus and Grafana"},{"location":"kafka/monitoring/#what-to-monitor","text":"A long list of metrics is made available by Kafka ( here ) and Zookeeper ( here ). The easiest way to see the available metrics is to fire up jconsole and point it at a running kafka client or Kafka/Prometheus server; this will allow browsing all metrics with JMX. But you are still left to figure out which ones you want to actively monitor and the ones that you want to be actively alerted. An simple way to get started would be to start with the Grafana\u2019s sample dashboards for the Prometheus exporters you chose to use and then modify them as you learn more about the available metrics and/or your environment on ICP. The Monitoring Kafka metrics article by DataDog and How to monitor Kafka by Server Density provides guidance on key Kafka and Prometheus metrics, reasoning to why you should care about them and suggestions on thresholds to trigger alerts. In the next section, we will demonstrate exactly that; we will start with sample dashboards and make few modifications to exemplify how to configure key Kafka metrics to display in the dashboard.","title":"What to monitor"},{"location":"kafka/monitoring/#configuring-server-and-agents","text":"For convenience and easy configuration, we will use Docker images from DockerHub and make few modifications to DockerFiles to include few additional steps to install, configure and start the servers and exporter agents locally.","title":"Configuring server and agents"},{"location":"kafka/monitoring/#kafka-and-zookeeper-servers-with-jmx-exporter","text":"We will start with the DockerFile of the Spotify kafka image from DockerHub as it includes Zookeeper and Kafka in a single image. The DockerFile was modified as shown below to download, install the Prometheus JMX exporter. The exporter can be configured to scrape and expose mBeans of a JMX target. It runs as a Java Agent, exposing a HTTP server and serving metrics of the JVM. In the DockerFile below, Kafka is started with JMX exporter agent on port 7071 and metrics will be expose in the /metrics endpoint. FROM java:openjdk-8-jre ENV DEBIAN_FRONTEND noninteractive ENV SCALA_VERSION 2.11 ENV KAFKA_VERSION 0.10.2.2 ENV KAFKA_HOME /opt/kafka_\"$SCALA_VERSION\"-\"$KAFKA_VERSION\" # Install Kafka, Zookeeper and other needed things RUN apt-get update && \\ apt-get install -y zookeeper wget supervisor dnsutils vim && \\ rm -rf /var/lib/apt/lists/* && \\ apt-get clean && \\ wget -q http://apache.mirrors.spacedump.net/kafka/\"$KAFKA_VERSION\"/kafka_\"$SCALA_VERSION\"-\"$KAFKA_VERSION\".tgz -O /tmp/kafka_\"$SCALA_VERSION\"-\"$KAFKA_VERSION\".tgz && \\ tar xfz /tmp/kafka_\"$SCALA_VERSION\"-\"$KAFKA_VERSION\".tgz -C /opt && \\ rm /tmp/kafka_\"$SCALA_VERSION\"-\"$KAFKA_VERSION\".tgz ADD scripts/start-kafka.sh /usr/bin/start-kafka.sh # ADD scripts/jmx_prometheus_javaagent-0.9.jar \"$KAFKA_HOME\"/jmx_prometheus_javaagent-0.9.jar # ADD scripts/kafka-0-8-2.yml \"$KAFKA_HOME\"/kafka-0-8-2.yml # Supervisor config ADD supervisor/kafka.conf supervisor/zookeeper.conf /etc/supervisor/conf.d/ # 2181 is zookeeper, 9092 is kafka EXPOSE 2181 9092 # ********** # start - modifications to run Prometheus JMX exporter and community Kafka exporter agents ENV KAFKA_OPTS \"-javaagent:$KAFKA_HOME/jmx_prometheus_javaagent-0.9.jar=7071:$KAFKA_HOME/kafka-0-8-2.yml\" RUN wget -q https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.9/jmx_prometheus_javaagent-0.9.jar -O \"$KAFKA_HOME\"/jmx_prometheus_javaagent-0.9.jar && \\ wget -q https://raw.githubusercontent.com/prometheus/jmx_exporter/master/example_configs/kafka-0-8-2.yml -O \"$KAFKA_HOME\"/kafka-0-8-2.yml EXPOSE 7071 # end - modifications # ********** CMD [\"supervisord\", \"-n\"] For your convenience, the modified DockerFile and scripts are available on this GitHub repository . You can run the following commands to create and run the container locally. download git repo with DockerFile and scripts mkdir /tmp/monitor git clone https://github.com/anagiordano/ibm-artifacts.git /tmp/monitor/. Build image from DockerFile docker build --tag kafka_i /tmp/monitor/kafka/. Create/Run Docker container docker run -d -p 2181:2181 -p 9092:9092 -p 7071:7071 --env ADVERTISED_PORT=9092 --name kafka_c kafka_i Create kafka topics docker exec -it kafka_c /bin/bash cd /opt/kafka*/bin export KAFKA_OPTS=\"\" ./kafka-topics.sh --create --zookeeper localhost:2181 --replication-fact 1 --partitions 1 --topic my-topic1 ./kafka-topics.sh --create --zookeeper localhost:2181 --replication-fact 1 --partitions 1 --topic my-topic2 ./kafka-topics.sh --list --zookeeper localhost:2181 (optional) Produce few message into topics from console and exit container ./kafka-console-producer.sh --broker-list localhost:9092 --topic my-topic1 ./kafka-console-producer.sh --broker-list localhost:9092 --topic my-topic2 exit Lastly you can validate that the /metrics endpoint is returning metrics from Kafka. On a browser, open the http://localhost:7071/metrics URL.","title":"Kafka and Zookeeper servers with JMX Exporter"},{"location":"kafka/monitoring/#prometheus-server-and-scrape-jobs","text":"Prometheus uses a configuration file in YAML format to define the scraping jobs and their instances . You can also use the configuration file to define recording rules and alerting rules : Recording rules allow you to precompute frequently needed or computationally expensive expressions and save their result as a new set of time series. Querying the precomputed result will then often be much faster than executing the original expression every time it is needed. This is especially useful for dashboards, which need to query the same expression repeatedly every time they refresh. Alerting rules allow you to define alert conditions based on Prometheus expression language expressions and to send notifications about firing alerts to an external service. Alerting rules in Prometheus servers send alerts to an Alertmanager. The Alertmanager then manages those alerts, including silencing, inhibition, aggregation and sending out notifications via methods such as email, PagerDuty and others. Below, we will go thru the steps to stand-up a local Prometheus server as a Docker container and to modify the configuration file to scrape Kafka metrics. Create/run a docker container using Prometheus official image from DockerHub docker run -d -p 9090:9090 prom/prometheus Obtain the IP address of the Kafka container docker inspect kafka_c | grep IPAddress Edit the prometheus.yml to add Kafka as a target docker exec -it prometheus_c \\sh vi /etc/prometheus/prometheus.yml Locate the scrape_configs section in the properties file and add the lines below to define the Kafka job, where the IP should be the IP of the kafka container - job_name: 'kafka' static_configs: - targets: ['172.17.0.4:7071'] Reload the configuration file ps -ef kill -HUP <prometheus PID> You can now verify that Kafka is listed as a target job in Prometheus. On a Browser, open the http://localhost:9090/targets URL.","title":"Prometheus Server and scrape jobs"},{"location":"kafka/monitoring/#grafana-server-and-dashboards","text":"We will use Grafana for visualization of the metrics scraped by Prometheus for that, we will need to: Stand-up a local Grafana server as a Docker container Configure Prometheus as a data source in Grafana Import sample dashboards provided by Grafana and/or community Modify the sample dashboards as we see fit Let\u2019s get started: Create a docker container using Prometheus official image from DockerHub docker run -d --name=grafana_c -p 3000:3000 grafana/grafana On a Browser, open the http://localhost:3000 URL. Login as admin/admin . You will be prompted to change the password. Once logged in, Grafana provides visual guidance on what the next steps are: a) Add data sources b) Create first dashboard and others Configure Prometheus as a data source: Enter a Name for the data source (e.g. Prometheus) Select Prometheus as Type Enter http://localhost:9090 for HTTP URL In our simple server configuration, select Browser for HTTP Access Click Save and Test to validate configuration Back to Home, click Dashboards -> Manage to import sample dashboards Click the +Import button and paste this URL https://grafana.com/dashboards/721 Make sure to select Prometheus as the data source. NOTE: You can also explore other sample dashboard options at https://grafana.com/dashboards. For instance, there is a Kubernetes Kafka resource metrics sample dashboard that you could use instead as the starting point when configuring Kafka monitoring on ICP. The six graphs displayed in the dashboard are configured as follows: NOTE: You might want to go back to your Kafka Docker container and push messages into the topics you have created above to see changes to the graph. Or, if you have already pushed messages, you can change the Quick Range from last 5 minutes to something else (e.g. last 6 hours ) on the top right hand corner of the dashboard. Graph Formula Format As CPU Usage rate(process_cpu_seconds_total{job=\"kafka\"}[1m]) Time Series JVM Memory Used sum without(area)(jvm_memory_bytes_used{job=\"kafka\"}) Time Series Time spent in GC sum without(gc)(rate(jvm_gc_collection_seconds_sum{job=\"kafka\"}[5m])) Time Series Messages In per Topic sum without(instance)(rate(kafka_server_brokertopicmetrics_messagesin_total{job=\"kafka\",topic!=\"\"}[5m])) Time Series Bytes In per Topic sum without(instance)(rate(kafka_server_brokertopicmetrics_bytesin_total{job=\"kafka\",topic!=\"\"}[5m])) Time Series Bytes Out per Topic sum without(instance)(rate(kafka_server_brokertopicmetrics_bytesout_total{job=\"kafka\",topic!=\"\"}[5m])) Time Series Prometheus provides a functional expression language that lets the user select and aggregate time series data in real time. Before proceeding review the information on these pages to gain basic understanding of: Prometheus Expression language - http://docs.grafana.org/features/datasources/prometheus/ Grafana Query Editor - http://docs.grafana.org/features/datasources/prometheus/ As you make modifications to the dashboard it is also important to understand the data returned by the scrape jobs in the first place. For two of the metrics above, this is what the Kafka JMX exportex returns. You can go to https://localhost:7071/metrics to inspect others returned in /metrics endpoint response: Messages in Per Topic Time spent in GC","title":"Grafana Server and dashboards"},{"location":"kafka/producers/","text":"Producers considerations A producer is a thread safe kafka client API that publishes records to the cluster. It uses buffers, thread pool, and serializer to send data. They are stateless. This is the consumers that are managing the offsets. The assignment of messae to partition is done following different algorithms: round-robin, simple load balancing, or custom defined. Producers are more simple to implement but still you need to assess some design considerations. Design considerations When developing a record producer you need to assess the following: What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size. By default, the buffer size is set at 32Mb, but can be configured with buffer.memory . (See producer configuration API Can the producer batch events together to send them in batch over one send operation? Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size, and ensure to have at least 3 brokers and even 5 to maintain quorum in case of one failure. The client API is implemented to support reconnection. Assess exactly once delivery requirement. Look at idempotent producer: retries will not introduce duplicate records. Where the event timestamp comes from? Should the producer send operation set it or is it loaded from external data? Remember that LogAppendTime is considered to be processing time, and CreateTime is considered to be event time. See related discussions on confluent web site. Typical code structure The producer code does the following steps: define producer properties create a producer instance send event records and get resulting metadata. Producers are thread safe. The send() operation is asynchronous and returns immediately once record has been stored in the buffer of records, and it is possible to add a callback to process the broker acknowledgement. Kafka useful Producer APIs Here is a list of common API to use in your producer and consumer code. KafkaProducer A Kafka client that publishes records to the Kafka cluster. The send method is asynchronous. A producer is thread safe so we can have per topic to interface. ProducerRecord to be published to a topic RecordMetadata metadata for a record that has been acknowledged by the server. Properties to consider The following properties are helpful to tune at each topic and producer and will vary depending on the deployment: Properties Description BOOTSTRAP_SERVERS_CONFIG A comma-separated list of host:port values for all the brokers deployed. So producer may use any brokers KEY_SERIALIZER_CLASS_CONFIG and VALUE_SERIALIZER_CLASS_CONFIG convert the keys and values into byte arrays. Using default String serializer should be a good solution for Json payload. For streaming app, use customer serializer. ACKS_CONFIG specifies the minimum number of acknowledgments from a broker that the producer will wait for before considering a record send completed. Values = all, 0, and 1. 0 is for fire and forget. RETRIES_CONFIG specifies the number of times to attempt to resend a batch of events. ENABLE_IDEMPOTENCE_CONFIG Set to true, the number of retries will be maximized, and the acks will be set to All . How to support exactly once delivery? Knowing that exactly once delivery is one of the hardest problems to solve in distributed systems, how kafka does it?. Broker can fail or a network may respond slowly while a producer is trying to send events. Producer can set acknowledge level to control the delivery semantic: At least once: means the producer set ACKS_CONFIG=all and get an acknowledgement message when the message has been written at least one time in the cluster (assume replicas = 3). If the ack is not received, the producer may retry, which may generate duplicate records in case the broker stops after saving to the topic and before sending back the acknowledgement message. At most semantic: means the producer will not do retry in case of no acknowldege received. It may create log and compensation, but the message is lost. Exactly once means even if the producer sends the message twice the system will send only one message to the consumer. Once the consumer commits the read offset, it will not receive the message again, even if it restarts. Consumer offset needs to be in sync with produced event. With the identpotence property (ENABLE_IDEMPOTENCE_CONFIG = true), the record sent has a sequence number, that the broker will consider to avoid having duplicate records per partition. The sequence number is persisted in a log so event in case of broker leader failure, the new leader will have a good view of the states of the system. Note The replication mechanism guarantees that when a message is written to the leader replica, it will be replicated to all available replicas. To add to these, as topic may have multiple partitions, kafka supports atomic writes to all partitions, so that all records are saved or none of them are visible to consumers. This transaction control is done by using the producer transactional API, and a unique transaction identifier to keep integrated state. The consumer is also interested to configure the reading of the transactional messages by defining the isolation level. Read this article from confluent. Code Examples Simple text message A Pump simulator Ship movement and container metrics event producers More readings * Creating advanced kafka producer in java - Cloudurable","title":"Kafka producer"},{"location":"kafka/producers/#producers-considerations","text":"A producer is a thread safe kafka client API that publishes records to the cluster. It uses buffers, thread pool, and serializer to send data. They are stateless. This is the consumers that are managing the offsets. The assignment of messae to partition is done following different algorithms: round-robin, simple load balancing, or custom defined. Producers are more simple to implement but still you need to assess some design considerations.","title":"Producers considerations"},{"location":"kafka/producers/#design-considerations","text":"When developing a record producer you need to assess the following: What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size. By default, the buffer size is set at 32Mb, but can be configured with buffer.memory . (See producer configuration API Can the producer batch events together to send them in batch over one send operation? Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size, and ensure to have at least 3 brokers and even 5 to maintain quorum in case of one failure. The client API is implemented to support reconnection. Assess exactly once delivery requirement. Look at idempotent producer: retries will not introduce duplicate records. Where the event timestamp comes from? Should the producer send operation set it or is it loaded from external data? Remember that LogAppendTime is considered to be processing time, and CreateTime is considered to be event time. See related discussions on confluent web site.","title":"Design considerations"},{"location":"kafka/producers/#typical-code-structure","text":"The producer code does the following steps: define producer properties create a producer instance send event records and get resulting metadata. Producers are thread safe. The send() operation is asynchronous and returns immediately once record has been stored in the buffer of records, and it is possible to add a callback to process the broker acknowledgement.","title":"Typical code structure"},{"location":"kafka/producers/#kafka-useful-producer-apis","text":"Here is a list of common API to use in your producer and consumer code. KafkaProducer A Kafka client that publishes records to the Kafka cluster. The send method is asynchronous. A producer is thread safe so we can have per topic to interface. ProducerRecord to be published to a topic RecordMetadata metadata for a record that has been acknowledged by the server.","title":"Kafka useful Producer APIs"},{"location":"kafka/producers/#properties-to-consider","text":"The following properties are helpful to tune at each topic and producer and will vary depending on the deployment: Properties Description BOOTSTRAP_SERVERS_CONFIG A comma-separated list of host:port values for all the brokers deployed. So producer may use any brokers KEY_SERIALIZER_CLASS_CONFIG and VALUE_SERIALIZER_CLASS_CONFIG convert the keys and values into byte arrays. Using default String serializer should be a good solution for Json payload. For streaming app, use customer serializer. ACKS_CONFIG specifies the minimum number of acknowledgments from a broker that the producer will wait for before considering a record send completed. Values = all, 0, and 1. 0 is for fire and forget. RETRIES_CONFIG specifies the number of times to attempt to resend a batch of events. ENABLE_IDEMPOTENCE_CONFIG Set to true, the number of retries will be maximized, and the acks will be set to All .","title":"Properties to consider"},{"location":"kafka/producers/#how-to-support-exactly-once-delivery","text":"Knowing that exactly once delivery is one of the hardest problems to solve in distributed systems, how kafka does it?. Broker can fail or a network may respond slowly while a producer is trying to send events. Producer can set acknowledge level to control the delivery semantic: At least once: means the producer set ACKS_CONFIG=all and get an acknowledgement message when the message has been written at least one time in the cluster (assume replicas = 3). If the ack is not received, the producer may retry, which may generate duplicate records in case the broker stops after saving to the topic and before sending back the acknowledgement message. At most semantic: means the producer will not do retry in case of no acknowldege received. It may create log and compensation, but the message is lost. Exactly once means even if the producer sends the message twice the system will send only one message to the consumer. Once the consumer commits the read offset, it will not receive the message again, even if it restarts. Consumer offset needs to be in sync with produced event. With the identpotence property (ENABLE_IDEMPOTENCE_CONFIG = true), the record sent has a sequence number, that the broker will consider to avoid having duplicate records per partition. The sequence number is persisted in a log so event in case of broker leader failure, the new leader will have a good view of the states of the system. Note The replication mechanism guarantees that when a message is written to the leader replica, it will be replicated to all available replicas. To add to these, as topic may have multiple partitions, kafka supports atomic writes to all partitions, so that all records are saved or none of them are visible to consumers. This transaction control is done by using the producer transactional API, and a unique transaction identifier to keep integrated state. The consumer is also interested to configure the reading of the transactional messages by defining the isolation level. Read this article from confluent.","title":"How to support exactly once delivery?"},{"location":"kafka/producers/#code-examples","text":"Simple text message A Pump simulator Ship movement and container metrics event producers","title":"Code Examples"},{"location":"kafka/producers/#more-readings","text":"* Creating advanced kafka producer in java - Cloudurable","title":"More readings"},{"location":"kafka/readme/","text":"Apache Kafka In this article we are summarizing what Apache Kafka is and group some references and notes we gathered during our different implementations and Kafka deployment within Kubernetes cluster. We are documenting how to deploy Kafka on IBM Cloud Private or deploying IBM Event Streams product . This content does not replace the excellent introduction every developer using Kafka should read. Update 06/2019 - Author: Jerome Boyer Introduction Kafka is a distributed real time event streaming platform with the following key capabilities: Publish and subscribe streams of records. Data are stored so consuming applications can pull the information they need, and keep track of what they have seen so far. It can handle hundred of reads and writes operation per second from many producers and consumers Atomic broadcast, send a record once, every subscriber gets it once. Store streams of data records on disk and replicate within the distributed cluster for fault-tolerance. Keep data for a time period before delete. Can grow elastically and transparently with no downtime Built on top of the ZooKeeper synchronization service to keep topic, partitions and metadata highly available. Use cases The typical use cases where Kafka helps are: Centralize online data pipeline to decouple applications and microservices pub/sub messaging Aggregation of event coming from multiple producers. Monitor distributed applications to produce centralized feed of operational data. Logs collector from multiple services Implement event soucing pattern out of the box, using configuration to keep message for a long time period. Data are replicated between broker within the cluster and cross availability zones if needed. Manage loosely coupled communication between microservices. (See this note where I present a way to support a service mesh solution using asynchronous event) Key concepts The diagram below presents Kafka's key components: Brokers Kafka runs as a cluster of one or more broker servers that can, in theory, span multiple data centers. It is really possible if the latency is very low at the 10ms or better as there are a lot of communication between kafka brokers and kafka and zookeepers. The Kafka cluster stores streams of records in topics . Topic is referenced by producer to send data too, and subscribed by consumers to get data. In the figure above, the Kafka brokers are allocated on three servers, with data within the topic are replicated three times. In production, it is recommended to use five nodes to authorise planned failure and un-planned failure. Topics Topics represent end points to put or get records to. Each record consists of a key, a value, and a timestamp. Producers publish data records to topic and consumers subscribe to topics. When a record is produced without specifying a partition, a partition will be chosen using a hash of the key. If the record did not provide a timestamp, the producer will stamp the record with its current time (creation time or log append time). Producers hold a pool of buffer to keep records not yet transmitted to the server. Kafka store log data in its log.dir and topic maps to subdirectories in this log directory. Kafka uses topics with a pub/sub combined with queue model: it uses the concept of consumer group to divide the processing over a collection of consumer processes, running in parallel, and messages can be broadcasted to multiple groups. Consumer performs asynchronous pull to the connected broker via the subscription to a topic. The figure below illustrates one topic having multiple partitions, replicated within the broker cluster: Partitions Partitions are used by producers and consumers and data replication. Partitions are basically used to parallelize the event processing when a single server would not be able to process all events, using the broker clustering. So to manage increase in the load of messages Kafka uses partitions. Each broker may have zero or more partitions per topic. When creating topic we specify the number of partition to use. Each partition will run on a separate server. If you have 5 brokers you can define topic with 5 partitions. Kafka tolerates up to N-1 server failure without losing any messages. N is the replication factor for a given parition. Each partition is a time ordered immutable sequence of records, that are persisted for a long time period. It is a log. Topic is a labelled log. Consumers see messages in the order they are stored in the log. Each partition is replicated across a configurable number of servers for fault tolerance. The number of partition will depend on characteristics like the number of consumers, the traffic pattern, etc... Each partitioned message has a unique sequence id called offset (\"abcde, ab, a ...\" in the figure above are offsets). Those offset ids are defined when events arrived at the broker level, and are local to the partition. They are unmutable. When a consumer reads a topic, it actually reads data from all the partitions. As a consumer reads data from a partition, it advances its offset. To read an event the consumer needs to use the topic name, the partition number and the last offset to read from. As brokers are stateless, the consumers are responsible to keep track of the offsets. Partitions guarantee that data with the same keys will be sent to the same consumer and in order. Adding more partition, in the limit of number of brokers, improve throughtput. Replication Each partition can be replicated accross a number of server. The replication factor is capted by the number of brokers. Partitions have one leader and zero or more followers. The leader manages all the read and write requests for the partition. Leader is also responsible to track the in sync replicas. The followers replicate the leader content. If a leader fails, followers elect a new one. When a producer sends message, he can control how to get the response from the committed message: wait for all replicas to succeed. Consumers receive only committed messages. Zookeeper Zookeeper is used to persist the component and platform states and it runs in cluster to ensure high availability. One zookeeper server is the leader and other are used in backup. Kafka does not keep state regarding consumers and producers. Depends on kafka version, offsets are maintained in Zookeeper or in Kafka : newer versions use an internal Kafka topic called __consumer_offsets. In any case consumers can read next message (or from a specific offset) correctly even during broker server outrages. Access Controls are saved in Zookeeper Consumer group This is the way to group consumers so the processing of event is parallelized. The number of consumers in a group is the same as the number of partition defined in a topic. We are detailinh consumer group implementation in this note Architecture See this separate note Solution considerations There are a set of design considerations to assess for each Kafka solution: Topics Performance is more a function of number of partitions than topics. Expect that each topic has at least one partition. When considering latency you should aim for limiting to hundreds of topic-partition per broker node. What of the most important question is what topics to use?. What is an event type? Should we use one topic to support multiple event types? Let define that an event type is linked to a main business entity like an Order, a ship, a FridgeredContainer. OrderCreated, OrderCancelled, OrderUpdated, OrderClosed are events linked to the states of the Order. The order of those events matter. So the natural approach is to use one topic per data type or schema, specially when using the topic as Event Sourcing where event order is important to build the audit log. You will use a unique partition to support that. The orderID is the partition key and all events related to the order are in the same topic. The important requirement to consider is the sequencing or event order. When event order is very important then use a unique partition, and use the entity unique identifier as key. Ordering is not preserved across partitions. When dealing with entity, independent entities may be in separate topics, when strongly related one may stay together. Other best practices: When event order is important use the same topic and use the entity unique identifier as partition key. When two entities are related together by containment relationship then they can be in the same topic. Different entities are separated to different topics. It is possible to group topics in coarse grained one when we discover that several consumers are listening to the same topics. Clearly define the partition key as it could be an compound key based on multiple entities. With Kafka stream, state store or KTable, you should separate the changelog topic from the others. Producers When developing a record producer you need to assess the following: What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size. Can the producer batch events together to send them in batch over one send operation? Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size Assess once to exactly once delivery requirement. Look at idempotent producer. See implementation considerations discussion Consumers From the consumer point of view a set of items need to be addressed during design phase: Do you need to group consumers for parallel consumption of events? What is the processing done once the record is processed out of the topic? And how a record is supposed to be consumed?. How to persist consumer committed position? (the last offset that has been stored securely) Assess if offsets need to be persisted outside of Kafka?. From version 0.9 offset management is more efficient, and synchronous or asynchronous operations can be done from the consumer code. Does record time sensitive, and it is possible that consumers fall behind, so when a consumer restarts he can bypass missed records? Do the consumer needs to perform joins, aggregations between multiple partitions? See implementation considerations discussion See also the compendium note for more readings.","title":"Kafka concepts summary"},{"location":"kafka/readme/#apache-kafka","text":"In this article we are summarizing what Apache Kafka is and group some references and notes we gathered during our different implementations and Kafka deployment within Kubernetes cluster. We are documenting how to deploy Kafka on IBM Cloud Private or deploying IBM Event Streams product . This content does not replace the excellent introduction every developer using Kafka should read. Update 06/2019 - Author: Jerome Boyer","title":"Apache Kafka"},{"location":"kafka/readme/#introduction","text":"Kafka is a distributed real time event streaming platform with the following key capabilities: Publish and subscribe streams of records. Data are stored so consuming applications can pull the information they need, and keep track of what they have seen so far. It can handle hundred of reads and writes operation per second from many producers and consumers Atomic broadcast, send a record once, every subscriber gets it once. Store streams of data records on disk and replicate within the distributed cluster for fault-tolerance. Keep data for a time period before delete. Can grow elastically and transparently with no downtime Built on top of the ZooKeeper synchronization service to keep topic, partitions and metadata highly available.","title":"Introduction"},{"location":"kafka/readme/#use-cases","text":"The typical use cases where Kafka helps are: Centralize online data pipeline to decouple applications and microservices pub/sub messaging Aggregation of event coming from multiple producers. Monitor distributed applications to produce centralized feed of operational data. Logs collector from multiple services Implement event soucing pattern out of the box, using configuration to keep message for a long time period. Data are replicated between broker within the cluster and cross availability zones if needed. Manage loosely coupled communication between microservices. (See this note where I present a way to support a service mesh solution using asynchronous event)","title":"Use cases"},{"location":"kafka/readme/#key-concepts","text":"The diagram below presents Kafka's key components:","title":"Key concepts"},{"location":"kafka/readme/#brokers","text":"Kafka runs as a cluster of one or more broker servers that can, in theory, span multiple data centers. It is really possible if the latency is very low at the 10ms or better as there are a lot of communication between kafka brokers and kafka and zookeepers. The Kafka cluster stores streams of records in topics . Topic is referenced by producer to send data too, and subscribed by consumers to get data. In the figure above, the Kafka brokers are allocated on three servers, with data within the topic are replicated three times. In production, it is recommended to use five nodes to authorise planned failure and un-planned failure.","title":"Brokers"},{"location":"kafka/readme/#topics","text":"Topics represent end points to put or get records to. Each record consists of a key, a value, and a timestamp. Producers publish data records to topic and consumers subscribe to topics. When a record is produced without specifying a partition, a partition will be chosen using a hash of the key. If the record did not provide a timestamp, the producer will stamp the record with its current time (creation time or log append time). Producers hold a pool of buffer to keep records not yet transmitted to the server. Kafka store log data in its log.dir and topic maps to subdirectories in this log directory. Kafka uses topics with a pub/sub combined with queue model: it uses the concept of consumer group to divide the processing over a collection of consumer processes, running in parallel, and messages can be broadcasted to multiple groups. Consumer performs asynchronous pull to the connected broker via the subscription to a topic. The figure below illustrates one topic having multiple partitions, replicated within the broker cluster:","title":"Topics"},{"location":"kafka/readme/#partitions","text":"Partitions are used by producers and consumers and data replication. Partitions are basically used to parallelize the event processing when a single server would not be able to process all events, using the broker clustering. So to manage increase in the load of messages Kafka uses partitions. Each broker may have zero or more partitions per topic. When creating topic we specify the number of partition to use. Each partition will run on a separate server. If you have 5 brokers you can define topic with 5 partitions. Kafka tolerates up to N-1 server failure without losing any messages. N is the replication factor for a given parition. Each partition is a time ordered immutable sequence of records, that are persisted for a long time period. It is a log. Topic is a labelled log. Consumers see messages in the order they are stored in the log. Each partition is replicated across a configurable number of servers for fault tolerance. The number of partition will depend on characteristics like the number of consumers, the traffic pattern, etc... Each partitioned message has a unique sequence id called offset (\"abcde, ab, a ...\" in the figure above are offsets). Those offset ids are defined when events arrived at the broker level, and are local to the partition. They are unmutable. When a consumer reads a topic, it actually reads data from all the partitions. As a consumer reads data from a partition, it advances its offset. To read an event the consumer needs to use the topic name, the partition number and the last offset to read from. As brokers are stateless, the consumers are responsible to keep track of the offsets. Partitions guarantee that data with the same keys will be sent to the same consumer and in order. Adding more partition, in the limit of number of brokers, improve throughtput.","title":"Partitions"},{"location":"kafka/readme/#replication","text":"Each partition can be replicated accross a number of server. The replication factor is capted by the number of brokers. Partitions have one leader and zero or more followers. The leader manages all the read and write requests for the partition. Leader is also responsible to track the in sync replicas. The followers replicate the leader content. If a leader fails, followers elect a new one. When a producer sends message, he can control how to get the response from the committed message: wait for all replicas to succeed. Consumers receive only committed messages.","title":"Replication"},{"location":"kafka/readme/#zookeeper","text":"Zookeeper is used to persist the component and platform states and it runs in cluster to ensure high availability. One zookeeper server is the leader and other are used in backup. Kafka does not keep state regarding consumers and producers. Depends on kafka version, offsets are maintained in Zookeeper or in Kafka : newer versions use an internal Kafka topic called __consumer_offsets. In any case consumers can read next message (or from a specific offset) correctly even during broker server outrages. Access Controls are saved in Zookeeper","title":"Zookeeper"},{"location":"kafka/readme/#consumer-group","text":"This is the way to group consumers so the processing of event is parallelized. The number of consumers in a group is the same as the number of partition defined in a topic. We are detailinh consumer group implementation in this note","title":"Consumer group"},{"location":"kafka/readme/#architecture","text":"See this separate note","title":"Architecture"},{"location":"kafka/readme/#solution-considerations","text":"There are a set of design considerations to assess for each Kafka solution:","title":"Solution considerations"},{"location":"kafka/readme/#topics_1","text":"Performance is more a function of number of partitions than topics. Expect that each topic has at least one partition. When considering latency you should aim for limiting to hundreds of topic-partition per broker node. What of the most important question is what topics to use?. What is an event type? Should we use one topic to support multiple event types? Let define that an event type is linked to a main business entity like an Order, a ship, a FridgeredContainer. OrderCreated, OrderCancelled, OrderUpdated, OrderClosed are events linked to the states of the Order. The order of those events matter. So the natural approach is to use one topic per data type or schema, specially when using the topic as Event Sourcing where event order is important to build the audit log. You will use a unique partition to support that. The orderID is the partition key and all events related to the order are in the same topic. The important requirement to consider is the sequencing or event order. When event order is very important then use a unique partition, and use the entity unique identifier as key. Ordering is not preserved across partitions. When dealing with entity, independent entities may be in separate topics, when strongly related one may stay together. Other best practices: When event order is important use the same topic and use the entity unique identifier as partition key. When two entities are related together by containment relationship then they can be in the same topic. Different entities are separated to different topics. It is possible to group topics in coarse grained one when we discover that several consumers are listening to the same topics. Clearly define the partition key as it could be an compound key based on multiple entities. With Kafka stream, state store or KTable, you should separate the changelog topic from the others.","title":"Topics"},{"location":"kafka/readme/#producers","text":"When developing a record producer you need to assess the following: What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size. Can the producer batch events together to send them in batch over one send operation? Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size Assess once to exactly once delivery requirement. Look at idempotent producer. See implementation considerations discussion","title":"Producers"},{"location":"kafka/readme/#consumers","text":"From the consumer point of view a set of items need to be addressed during design phase: Do you need to group consumers for parallel consumption of events? What is the processing done once the record is processed out of the topic? And how a record is supposed to be consumed?. How to persist consumer committed position? (the last offset that has been stored securely) Assess if offsets need to be persisted outside of Kafka?. From version 0.9 offset management is more efficient, and synchronous or asynchronous operations can be done from the consumer code. Does record time sensitive, and it is possible that consumers fall behind, so when a consumer restarts he can bypass missed records? Do the consumer needs to perform joins, aggregations between multiple partitions? See implementation considerations discussion See also the compendium note for more readings.","title":"Consumers"},{"location":"methodology/ddd/","text":"Event and Domain Driven Design Event storming is part of the domain driven design methodology. There are a lot of literatures on domain-driven design, starting by Eric Evans's book from 2004. At IBM we also summarized the concepts needed for implementing microservice in Kyle Brown's article . UNDER CONSTRUCTION","title":"Domain driven design"},{"location":"methodology/ddd/#event-and-domain-driven-design","text":"Event storming is part of the domain driven design methodology. There are a lot of literatures on domain-driven design, starting by Eric Evans's book from 2004. At IBM we also summarized the concepts needed for implementing microservice in Kyle Brown's article . UNDER CONSTRUCTION","title":"Event and Domain Driven Design"},{"location":"methodology/readme/","text":"Event driven solution implementation methodology In this article we are presenting an end to end set of activities to run a successful Minimum Viable Product for an event-driven solution using cloud native microservices and event backbone as the core technology approach. The discovery and analysis of the MVP scope starts with an event storming workshop where designer, architect work hand to hand with business users and domain subject matter experts. From the different outcomes of the workshop, the development team starts to outline components, microservices, business entity life cycle, etc... in a short design iteration . The scope is well defined Epics, Hill and user stories defined, at least for the first iterations, and the MVP can start. Event Storming workshop Event storming is a workshop format for quickly exploring complex business domains by focusing on domain events generated in the context of a business process or a business application. A domain event is something meaningful to the experts that happened in the domain. The workshop focuses on communication between product owner, domain experts and developers. The event storming method was introduced and publicized by Alberto Brandolini in \"Introducing event storming book\" . This approach is recognized in the Domain Driven Design (DDD) community as a technique for rapid capture of a solution design and improved team understanding of the domain. Domain represents some are of the business that has the analysis focus. This article outlines the method and describes refinements and extensions that are useful in designs for an event-driven architecture. This extended approach adds an insight storming step to identify and capture value adding predictive insights about possible future events. The predictive insights are generated by using data science analysis, data models, artificial intelligence (AI), or machine learning (ML). This article describes in general terms all the steps to run an event storming workshop. The output of an actual workshop done on a sample problem - a world wide container shipment, is further detailed in the container shipment analysis example . Conducting the event and insight storming workshop Before conducting an event storming workshop, complete a Design Thinking Workshop in which Personas and Empathy Maps are developed and business pains and goals are defined. The event storming workshop adds more specific design on the events occuring at each step of the process, natural contexts for microservices and predictive insights to guide operation of the system. With this approach, a team that includes business owners and stakeholders can define a Minimal Viable Prototype (MVP) design for the solution. The resulting design is organized as a collection of loosely coupled microservices linked through an event-driven architecture and one or more event backbones. This style of design can be deployed into multicloud execution environments and allows for scaling and agile deployment. Preparations for the event storming workshop include the following steps: Get a room big enough to hold at least 6 to 8 persons and with enough wall space on which to stick big paper sheets: you will need a lot of wall space to define the models. Obtain green, orange, blue, and red square sticky notes, black sharpie pens and blue painter's tape. Discourage the use of open laptops during the meeting. Limit the number of chairs so that the team stays focused and connected and conversation flows easily. Concepts Many of the concepts addressed during the event storming workshop are defined in the Domain Driven Design approach. The following diagrams present the elements used during the analysis. The first diagram shows the initial set of concepts that are used in the process. Domain events are also named business events . An event is some action or happening which occurred in the system at a specific time in the past. The first step in the event storming process consists of these actions: Identifying all relevant events in the domain and specific process being analyzed, Writing a very short description of each event on a \"sticky\" note and placing all the event \"sticky\" notes in sequence on a timeline. The act of writing event descriptions often results in questions to be resolved later, or discussions about definitions that need to be recorded to ensure that everyone agrees on basic domain concepts. A timeline of domain events is the critical output of the first step in the event storming process. The timeline gives everyone a common understanding of when events take place in relation to each other. You still need to be able to take this initial level of understanding and move it towards an implementation. In making that step, you must expand your thinking to encompass the idea of a command, which is the action that kicks off the processing that triggers an event. As part of understanding the role of the command, you will also want to know who invokes a command (actors) and what information is needed to allow the command to be executed. This diagram show how those analysis elements are linked together: One-View Figure. Actors consume data by using a user interface and use the UI to interact with the system via commands. Actors could also be replace by articial intelligent agents. Commands are the result of some user decision or policy, and act on relevant data which are part of a Read model in the CQRS pattern. Policies (represented by lilac stickies) are reactive logics that take place after an event occurs, and trigger other commands. Policies always start with the phrase \"whenever...\". They can be a manual step a human follows, such as a documented procedure or guidance, or they may be automated. When applying the Agile Business Rule Development methodology it will be mapped to a Decision within the Decision Model Notation . External systems produce events. Data can be presented to users in a user interface or modified by the system. Events can be created by commands or by external systems including IOT devices. They can be triggerred by the processing of other events or by some period of elapsed time. When an event is repeated or occurs regularly on a schedule, draw a clock or calendar icon in the corner of the sticky note for that event. As the events are identified and sequenced into a time line, you might find multiple independent subsequences that are not directly coupled to each other and thatrepresent different perspectives of the system, but occur in overlapped periods of time. These parallel event streams can be addressed by putting them into separate swimlanes delineated by using horizontal blue painter's tape. As the events are organized into a timeline, possibly with swim lanes, you can identify pivotal events. Pivotal events indicate major changes in the domain and often form the boundary between one phase of the system and another. Pivotal events will typically separate (a bounded context in DDD terms). Pivotal events are identified with vertical blue painters tape (crossing all the swimlanes). An example of a section of a completed event time line with pivotal events and swimlanes is shown below. Conducting the workshop The goal of the workshop is to better understand the business problem to address with a future application. But the approach can also apply to finding solutions to bottlenecks or other issues in existing applications. The workshop helps the team to understand the big picture of the solution by building a timeline of domain events as they occur during the business process life span. During the workshop, avoid documenting processing steps. The event storming method is not trying to specify a particular implementation. Instead, the focus in initial stages of the workshop is on identifying and sequencing the events that occur in the solution. The event timeline is a useful representation of the overall steps, communicating what must happen while remaining open to many possible implementation approaches. Step 1: Domain events discovery Begin by writing each domain event on an orange sticky note with a few words and a verb in a past tense. Describe What's happened . At first just \"storm\" the events by having each domain expert generate an individual lists of domain events. You might not need to initially place the events on the ordered timeline as they write them. The events must be worded in a way that is meaningful to the domain experts and business stakeholder. You are explaining what happens in business terms, not what happens inside the implementation of the system. You don't need to describe all the events in your domain, but you must cover the process that you are interested in exploring from end to end. Therefore, make sure that you identify the start and end events and place them on the timeline at the beginning and end of the wall covered with paper. Place the other events that you identified between these two endpoints in the closest approximation that the team can agree to a sequential order. Don\u2019t worry about overlaps at this point; overlaps are addressed later. Step 2: Tell the story In this step, you retell the story by talking about how to relate events to particular personas. A member of the team (often the facilitator, but others can do this as well) acts this out by taking on the perspective of a persona in the domain, such as a \"manufacturer\" who wants to ship a widget to a customer, and asking which events follow which other events. Start at the beginning of that persona's interaction and ask \"what happens next?\". Pick up and rearrange the events that the team storms. If you discover events that are duplicates, take those off the board. If events are in the wrong order, move them into the right order. When some parts are unclear, add questions or comments by using the red stickies.. Red stickies indicate that the team needs to follow up and clarify issues later. Likewise you want to use this time to document assumptions on the definition stickies. This is also a good time to rephrase events as you proceed through the story. Sometimes you need to rephrase an event description by putting the verbing in past tense, or adjusting the terms that are used to relate clearly to other identified events. In this step you focus on the mainline \"happy\" end-to-end path to avoid getting bogged down in details of exceptions and error handling. Exceptions can be added later Step 3: Find the Boundaries The next step of this part of the process is to find the boundaries of your system by looking at the events. Two types of boundaries can emerge; the first type of boundary is a time boundary. Often specific key \"pivotal events\" indicate a change from one aspect of a system to another. This can happen at a hand-off from one persona to another, but it can also happen at a change of geographical, legal, or other type of boundary. If you notice that the terms that are used on the event stickies change at these boundaries, you are seeing a \"bounded context\" in Domain Driven Design terms. Highlight pivotal events by putting up blue painter\u2019s tape vertically behind the event. The second type of boundary is a subject boundary. You can detect a subject boundary by looking for the following conditions: You have multiple simultaneous series of events that only come together at a later time. You see the same terms being used in the event descriptions for a particular series of events. You can \u201cread\u201d a series of events from the point of view of a different persona when you are replaying them. You can delineate these different sets of simultaneous event streams by applying blue painter\u2019s tape horizontally, dividing the board into different swim lanes. Below is an example of a set of ordered domain events with pivotal events and subject swim lanes indicated. This example comes from applying event storming to the domain of container shipping process and is discussed in more detail in the container shipment analysis example . Step 4: Locate the Commands In this step you shift from analysis of the domain to the first stages of system design. Up until this point, you are simply trying to understand how the events in the domain relate to one another - this is why the participation of domain experts is so critical. However, to build a system that implements the business process that you are interested in, you have to move on to the question of how these events come into being. Commands are the most common mechanism by which events are created. The key to finding commands is to ask the question: \"Why did this event occur?\". In this step, the focus of the process moves to the sequence of actions that lead to events. Your goal is to find the causes for which the events record the effects. Expected event trigger types are: A human operator makes a decision and issues a command Some external system or sensor provides a stimulus An event results from some policy - typically automated processing of a precursor event The completion of some determined period of elapsed time. The triggering command is identified in a blue (sticky) note this may become a microservice api in a later implementation the human persona issuing the command is identified and shown in a yellow note. The diagram in next section illustrates the manufacturer actor using the place a shipment order command to create a shipment order placed event as a result of getting a quote from a previous request for quotation so he can deliver goods in container. It is possible to chain events and commands as presented in the \"one view\" figure above in the concepts section. Step 5: Describe the Data You can't truly define a command without understanding the data that is needed for the command to execute in order to produce the event. You can identify several types of data during this step. First, users (personas) need data from the user interface in order to make decisions before executing a command. That data forms part of the read model in a CQRS implementation. For each command and event pair, you add a data description of the expected attributes and data elements needed to take such a decision. Here is a simple example for a shipment order placed event created from a place a shipment order action . Another important part of the process that becomes more fully fleshed out at this step is the description of policies that can trigger the generation of an event from a previous event (or set of events). Assess if the data element is a main business entity, uniquely identified by a key, supported by multiple commands. It has a life span over the business process. This will lead to develop an entity life cycle analysis. This first level of data definition helps to assess the microservice scope and responsibility as you start to see commonalities emerge from the data used among several related events. Those concepts become more obvious in the next step. Step 6: Identify the Aggregates In DDD, entities and value objects can exist independently, but often, the relations are such that an entity or a value object has no value without its context. Aggregates provide that context by being those \"roots\" that comprise one or more entities and value objects that are linked together through a lifecycle. In event storming, aggregates emerge through the process by grouping events and commands that are related. This grouping not only consists of related data (entities and value objects) but also related actions (commands) that are connected by the lifecycle of that aggregate. Aggregates ultimately suggest microservice boundaries. In the container shipment example, you can see that you can group several commands and event pairs (with their associated data) together that are related through the lifecycle of an order for shipping. Step 7: Define Bounded Context In this step, you define terms and concepts with a clear meaning valid in a clear boundary and you define the context within which a model applies. (The term definition can change outside of the business unit for which an application is developed). The following items may be addressed: Which team owns the model? Which part of the model transit between team organization? What are the different code bases foreseen we need to implement? What are the different data schema ? (database or json or xml schemas) Here is an example of bounded context that will, most likely, lead to a microservice: Keep the model strictly consistent within these bounds. Step 8: Looking forward with insight storming In event atorming for Event Driven Architecture (EDA) solutions it is helpful to include an additional method step at this point identifying useful predictive analytics insights. Insights storming extends the basic methodology by looking forward and considering what if you could know in advance that an event is going to occur. How would this change your actions, and what would you do in advance of that event actually happening? You can think of insight storming as extending the analysis to Derived Events . Rather than being the factual recording of a past event, a derived event is a forward-looking or predictive event, that is, \"this event is probably going to happen at some time in the next n hours\u201d. By using this forward-looking insight combined with the known business data from earlier events, human actors and event triggering policies can make better decisions about how to react to new events as they occur. Insight storming amounts to asking workshop participants the question: \"What data would be helpful at each event trigger to assist the human user or automated event triggering policy make the best possible decision of how and when to act?\" An important motivation that drives the use of an event-driven architecture is that it simplifies design and realization of highly responsive systems that react immediately and intelligently, that is, in a personalized and context-aware way, and optimally to new events as they occur. This immediately suggests that predictive analytics and models to generate predictive insights have an important role to play. Predictive analytic insights are effectively probabilistic statements about which future events are likely to occur and what are the likely properties of those events. These probabilistic statements are typicaly generated by using models created by data scientists or using AI or ML. Correlating or joining independently gathered sources of information can also generate important predictive insights or be input to predictive analytic models. Business owners and stakeholders in the event storming workshop can offer good intuitions in several areas: Which probabilistic insights are likely to lead to improved or optimal decision making and action? The action could take the form of an immediate response to an event when it occurs. The action could be proactive behavior to avoid an undesirable event. What combined sources of information are likely to help create a model to predict this insight? With basic event storming, you look backwards at each event because an event is something that has already happened. When you identify data needed for an actor or policy to decide when and how to issue a command, there is a tendency to restrict consideration to properties of earlier known and captured business events. In insight storming you extend the approach to explicitly look forward and consider what is the probability that a particular event will occur at some future time and what would be its expected property values? How would this change the best action to take when and if this event occurs? Is there action we can take now proactively in advance of an expected undesirable event to prevent it happening or mitigate the consequences? The insight method step amounts to getting workshop participants to identify derived events and the data sources needed for the models that generate them. Adding an insight storming step using the questions above into the workshop will improve decision making and proactive behavior in the resulting design. Insights can be published into a bus and subscribed to by any decision step guidance. By identifying derived events, you can integrate analytic models and machine learning into the designed solution. Event and derived event feeds can be processed, filtered, joined, aggregated, modeled and scored to create valuable predictive insights. Use the following new notations for the insight step: Pale blue stickies for derived events. Parallelogram shape to show when events and derived events are combined to enable deeper insight models and predictions. Identify predictive insights as early as possible in the development life cycle. The best opportunity to do this is to add this step to the event storming workshop. The two diagrams below show the results of the insight storming step for the use case of container shipment analysis. The first diagram captures insights and associated linkages for each refrigerated container, identifying when automated changes to the thermostat settings can be made, when unit maintenance should be scheduled and when the container contents must be considered spoiled. The second diagram captures insights that could trigger recommendations to adjust ship course or speed in response to expected severe weather forcasts for the route ahead or predicted congestion and expected docking and unloading delays at the next port of call. Design iteration Attention we are not proposing to apply a waterfall approach, but before starting the deeper implementation with iterations, we want to spend sometime to define in more details what we want to build, how to organize the CI/CD projects and pipeline, select the development, test and product plaform, and define epics, user stories, components, microservices... This iteration can take from few hours to a week, depending on the expected MVP goals. For an event-driven solution a MVP for a single application should not take more than 3 to 4 iterations. Apply Domain-Driven Design See our separate note on applying DDD and event storming for event-driven microservice implementation Event storming to user stories and epics In agile methodology, creating user stories or epics is one of the most important elements in project management. The commands and policies related to events can be easily described as user stories, because commands and decisions are done by actors. The actor could be a system as well. For the data you must model the \"Create, Read, Update, Delete\" operations as user stories, mostly supported by a system actor. An event is the result or outcome of a user story. Events can be added as part of the acceptance criteria of the user stories to verify that the event really occurs. Applying to the container shipment use case The K Container Shipment use case demonstrates an implementation solution to validate the event-driven architecture. The container shipment analysis example , shows event storming and design thinking main artifacts, including artifacts for the monitoring of refrigerated containers. Further Readings Introduction to event storming from Alberto Brandolini Event Storming Guide Wikipedia Domain Driven Design Eric Evans: \"Domain Driven Design - Tacking complexity in the heart of software\" Domain drive design with event storming introduction video Patterns related to Domain Driven Design by Martin Fowler Kyle Brown - IBM - Apply Domain-Driven Design to microservices architecture Applying DDD and event storming for event-driven microservice implementation","title":"Event Storming Workshop"},{"location":"methodology/readme/#event-driven-solution-implementation-methodology","text":"In this article we are presenting an end to end set of activities to run a successful Minimum Viable Product for an event-driven solution using cloud native microservices and event backbone as the core technology approach. The discovery and analysis of the MVP scope starts with an event storming workshop where designer, architect work hand to hand with business users and domain subject matter experts. From the different outcomes of the workshop, the development team starts to outline components, microservices, business entity life cycle, etc... in a short design iteration . The scope is well defined Epics, Hill and user stories defined, at least for the first iterations, and the MVP can start.","title":"Event driven solution implementation methodology"},{"location":"methodology/readme/#event-storming-workshop","text":"Event storming is a workshop format for quickly exploring complex business domains by focusing on domain events generated in the context of a business process or a business application. A domain event is something meaningful to the experts that happened in the domain. The workshop focuses on communication between product owner, domain experts and developers. The event storming method was introduced and publicized by Alberto Brandolini in \"Introducing event storming book\" . This approach is recognized in the Domain Driven Design (DDD) community as a technique for rapid capture of a solution design and improved team understanding of the domain. Domain represents some are of the business that has the analysis focus. This article outlines the method and describes refinements and extensions that are useful in designs for an event-driven architecture. This extended approach adds an insight storming step to identify and capture value adding predictive insights about possible future events. The predictive insights are generated by using data science analysis, data models, artificial intelligence (AI), or machine learning (ML). This article describes in general terms all the steps to run an event storming workshop. The output of an actual workshop done on a sample problem - a world wide container shipment, is further detailed in the container shipment analysis example .","title":"Event Storming workshop"},{"location":"methodology/readme/#conducting-the-event-and-insight-storming-workshop","text":"Before conducting an event storming workshop, complete a Design Thinking Workshop in which Personas and Empathy Maps are developed and business pains and goals are defined. The event storming workshop adds more specific design on the events occuring at each step of the process, natural contexts for microservices and predictive insights to guide operation of the system. With this approach, a team that includes business owners and stakeholders can define a Minimal Viable Prototype (MVP) design for the solution. The resulting design is organized as a collection of loosely coupled microservices linked through an event-driven architecture and one or more event backbones. This style of design can be deployed into multicloud execution environments and allows for scaling and agile deployment. Preparations for the event storming workshop include the following steps: Get a room big enough to hold at least 6 to 8 persons and with enough wall space on which to stick big paper sheets: you will need a lot of wall space to define the models. Obtain green, orange, blue, and red square sticky notes, black sharpie pens and blue painter's tape. Discourage the use of open laptops during the meeting. Limit the number of chairs so that the team stays focused and connected and conversation flows easily.","title":"Conducting the event and insight storming workshop"},{"location":"methodology/readme/#concepts","text":"Many of the concepts addressed during the event storming workshop are defined in the Domain Driven Design approach. The following diagrams present the elements used during the analysis. The first diagram shows the initial set of concepts that are used in the process. Domain events are also named business events . An event is some action or happening which occurred in the system at a specific time in the past. The first step in the event storming process consists of these actions: Identifying all relevant events in the domain and specific process being analyzed, Writing a very short description of each event on a \"sticky\" note and placing all the event \"sticky\" notes in sequence on a timeline. The act of writing event descriptions often results in questions to be resolved later, or discussions about definitions that need to be recorded to ensure that everyone agrees on basic domain concepts. A timeline of domain events is the critical output of the first step in the event storming process. The timeline gives everyone a common understanding of when events take place in relation to each other. You still need to be able to take this initial level of understanding and move it towards an implementation. In making that step, you must expand your thinking to encompass the idea of a command, which is the action that kicks off the processing that triggers an event. As part of understanding the role of the command, you will also want to know who invokes a command (actors) and what information is needed to allow the command to be executed. This diagram show how those analysis elements are linked together: One-View Figure. Actors consume data by using a user interface and use the UI to interact with the system via commands. Actors could also be replace by articial intelligent agents. Commands are the result of some user decision or policy, and act on relevant data which are part of a Read model in the CQRS pattern. Policies (represented by lilac stickies) are reactive logics that take place after an event occurs, and trigger other commands. Policies always start with the phrase \"whenever...\". They can be a manual step a human follows, such as a documented procedure or guidance, or they may be automated. When applying the Agile Business Rule Development methodology it will be mapped to a Decision within the Decision Model Notation . External systems produce events. Data can be presented to users in a user interface or modified by the system. Events can be created by commands or by external systems including IOT devices. They can be triggerred by the processing of other events or by some period of elapsed time. When an event is repeated or occurs regularly on a schedule, draw a clock or calendar icon in the corner of the sticky note for that event. As the events are identified and sequenced into a time line, you might find multiple independent subsequences that are not directly coupled to each other and thatrepresent different perspectives of the system, but occur in overlapped periods of time. These parallel event streams can be addressed by putting them into separate swimlanes delineated by using horizontal blue painter's tape. As the events are organized into a timeline, possibly with swim lanes, you can identify pivotal events. Pivotal events indicate major changes in the domain and often form the boundary between one phase of the system and another. Pivotal events will typically separate (a bounded context in DDD terms). Pivotal events are identified with vertical blue painters tape (crossing all the swimlanes). An example of a section of a completed event time line with pivotal events and swimlanes is shown below.","title":"Concepts"},{"location":"methodology/readme/#conducting-the-workshop","text":"The goal of the workshop is to better understand the business problem to address with a future application. But the approach can also apply to finding solutions to bottlenecks or other issues in existing applications. The workshop helps the team to understand the big picture of the solution by building a timeline of domain events as they occur during the business process life span. During the workshop, avoid documenting processing steps. The event storming method is not trying to specify a particular implementation. Instead, the focus in initial stages of the workshop is on identifying and sequencing the events that occur in the solution. The event timeline is a useful representation of the overall steps, communicating what must happen while remaining open to many possible implementation approaches.","title":"Conducting the workshop"},{"location":"methodology/readme/#step-1-domain-events-discovery","text":"Begin by writing each domain event on an orange sticky note with a few words and a verb in a past tense. Describe What's happened . At first just \"storm\" the events by having each domain expert generate an individual lists of domain events. You might not need to initially place the events on the ordered timeline as they write them. The events must be worded in a way that is meaningful to the domain experts and business stakeholder. You are explaining what happens in business terms, not what happens inside the implementation of the system. You don't need to describe all the events in your domain, but you must cover the process that you are interested in exploring from end to end. Therefore, make sure that you identify the start and end events and place them on the timeline at the beginning and end of the wall covered with paper. Place the other events that you identified between these two endpoints in the closest approximation that the team can agree to a sequential order. Don\u2019t worry about overlaps at this point; overlaps are addressed later.","title":"Step 1: Domain events discovery"},{"location":"methodology/readme/#step-2-tell-the-story","text":"In this step, you retell the story by talking about how to relate events to particular personas. A member of the team (often the facilitator, but others can do this as well) acts this out by taking on the perspective of a persona in the domain, such as a \"manufacturer\" who wants to ship a widget to a customer, and asking which events follow which other events. Start at the beginning of that persona's interaction and ask \"what happens next?\". Pick up and rearrange the events that the team storms. If you discover events that are duplicates, take those off the board. If events are in the wrong order, move them into the right order. When some parts are unclear, add questions or comments by using the red stickies.. Red stickies indicate that the team needs to follow up and clarify issues later. Likewise you want to use this time to document assumptions on the definition stickies. This is also a good time to rephrase events as you proceed through the story. Sometimes you need to rephrase an event description by putting the verbing in past tense, or adjusting the terms that are used to relate clearly to other identified events. In this step you focus on the mainline \"happy\" end-to-end path to avoid getting bogged down in details of exceptions and error handling. Exceptions can be added later","title":"Step 2: Tell the story"},{"location":"methodology/readme/#step-3-find-the-boundaries","text":"The next step of this part of the process is to find the boundaries of your system by looking at the events. Two types of boundaries can emerge; the first type of boundary is a time boundary. Often specific key \"pivotal events\" indicate a change from one aspect of a system to another. This can happen at a hand-off from one persona to another, but it can also happen at a change of geographical, legal, or other type of boundary. If you notice that the terms that are used on the event stickies change at these boundaries, you are seeing a \"bounded context\" in Domain Driven Design terms. Highlight pivotal events by putting up blue painter\u2019s tape vertically behind the event. The second type of boundary is a subject boundary. You can detect a subject boundary by looking for the following conditions: You have multiple simultaneous series of events that only come together at a later time. You see the same terms being used in the event descriptions for a particular series of events. You can \u201cread\u201d a series of events from the point of view of a different persona when you are replaying them. You can delineate these different sets of simultaneous event streams by applying blue painter\u2019s tape horizontally, dividing the board into different swim lanes. Below is an example of a set of ordered domain events with pivotal events and subject swim lanes indicated. This example comes from applying event storming to the domain of container shipping process and is discussed in more detail in the container shipment analysis example .","title":"Step 3: Find the Boundaries"},{"location":"methodology/readme/#step-4-locate-the-commands","text":"In this step you shift from analysis of the domain to the first stages of system design. Up until this point, you are simply trying to understand how the events in the domain relate to one another - this is why the participation of domain experts is so critical. However, to build a system that implements the business process that you are interested in, you have to move on to the question of how these events come into being. Commands are the most common mechanism by which events are created. The key to finding commands is to ask the question: \"Why did this event occur?\". In this step, the focus of the process moves to the sequence of actions that lead to events. Your goal is to find the causes for which the events record the effects. Expected event trigger types are: A human operator makes a decision and issues a command Some external system or sensor provides a stimulus An event results from some policy - typically automated processing of a precursor event The completion of some determined period of elapsed time. The triggering command is identified in a blue (sticky) note this may become a microservice api in a later implementation the human persona issuing the command is identified and shown in a yellow note. The diagram in next section illustrates the manufacturer actor using the place a shipment order command to create a shipment order placed event as a result of getting a quote from a previous request for quotation so he can deliver goods in container. It is possible to chain events and commands as presented in the \"one view\" figure above in the concepts section.","title":"Step 4: Locate the Commands"},{"location":"methodology/readme/#step-5-describe-the-data","text":"You can't truly define a command without understanding the data that is needed for the command to execute in order to produce the event. You can identify several types of data during this step. First, users (personas) need data from the user interface in order to make decisions before executing a command. That data forms part of the read model in a CQRS implementation. For each command and event pair, you add a data description of the expected attributes and data elements needed to take such a decision. Here is a simple example for a shipment order placed event created from a place a shipment order action . Another important part of the process that becomes more fully fleshed out at this step is the description of policies that can trigger the generation of an event from a previous event (or set of events). Assess if the data element is a main business entity, uniquely identified by a key, supported by multiple commands. It has a life span over the business process. This will lead to develop an entity life cycle analysis. This first level of data definition helps to assess the microservice scope and responsibility as you start to see commonalities emerge from the data used among several related events. Those concepts become more obvious in the next step.","title":"Step 5: Describe the Data"},{"location":"methodology/readme/#step-6-identify-the-aggregates","text":"In DDD, entities and value objects can exist independently, but often, the relations are such that an entity or a value object has no value without its context. Aggregates provide that context by being those \"roots\" that comprise one or more entities and value objects that are linked together through a lifecycle. In event storming, aggregates emerge through the process by grouping events and commands that are related. This grouping not only consists of related data (entities and value objects) but also related actions (commands) that are connected by the lifecycle of that aggregate. Aggregates ultimately suggest microservice boundaries. In the container shipment example, you can see that you can group several commands and event pairs (with their associated data) together that are related through the lifecycle of an order for shipping.","title":"Step 6: Identify the Aggregates"},{"location":"methodology/readme/#step-7-define-bounded-context","text":"In this step, you define terms and concepts with a clear meaning valid in a clear boundary and you define the context within which a model applies. (The term definition can change outside of the business unit for which an application is developed). The following items may be addressed: Which team owns the model? Which part of the model transit between team organization? What are the different code bases foreseen we need to implement? What are the different data schema ? (database or json or xml schemas) Here is an example of bounded context that will, most likely, lead to a microservice: Keep the model strictly consistent within these bounds.","title":"Step 7: Define Bounded Context"},{"location":"methodology/readme/#step-8-looking-forward-with-insight-storming","text":"In event atorming for Event Driven Architecture (EDA) solutions it is helpful to include an additional method step at this point identifying useful predictive analytics insights. Insights storming extends the basic methodology by looking forward and considering what if you could know in advance that an event is going to occur. How would this change your actions, and what would you do in advance of that event actually happening? You can think of insight storming as extending the analysis to Derived Events . Rather than being the factual recording of a past event, a derived event is a forward-looking or predictive event, that is, \"this event is probably going to happen at some time in the next n hours\u201d. By using this forward-looking insight combined with the known business data from earlier events, human actors and event triggering policies can make better decisions about how to react to new events as they occur. Insight storming amounts to asking workshop participants the question: \"What data would be helpful at each event trigger to assist the human user or automated event triggering policy make the best possible decision of how and when to act?\" An important motivation that drives the use of an event-driven architecture is that it simplifies design and realization of highly responsive systems that react immediately and intelligently, that is, in a personalized and context-aware way, and optimally to new events as they occur. This immediately suggests that predictive analytics and models to generate predictive insights have an important role to play. Predictive analytic insights are effectively probabilistic statements about which future events are likely to occur and what are the likely properties of those events. These probabilistic statements are typicaly generated by using models created by data scientists or using AI or ML. Correlating or joining independently gathered sources of information can also generate important predictive insights or be input to predictive analytic models. Business owners and stakeholders in the event storming workshop can offer good intuitions in several areas: Which probabilistic insights are likely to lead to improved or optimal decision making and action? The action could take the form of an immediate response to an event when it occurs. The action could be proactive behavior to avoid an undesirable event. What combined sources of information are likely to help create a model to predict this insight? With basic event storming, you look backwards at each event because an event is something that has already happened. When you identify data needed for an actor or policy to decide when and how to issue a command, there is a tendency to restrict consideration to properties of earlier known and captured business events. In insight storming you extend the approach to explicitly look forward and consider what is the probability that a particular event will occur at some future time and what would be its expected property values? How would this change the best action to take when and if this event occurs? Is there action we can take now proactively in advance of an expected undesirable event to prevent it happening or mitigate the consequences? The insight method step amounts to getting workshop participants to identify derived events and the data sources needed for the models that generate them. Adding an insight storming step using the questions above into the workshop will improve decision making and proactive behavior in the resulting design. Insights can be published into a bus and subscribed to by any decision step guidance. By identifying derived events, you can integrate analytic models and machine learning into the designed solution. Event and derived event feeds can be processed, filtered, joined, aggregated, modeled and scored to create valuable predictive insights. Use the following new notations for the insight step: Pale blue stickies for derived events. Parallelogram shape to show when events and derived events are combined to enable deeper insight models and predictions. Identify predictive insights as early as possible in the development life cycle. The best opportunity to do this is to add this step to the event storming workshop. The two diagrams below show the results of the insight storming step for the use case of container shipment analysis. The first diagram captures insights and associated linkages for each refrigerated container, identifying when automated changes to the thermostat settings can be made, when unit maintenance should be scheduled and when the container contents must be considered spoiled. The second diagram captures insights that could trigger recommendations to adjust ship course or speed in response to expected severe weather forcasts for the route ahead or predicted congestion and expected docking and unloading delays at the next port of call.","title":"Step 8: Looking forward with insight storming"},{"location":"methodology/readme/#design-iteration","text":"Attention we are not proposing to apply a waterfall approach, but before starting the deeper implementation with iterations, we want to spend sometime to define in more details what we want to build, how to organize the CI/CD projects and pipeline, select the development, test and product plaform, and define epics, user stories, components, microservices... This iteration can take from few hours to a week, depending on the expected MVP goals. For an event-driven solution a MVP for a single application should not take more than 3 to 4 iterations.","title":"Design iteration"},{"location":"methodology/readme/#apply-domain-driven-design","text":"See our separate note on applying DDD and event storming for event-driven microservice implementation","title":"Apply Domain-Driven Design"},{"location":"methodology/readme/#event-storming-to-user-stories-and-epics","text":"In agile methodology, creating user stories or epics is one of the most important elements in project management. The commands and policies related to events can be easily described as user stories, because commands and decisions are done by actors. The actor could be a system as well. For the data you must model the \"Create, Read, Update, Delete\" operations as user stories, mostly supported by a system actor. An event is the result or outcome of a user story. Events can be added as part of the acceptance criteria of the user stories to verify that the event really occurs.","title":"Event storming to user stories and epics"},{"location":"methodology/readme/#applying-to-the-container-shipment-use-case","text":"The K Container Shipment use case demonstrates an implementation solution to validate the event-driven architecture. The container shipment analysis example , shows event storming and design thinking main artifacts, including artifacts for the monitoring of refrigerated containers.","title":"Applying to the container shipment use case"},{"location":"methodology/readme/#further-readings","text":"Introduction to event storming from Alberto Brandolini Event Storming Guide Wikipedia Domain Driven Design Eric Evans: \"Domain Driven Design - Tacking complexity in the heart of software\" Domain drive design with event storming introduction video Patterns related to Domain Driven Design by Martin Fowler Kyle Brown - IBM - Apply Domain-Driven Design to microservices architecture Applying DDD and event storming for event-driven microservice implementation","title":"Further Readings"},{"location":"ml-workbench/","text":"Data Scientist Workbench The goals here are to illustrate how we can integrate tools like notebook, used by Data Scientist to be connected in real time to event stream to do Data Analysis. Supporting Products Code Samples UNDER construction!","title":"Data Scientist Workbench"},{"location":"ml-workbench/#data-scientist-workbench","text":"The goals here are to illustrate how we can integrate tools like notebook, used by Data Scientist to be connected in real time to event stream to do Data Analysis.","title":"Data Scientist Workbench"},{"location":"ml-workbench/#supporting-products","text":"","title":"Supporting Products"},{"location":"ml-workbench/#code-samples","text":"UNDER construction!","title":"Code Samples"},{"location":"rt-analytics/","text":"Process continuous streaming events One of the essential elements of modern event-driven solutions is the ability to process continuous event streams to derive real time insights and intelligence. In this section we will take more detailed look at what this means in terms of required capabilities and the technology choices that are available to provide these as part of the Event Driven Architecture. Streaming analytics (real-time analytics) Streaming analytics provides the capabilities to look into and understand the events flowing through unbounded real-time event streams. Streaming applications process the event flow and allow data and analytical functions to be applied to information in the stream. Streaming applications are written as multistep flows across the following capabilities: Ingest many sources of events. Prepare data by transforming, filtering, correlating, aggregating on some metrics and leveraging other data sources for data enrichment. Detect and predict event patterns using scoring and classification. Decide by applying business rules and business logic. Act by directly executing an action, or in event-driven systems publishing an event notification or command. Basic streaming analytics capabilities To support the real-time analytical processing of the unbounded event streams, the following capabilities are essential to the event stream processing component: Continuous event ingestion and analytical processing. Processing across multiple event streams. Low latency processing, where data do not have to be stored. Processing of high-volume and high-velocity streams of data. Continuous query and analysis of the feed. Correlation across events and streams. Windowing and stateful processing. Query and analysis of stored data. Development and execution of data pipelines. Development and execution of analytics pipelines. Scoring of machine learning models in line in the real-time event stream processing. Support for real-time analytics and decision-making Beyond the basic capabilities, consider supporting other frequently-seen event stream types and processing capabilities in your event stream processing component. By creating functions for these stream types and processes in the streaming application code, you can simplify the problem and reduce the development time. These capabilities include the following: Geospatial Location-based analytics Geofencing & map matching Spatio-temporal hangout detection Time series analysis Timestamped data analysis Anomaly detection & forecasting Text analytics Natural Language Processing & Natural Language Understanding Sentiment analysis & entity extraction Video and audio Speech-to-text conversion Image recognition Rules Decisions described as business logic Complex Event Processing (CEP) Temporal pattern detection Entity Analytics Relationships between entities Probabilistic matching Application programming languages and standards Few standards exist for event stream applications and languages. Typically, streaming engines have provided language-specific programming models tied to a specific platform. The commonly used languages include the following: * Python supports working with data and is popular with data scientists and data engineers. * Java is the pervasive application development language. * Scala adds functional programming and immutable objects to Java. Other platform specific languages have emerged when real-time processing demands stringent performance requirements real time processing performance is required. More recently Google initiated the Apache Beam project https://beam.apache.org/ to provide a unified programming model for streaming analytics applications. Beam is a higher-level unified programming model that provides a standard way of writing streaming analytics applications in many supported languages, including Java, Python, Go and SQL. Streaming analytics engines typically support this unified programming model through a Beam runner that takes the code and converts it to platform-native executable code for the specific engine. See https://beam.apache.org/documentation/runners/capability-matrix/ for details of supporting engines and the capabilities. Leading engines include Google Cloud DataFlow, Apache Flink, Apache Spark, Apache Apex, and IBM Streams. Run time characteristics In operational terms streaming analytics engines must receive and analyze arriving data continuously: The \"Feed Never Ends\" The collection is unbounded. Not a request response set based model. The \"Firehose Doesn\u2019t Stop\" Keep drinking and keep up. The processing rate is greater than or equal to the feed rate. The analytics engine must be resilient and self-healing. These specialized demands and concerns, which are not found in many other information processing environments, have led to highly-optimized runtimes and engines for stateful, parallel processing of analytical workloads across multiple event streams. Products Streaming Analytics The market for streaming analytics products is quite confused with lots of different offering and very few standards to bring them together. The potential product selection list for the streaming analytics component in the event driven architecture would need to consider: Top Open Source projects: * Flink - real time streaming engine, both real time and batch analytics in one tool. * Spark Streaming - micro batch processing through spark engine. * Storm - Has not shown enough adoption. * Kafka Streams - new/emerging API access for processing event streams in Kafka using a graph of operators Major Cloud Platform Providers support: * Google Cloud DataFlow \u2013 proprietary engine open source streams application language ( Beam ) * Azure Stream Analytics \u2013 proprietary engine , SQL interface * Amazon Kinesis - proprietary AWS IBM offerings * IBM Streams/streaming Analytics (High performing parallel processing engine for real time analytics work loads) * IBM Event streams (Kafka based event log/streaming platform) Evaluation of the various options, highlights * The proprietary engines from the major providers, Google, MicroSoft, Amazon and IBM Streams continue to provide significant benefits in terms of performance and functionality for real time analysis of high volume realtime event streams. * Kafka streams provides a convenient programming model for microservices to interact with the event stream data, but doesnt provide the optimized stream processing engine required for high volume real time analytics. Our decision for the Event Driven Architecture is to include: IBM streams as the performant, functionally rich real time event stream processing engine Event Streams (Kafka Streams), for manipulation of event streams within microservices IBM streams also supports Apache Beam as the open source Streams Application language, which would allow portability of streams applications across, Flink, Spark, Google DataFlow... Decision Insights Decision insight is a stateful operator to manage business decision on enriched event linked to business context and business entities. This is the cornerstone to apply business logic and best action using time related business rules. See this note too IBM [Operational Decision Manager Product documentation](https://www.ibm.com/support/knowledgecenter/en/SSQP76_8.9.1/com.ibm.odm.itoa.overview/topics/con_what_is_i2a.html","title":"Real time analytics"},{"location":"rt-analytics/#process-continuous-streaming-events","text":"One of the essential elements of modern event-driven solutions is the ability to process continuous event streams to derive real time insights and intelligence. In this section we will take more detailed look at what this means in terms of required capabilities and the technology choices that are available to provide these as part of the Event Driven Architecture.","title":"Process continuous streaming events"},{"location":"rt-analytics/#streaming-analytics-real-time-analytics","text":"Streaming analytics provides the capabilities to look into and understand the events flowing through unbounded real-time event streams. Streaming applications process the event flow and allow data and analytical functions to be applied to information in the stream. Streaming applications are written as multistep flows across the following capabilities: Ingest many sources of events. Prepare data by transforming, filtering, correlating, aggregating on some metrics and leveraging other data sources for data enrichment. Detect and predict event patterns using scoring and classification. Decide by applying business rules and business logic. Act by directly executing an action, or in event-driven systems publishing an event notification or command.","title":"Streaming analytics (real-time analytics)"},{"location":"rt-analytics/#basic-streaming-analytics-capabilities","text":"To support the real-time analytical processing of the unbounded event streams, the following capabilities are essential to the event stream processing component: Continuous event ingestion and analytical processing. Processing across multiple event streams. Low latency processing, where data do not have to be stored. Processing of high-volume and high-velocity streams of data. Continuous query and analysis of the feed. Correlation across events and streams. Windowing and stateful processing. Query and analysis of stored data. Development and execution of data pipelines. Development and execution of analytics pipelines. Scoring of machine learning models in line in the real-time event stream processing.","title":"Basic streaming analytics capabilities"},{"location":"rt-analytics/#support-for-real-time-analytics-and-decision-making","text":"Beyond the basic capabilities, consider supporting other frequently-seen event stream types and processing capabilities in your event stream processing component. By creating functions for these stream types and processes in the streaming application code, you can simplify the problem and reduce the development time. These capabilities include the following: Geospatial Location-based analytics Geofencing & map matching Spatio-temporal hangout detection Time series analysis Timestamped data analysis Anomaly detection & forecasting Text analytics Natural Language Processing & Natural Language Understanding Sentiment analysis & entity extraction Video and audio Speech-to-text conversion Image recognition Rules Decisions described as business logic Complex Event Processing (CEP) Temporal pattern detection Entity Analytics Relationships between entities Probabilistic matching","title":"Support for real-time analytics and decision-making"},{"location":"rt-analytics/#application-programming-languages-and-standards","text":"Few standards exist for event stream applications and languages. Typically, streaming engines have provided language-specific programming models tied to a specific platform. The commonly used languages include the following: * Python supports working with data and is popular with data scientists and data engineers. * Java is the pervasive application development language. * Scala adds functional programming and immutable objects to Java. Other platform specific languages have emerged when real-time processing demands stringent performance requirements real time processing performance is required. More recently Google initiated the Apache Beam project https://beam.apache.org/ to provide a unified programming model for streaming analytics applications. Beam is a higher-level unified programming model that provides a standard way of writing streaming analytics applications in many supported languages, including Java, Python, Go and SQL. Streaming analytics engines typically support this unified programming model through a Beam runner that takes the code and converts it to platform-native executable code for the specific engine. See https://beam.apache.org/documentation/runners/capability-matrix/ for details of supporting engines and the capabilities. Leading engines include Google Cloud DataFlow, Apache Flink, Apache Spark, Apache Apex, and IBM Streams.","title":"Application programming languages and standards"},{"location":"rt-analytics/#run-time-characteristics","text":"In operational terms streaming analytics engines must receive and analyze arriving data continuously: The \"Feed Never Ends\" The collection is unbounded. Not a request response set based model. The \"Firehose Doesn\u2019t Stop\" Keep drinking and keep up. The processing rate is greater than or equal to the feed rate. The analytics engine must be resilient and self-healing. These specialized demands and concerns, which are not found in many other information processing environments, have led to highly-optimized runtimes and engines for stateful, parallel processing of analytical workloads across multiple event streams.","title":"Run time characteristics"},{"location":"rt-analytics/#products","text":"","title":"Products"},{"location":"rt-analytics/#streaming-analytics","text":"The market for streaming analytics products is quite confused with lots of different offering and very few standards to bring them together. The potential product selection list for the streaming analytics component in the event driven architecture would need to consider: Top Open Source projects: * Flink - real time streaming engine, both real time and batch analytics in one tool. * Spark Streaming - micro batch processing through spark engine. * Storm - Has not shown enough adoption. * Kafka Streams - new/emerging API access for processing event streams in Kafka using a graph of operators Major Cloud Platform Providers support: * Google Cloud DataFlow \u2013 proprietary engine open source streams application language ( Beam ) * Azure Stream Analytics \u2013 proprietary engine , SQL interface * Amazon Kinesis - proprietary AWS IBM offerings * IBM Streams/streaming Analytics (High performing parallel processing engine for real time analytics work loads) * IBM Event streams (Kafka based event log/streaming platform) Evaluation of the various options, highlights * The proprietary engines from the major providers, Google, MicroSoft, Amazon and IBM Streams continue to provide significant benefits in terms of performance and functionality for real time analysis of high volume realtime event streams. * Kafka streams provides a convenient programming model for microservices to interact with the event stream data, but doesnt provide the optimized stream processing engine required for high volume real time analytics. Our decision for the Event Driven Architecture is to include: IBM streams as the performant, functionally rich real time event stream processing engine Event Streams (Kafka Streams), for manipulation of event streams within microservices IBM streams also supports Apache Beam as the open source Streams Application language, which would allow portability of streams applications across, Flink, Spark, Google DataFlow...","title":"Streaming Analytics"},{"location":"rt-analytics/#decision-insights","text":"Decision insight is a stateful operator to manage business decision on enriched event linked to business context and business entities. This is the cornerstone to apply business logic and best action using time related business rules. See this note too IBM [Operational Decision Manager Product documentation](https://www.ibm.com/support/knowledgecenter/en/SSQP76_8.9.1/com.ibm.odm.itoa.overview/topics/con_what_is_i2a.html","title":"Decision Insights"},{"location":"serverless/","text":"Function as a Service Kubeless To install kubeless on ICP we first connect to the cluster and then use the command below: $ kubectl create namespace kubeless $ kubectl create -f https://github.com/kubeless/kubeless/releases/download/v1.0.0-alpha.8/kubeless-v1.0.0-alpha.8.yaml -n kubeless The image is using RBAC: The deployment creates one pod with 3 containers inside: We need to have the kubeless CLI install: $ export OS=$(uname -s| tr '[:upper:]' '[:lower:]') $ curl -OL https://github.com/kubeless/kubeless/releases/download/$RELEASE/kubeless_$OS-amd64.zip && \\ unzip kubeless_$OS-amd64.zip && \\ sudo mv bundles/kubeless_$OS-amd64/kubeless /usr/local/bin/ To deploy a simple hello The code for this function is under this repository but it is a simple python function implementing the serverless 'interface': def hello(event, context): print(event) return event['data'] To deploy we can use the command: $ kubeless function deploy hellojb --runtime python3.6 --trigger-http --from-file functionHello.py --handler functionHello.hello INFO[0000] Deploying function... INFO[0001] Function hellojb submitted for deployment INFO[0001] Check the deployment status executing 'kubeless function ls hellojb' To see the functions deployed $ kubectl get functions or $ kubeless function ls NAME NAMESPACE HANDLER RUNTIME DEPENDENCIES STATUS hellojb greencompute test.hello python3.6 1/1 READY The deployment of a function creates automatically a pod: $ kubectl describe pod hellojb ... Containers: hellojb: Container ID: docker://53ca1131747e5b18bfeb67609b4e7bb2400cf45202ade2c03274b9df1eff9bc2 Image: kubeless/python@sha256:0c9f8f727d42625a4e25230cfe612df7488b65f283e7972f84108d87e7443d72 Image ID: docker-pullable://kubeless/python@sha256:0c9f8f727d42625a4e25230cfe612df7488b65f283e7972f84108d87e7443d72 Port: 8080/TCP Host Port: 0/TCP State: Running Started: Thu, 06 Sep 2018 17:12:52 -0700 Ready: True Restart Count: 0 Liveness: http-get http://:8080/healthz delay=3s timeout=1s period=30s #success=1 #failure=3 Environment: FUNC_HANDLER: hello MOD_NAME: test FUNC_TIMEOUT: 180 FUNC_RUNTIME: python3.6 FUNC_MEMORY_LIMIT: 0 FUNC_PORT: 8080 PYTHONPATH: /kubeless/lib/python3.6/site-packages:/kubeless Mounts: /kubeless from hellojb (rw) /var/run/secrets/kubernetes.io/serviceaccount from default-token-9nw2z (ro) and kubeless create service for each function: $ kubectl describe svc hellojb Name: hellojb Namespace: greencompute Labels: created-by=kubeless function=hellojb Annotations: <none> Selector: created-by=kubeless,function=hellojb Type: ClusterIP IP: 10.10.10.41 Port: http-function-port 8080/TCP TargetPort: 8080/TCP Endpoints: 192.168.130.101:8080 Session Affinity: None Events: <none> Remove the function kubeless delete Calling the function The quickest way is to proxy the server and then call the local URL: $ kubectl proxy -p 8080 & $ kubeless function call hellojb --data 'Hello Bill!' A second way is to test using HTTP client. Developing a predictive scoring function In this project we are addressing how to develop a scoring service using Python, sklearn and serveless to deploy the model as function. Compendium Excellent article from Martin Fowler Serverless framework : The Framework uses new event-driven compute services, like AWS Lambda, Google Cloud Functions, and more. It's a command-line tool, providing scaffolding, workflow automation and best practices for developing and deploying your serverless architecture. Apache OpenWhisk Claudia to deploy nodejs on AWS lambda Zappa : Zappa makes it super easy to build and deploy server-less, event-driven Python applications on AWS Lambda + API Gateway. Serverless conf: operational best practices Evaluating cost for FaaS","title":"Serverless"},{"location":"serverless/#function-as-a-service","text":"","title":"Function as a Service"},{"location":"serverless/#kubeless","text":"To install kubeless on ICP we first connect to the cluster and then use the command below: $ kubectl create namespace kubeless $ kubectl create -f https://github.com/kubeless/kubeless/releases/download/v1.0.0-alpha.8/kubeless-v1.0.0-alpha.8.yaml -n kubeless The image is using RBAC: The deployment creates one pod with 3 containers inside: We need to have the kubeless CLI install: $ export OS=$(uname -s| tr '[:upper:]' '[:lower:]') $ curl -OL https://github.com/kubeless/kubeless/releases/download/$RELEASE/kubeless_$OS-amd64.zip && \\ unzip kubeless_$OS-amd64.zip && \\ sudo mv bundles/kubeless_$OS-amd64/kubeless /usr/local/bin/","title":"Kubeless"},{"location":"serverless/#to-deploy-a-simple-hello","text":"The code for this function is under this repository but it is a simple python function implementing the serverless 'interface': def hello(event, context): print(event) return event['data'] To deploy we can use the command: $ kubeless function deploy hellojb --runtime python3.6 --trigger-http --from-file functionHello.py --handler functionHello.hello INFO[0000] Deploying function... INFO[0001] Function hellojb submitted for deployment INFO[0001] Check the deployment status executing 'kubeless function ls hellojb' To see the functions deployed $ kubectl get functions or $ kubeless function ls NAME NAMESPACE HANDLER RUNTIME DEPENDENCIES STATUS hellojb greencompute test.hello python3.6 1/1 READY The deployment of a function creates automatically a pod: $ kubectl describe pod hellojb ... Containers: hellojb: Container ID: docker://53ca1131747e5b18bfeb67609b4e7bb2400cf45202ade2c03274b9df1eff9bc2 Image: kubeless/python@sha256:0c9f8f727d42625a4e25230cfe612df7488b65f283e7972f84108d87e7443d72 Image ID: docker-pullable://kubeless/python@sha256:0c9f8f727d42625a4e25230cfe612df7488b65f283e7972f84108d87e7443d72 Port: 8080/TCP Host Port: 0/TCP State: Running Started: Thu, 06 Sep 2018 17:12:52 -0700 Ready: True Restart Count: 0 Liveness: http-get http://:8080/healthz delay=3s timeout=1s period=30s #success=1 #failure=3 Environment: FUNC_HANDLER: hello MOD_NAME: test FUNC_TIMEOUT: 180 FUNC_RUNTIME: python3.6 FUNC_MEMORY_LIMIT: 0 FUNC_PORT: 8080 PYTHONPATH: /kubeless/lib/python3.6/site-packages:/kubeless Mounts: /kubeless from hellojb (rw) /var/run/secrets/kubernetes.io/serviceaccount from default-token-9nw2z (ro) and kubeless create service for each function: $ kubectl describe svc hellojb Name: hellojb Namespace: greencompute Labels: created-by=kubeless function=hellojb Annotations: <none> Selector: created-by=kubeless,function=hellojb Type: ClusterIP IP: 10.10.10.41 Port: http-function-port 8080/TCP TargetPort: 8080/TCP Endpoints: 192.168.130.101:8080 Session Affinity: None Events: <none> Remove the function kubeless delete","title":"To deploy a simple hello"},{"location":"serverless/#calling-the-function","text":"The quickest way is to proxy the server and then call the local URL: $ kubectl proxy -p 8080 & $ kubeless function call hellojb --data 'Hello Bill!' A second way is to test using HTTP client.","title":"Calling the function"},{"location":"serverless/#developing-a-predictive-scoring-function","text":"In this project we are addressing how to develop a scoring service using Python, sklearn and serveless to deploy the model as function.","title":"Developing a predictive scoring function"},{"location":"serverless/#compendium","text":"Excellent article from Martin Fowler Serverless framework : The Framework uses new event-driven compute services, like AWS Lambda, Google Cloud Functions, and more. It's a command-line tool, providing scaffolding, workflow automation and best practices for developing and deploying your serverless architecture. Apache OpenWhisk Claudia to deploy nodejs on AWS lambda Zappa : Zappa makes it super easy to build and deploy server-less, event-driven Python applications on AWS Lambda + API Gateway. Serverless conf: operational best practices Evaluating cost for FaaS","title":"Compendium"}]}